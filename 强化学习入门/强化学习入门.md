# 强化学习入门 第一讲 MDP


 	强化学习入门第一讲 马尔科夫决策过程

强化学习算法理论的形成可以追溯到上个世纪七八十年代，近几十年来强化学习算法一直在默默地不断进步，真正火起来是最近几年。代表性的事件是DeepMind 团队于2013年12月首次展示了机器利用强化学习算法在雅达利游戏中打败人类专业玩家，其成果在2015年发布于顶级期刊《自然》上；2014年，谷歌将DeepMind 团队收购。2016年3月，DeepMind开发的AlphaGo程序利用强化学习算法以4:1击败世界围棋高手李世石，至此强化学习算法引起更多学者的关注。如今，强化学习算法已经在如游戏，机器人等领域中开花结果。各大科技公司，如谷歌，facebook，百度，微软等更是将强化学习技术作为其重点发展的技术之一。可以说强化学习算法正在改变和影响着这个世界，掌握了这门技术就掌握了改变世界和影响世界的工具。

现在网上有一些强化学习的教程，这些教程都来自世界顶尖名校，如2015年David Silver的经典课程[Teaching](https://link.zhihu.com/?target=http%3A//www0.cs.ucl.ac.uk/staff/D.Silver/web/Teaching.html) ，2017年加州大学伯克利分校Levine, Finn, Schulman的课程 [CS 294 Deep Reinforcement Learning, Spring 2017](https://link.zhihu.com/?target=http%3A//rll.berkeley.edu/deeprlcourse/) ，卡内基梅隆大学的2017 春季课程[Deep RL and Control](https://link.zhihu.com/?target=https%3A//katefvision.github.io/) 。据我所了解，国内还没有一门与之对应的中文课程，为了填补这个空白，因此决定写这个中文讲义。由于个人水平有限，理解的难免会有偏差的地方，有些地方也会有疏漏，不对的地方望各位见谅，也欢迎各位交流、批评、指正。以文会友也是本文写作的另外一个目的。公布个qq交流群202570720，有问题群里留言。

![img](https://pic4.zhimg.com/80/v2-3d1115f058799a11824a68b3d6d6d9c7_hd.jpg)



图1.1 强化学习原理解释

图1.1解释了强化学习的基本原理。智能体在完成某项任务时，首先通过动作A与周围环境进行交互，在动作A和环境的作用下，智能体会产生新的状态，同时环境会给出一个立即回报。如此循环下去，智能体与环境进行不断地交互从而产生很多数据。强化学习算法利用产生的数据修改自身的动作策略，再与环境交互，产生新的数据，并利用新的数据进一步改善自身的行为，经过数次迭代学习后，智能体能最终地学到完成相应任务的最优动作（最优策略）。

从强化学习的基本原理我们能看到强化学习与其他机器学习算法如监督学习和非监督学习的一些基本差别。在监督学习和非监督学习中，数据是静态的不需要与环境进行交互，比如图像识别，只要给足够的差异样本，将数据输入到深度网络中进行训练即可。然而，强化学习的学习过程是个动态的，不断交互的过程，所需要的数据也是通过与环境不断地交互产生的。所以，与监督学习和非监督学习相比，强化学习涉及到的对象更多，比如动作，环境，状态转移概率和回报函数等。强化学习更像是人学习的过程，人类通过与周围环境交互，学会了走路，奔跑，劳动。人类与大自然，与宇宙的交互创造了现代文明。另外，深度学习如图像识别和语音识别解决的是感知的问题，强化学习解决的是决策的问题。人工智能的终极目的是通过感知进行智能决策。所以，将近年发展起来的深度学习技术与强化学习算法结合而产生的深度强化学习算法是人类实现人工智能终极目的的一个很有前景的方法。

无数学者们通过几十年不断地努力和探索，提出了一套可以解决大部分强化学习问题的框架，这个框架就是马尔科夫决策过程，简称MDP。下面我们会循序渐进地介绍马尔科夫决策过程：先介绍马尔科夫性，再介绍马尔科夫过程，最后介绍马尔科夫决策过程。

第一个概念是马尔科夫性：所谓马尔科夫性是指系统的下一个状态![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt%2B1%7D)仅与当前状态![[公式]](https://www.zhihu.com/equation?tex=s_t)有关，而与以前的状态无关。

定义：状态![[公式]](https://www.zhihu.com/equation?tex=s_t) 是马尔科夫的，当且仅当![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+P%5Cleft%5Bs_%7Bt%2B1%7D%7Cs_t%5Cright%5D%3DP%5Cleft%5Bs_%7Bt%2B1%7D%7Cs_1%2C%5Ccdots+%2Cs_t%5Cright%5D+%5C%5D)。

定义中可以看到，当前状态![[公式]](https://www.zhihu.com/equation?tex=s_t) 其实是蕴含了所有相关的历史信息![[公式]](https://www.zhihu.com/equation?tex=s_1%2C%5Ccdots+%2Cs_t)，一旦当前状态已知，历史信息将会被抛弃。

马尔科夫性描述的是每个状态的性质，但真正有用的是如何描述一个**状态序列**。数学中用来描述随机变量序列的学科叫随机过程。所谓随机过程就是指**随机变量序列**。若随机变量序列中的每个状态都是马尔科夫的则称此随机过程为马尔科夫随机过程。

第二个概念是马尔科夫过程

马尔科夫过程的定义：马尔科夫过程是一个二元组![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28S%2CP%5Cright%29)，且满足：![[公式]](https://www.zhihu.com/equation?tex=S)是有限状态集合， ![[公式]](https://www.zhihu.com/equation?tex=P)是状态转移概率。状态转移概率矩阵为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+P%3D%5Cleft%5B%5Cbegin%7Bmatrix%7D+P_%7B11%7D%26+%5Ccdots%26+P_%7B1n%7D%5C%5C+%5Cvdots%26+%5Cvdots%26+%5Cvdots%5C%5C+P_%7Bn1%7D%26+%5Ccdots%26+P_%7Bnn%7D%5C%5C+%5Cend%7Bmatrix%7D%5Cright%5D+%5C%5D)。下面我们以一个例子来进行阐述。

![img](https://pic2.zhimg.com/80/v2-6722fb1c69f547d8f1630142ee63d231_hd.jpg)

图1.2 马尔科夫过程示例图

如图1.2所示为一个学生的7种状态{娱乐，课程1，课程2， 课程3，考过，睡觉，论文}，每种状态之间的转换概率如图所知。则该生从课程1开始一天可能的状态序列为：

课1-课2-课3-考过-睡觉

课1-课2-睡觉

以上状态序列称为马尔科夫链。当给定状态转移概率时，从某个状态出发存在多条马尔科夫链。对于游戏或者机器人，马尔科夫过程不足以描述其特点，因为不管是游戏还是机器人，他们都是通过动作与环境进行交互，并从环境中获得奖励，而马尔科夫过程中不存在动作和奖励。将动作（策略）和回报考虑在内的马尔科夫过程称为马尔科夫决策过程。

第三个概念是马尔科夫决策过程

马尔科夫决策过程由元组![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28S%2CA%2CP%2CR%2C%5Cgamma%5Cright%29)描述，其中：![[公式]](https://www.zhihu.com/equation?tex=S)为有限的状态集, ![[公式]](https://www.zhihu.com/equation?tex=A) 为有限的动作集, ![[公式]](https://www.zhihu.com/equation?tex=P) 为状态转移概率, ![[公式]](https://www.zhihu.com/equation?tex=R)为回报函数, ![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma+) 为折扣因子，用来计算累积回报。注意，跟马尔科夫过程不同的是，马尔科夫决策过程的状态转移概率是包含动作的即：![[公式]](https://www.zhihu.com/equation?tex=P_%7Bss%27%7D%5E%7Ba%7D%3DP%5Cleft%5BS_%7Bt%2B1%7D%3Ds%27%7CS_t%3Ds%2CA_t%3Da%5Cright%5D)

举个例子：

![img](https://pic4.zhimg.com/80/v2-604dcc4d56bfdfe410886f99520a5bdb_hd.jpg)

图1.3 马尔科夫决策过程示例图

图1.3为马尔科夫决策过程的示例图，图1.3与图1.2对应。在图1.3中，学生有五个状态，状态集为![[公式]](https://www.zhihu.com/equation?tex=S%3D%5Cleft%5C%7Bs_1%2Cs_2%2Cs_3%2Cs_4%2Cs_5%5Cright%5C%7D)，动作集为A={玩，退出，学习，发论文，睡觉}，在图1.3中立即回报用红色标记。

强化学习的目标是给定一个马尔科夫决策过程，寻找最优策略。所谓策略是指状态到动作的映射，策略常用符号![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+) 表示，它是指给定状态![[公式]](https://www.zhihu.com/equation?tex=s) 时，动作集上的一个分布，即

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28a%7Cs%5Cright%29%3Dp%5Cleft%5BA_t%3Da%7CS_t%3Ds%5Cright%5D+%5C%5D) (1.1)

这个公式是什么意思呢？策略的定义是用条件概率分布给出的。我相信，一涉及到概率公式，大部分人都会心里咯噔一下，排斥之情油然而生。但是，要想完全掌握强化学习这门工具，概率公式必不可少。只有掌握了概率公式，才能真正领会强化学习的精髓。

简单解释下概率在强化学习中的重要作用。首先，强化学习的策略往往是随机策略。采用随机策略的好处是可以将探索耦合到采样的过程中。所谓探索是指机器人尝试其他的动作以便找到更好的策略。其次，在实际应用中，存在各种噪声，这些噪声大都服从正态分布，如何去掉这些噪声也需要用到概率的知识。

言归正传，公式(1.1)的含义是：策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)在每个状态![[公式]](https://www.zhihu.com/equation?tex=s) 指定一个动作概率。如果给出的策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)是确定性的，那么策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)在每个状态![[公式]](https://www.zhihu.com/equation?tex=s)指定一个确定的动作。

例如其中一个学生的策略为![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi_1%5Cleft%28%5Ctextrm%7B%E7%8E%A9%7D%7Cs_1%5Cright%29%3D0.8+%5C%5D)，是指该学生在状态![[公式]](https://www.zhihu.com/equation?tex=s_1) 时玩的概率为0.8，不玩的概率是0.2，显然这个学生更喜欢玩。

另外一个学生的策略为![[公式]](https://www.zhihu.com/equation?tex=%5Cpi_2%5Cleft%28%5Ctextrm%7B%E7%8E%A9%7D%7Cs_1%5Cright%29%3D0.3)，是指该学生在状态![[公式]](https://www.zhihu.com/equation?tex=s_1)时玩的概率是0.3，显然这个学生不爱玩。依此类推，每学生都有自己的策略。强化学习是找到最优的策略，这里的最优是指得到的总回报最大。

当给定一个策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)时，我们就可以计算累积回报了。首先定义累积回报：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_t%3DR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Ccdots+%3D%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EkR_%7Bt%2Bk%2B1%7D%7D%5C%5C%5C%5C%5C+%5Cleft%281.2%5Cright%29+%5C%5D)

当给定策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)时，假设从状态![[公式]](https://www.zhihu.com/equation?tex=s_1) 出发，学生状态序列可能为：

![[公式]](https://www.zhihu.com/equation?tex=+s_1%5Crightarrow+s_2%5Crightarrow+s_3%5Crightarrow+s_4%5Crightarrow+s_5+%3B%5C%5C+s_1%5Crightarrow+s_2%5Crightarrow+s_3%5Crightarrow+s_5+%5C%5C+......)

此时，在策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)下，利用(1.2)式可以计算累积回报![[公式]](https://www.zhihu.com/equation?tex=G_1)，此时![[公式]](https://www.zhihu.com/equation?tex=G_1)有多个可能值 。由于策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)是随机的，因此累积回报也是随机的。为了评价状态![[公式]](https://www.zhihu.com/equation?tex=s_1)的价值，我们需要定义一个确定量来描述状态![[公式]](https://www.zhihu.com/equation?tex=s_1)的价值，很自然的想法是利用累积回报来衡量状态![[公式]](https://www.zhihu.com/equation?tex=s_1) 的价值。然而，累积回报![[公式]](https://www.zhihu.com/equation?tex=G_1) 是个随机变量，不是一个确定值，因此无法进行描述。但其期望是个确定值，可以作为状态值函数的定义。

状态值函数：

当智能体采用策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)时，累积回报服从一个分布，累积回报在状态![[公式]](https://www.zhihu.com/equation?tex=s)处的期望值定义为状态-值函数：

![[公式]](https://www.zhihu.com/equation?tex=%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3DE_%7B%5Cpi%7D%5Cleft%5B%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EkR_%7Bt%2Bk%2B1%7D%7CS_t%3Ds%7D%5Cright%5D) (1.3)

注意：状态值函数是与策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)相对应的，这是因为策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)决定了累积回报G的状态分布。

![img](https://pic2.zhimg.com/80/v2-02b0cab267c9ed16e3c292d3333e1515_hd.jpg)

图1.4 状态值函数示意图



如图1.4所示为与图1.3相对应的状态值函数图。图中白色圆圈中的数值为该状态下的值函数。即：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s_1%5Cright%29%3D-2.3%2C%5C%5C+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s_2%5Cright%29%3D-1.3%2C%5C%5C+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s_3%5Cright%29%3D2.7%2C%5C%5C%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s_4%5Cright%29%3D7.4%2C%5C%5C+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s_5%5Cright%29%3D0+)



相应地，状态-行为值函数为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3DE_%7B%5Cpi%7D%5Cleft%5B%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EkR_%7Bt%2Bk%2B1%7D%7CS_t%3Ds%2CA_t%3Da%7D%5Cright%5D+%5C%5D) (1.4)

式(1.3)和式(1.4)分别给出了状态值函数和状态-行为值函数的定义计算式，但在实际真正计算和编程的时候并不会按照定义式去编程。接下来我们会从不同的方面对定义式进行解读。

**状态值函数与状态-行为值函数的贝尔曼方程**

由状态值函数的定义式(1.3)可以得到：

![[公式]](https://www.zhihu.com/equation?tex=+%5Cupsilon%5Cleft%28s%5Cright%29%3DE%5Cleft%5BG_t%7CS_t%3Ds%5Cright%5D+%5C%5C+%3DE%5Cleft%5BR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Ccdots+%7CS_t%3Ds%5Cright%5D+%5C%5C+%3DE%5Cleft%5BR_%7Bt%2B1%7D%2B%5Cgamma%5Cleft%28R_%7Bt%2B2%7D%2B%5Cgamma+R_%7Bt%2B3%7D%2B%5Ccdots%5Cright%29%7CS_t%3Ds%5Cright%5D+%5C%5C+%3DE%5Cleft%5BR_%7Bt%2B1%7D%2B%5Cgamma+G_%7Bt%2B1%7D%7CS_t%3Ds%5Cright%5D+%5C%5C+%3DE%5Cleft%5BR_%7Bt%2B1%7D%2B%5Cgamma%5Cupsilon%5Cleft%28S_%7Bt%2B1%7D%5Cright%29%7CS_t%3Ds%5Cright%5D+) (1.5)

**最后一个等号的补充证明：**

![[公式]](https://www.zhihu.com/equation?tex=%5C%5C+V%5Cleft%28S_t%5Cright%29%3DE_%7Bs_t%2Cs_%7Bt%2B1%7D%2C%5Ccdots%7D%5Cleft%28R%5Cleft%28t%2B1%5Cright%29%2B%5Cgamma+G%5Cleft%28S_%7Bt%2B1%7D%5Cright%29%5Cright%29+%5C%5C+%3DE_%7Bs_t%7D%5Cleft%28R%5Cleft%28t%2B1%5Cright%29%2B%5Cgamma+E_%7Bs_%7Bt%2B1%7D%2C%5Ccdots%7D%5Cleft%28G%5Cleft%28S_%7Bt%2B1%7D%5Cright%29%5Cright%29%5Cright%29+%5C%5C+%3DE_%7Bs_t%7D%5Cleft%28R%5Cleft%28t%2B1%5Cright%29%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29%5Cright%29+%5C%5C+%3DE%5Cleft%28R%5Cleft%28t%2B1%5Cright%29%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29%5Cright%29+)

**需要注意的是对哪些变量求期望。**



同样我们可以得到状态-动作值函数的贝尔曼方程：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3DE_%7B%5Cpi%7D%5Cleft%5BR_%7Bt%2B1%7D%2B%5Cgamma+q%5Cleft%28S_%7Bt%2B1%7D%2CA_%7Bt%2B1%7D%5Cright%29%7CS_t%3Ds%2CA_t%3Da%5Cright%5D%5C%5C%5C%5C%5C+%5Cleft%281.6%5Cright%29+%5C%5D)

状态值函数与状态-行为值函数的具体推导过程：

图1.5和图1.6分别为状态值函数和行为值函数的具体计算过程。其中空心圆圈表示状态，实心圆圈表示状态-行为对。

![img](https://pic1.zhimg.com/80/v2-028e973c94f1e57babf693cd05392d0c_hd.jpg)

图1.5 状态值函数的计算示意图

图1.5为值函数的计算分解示意图，图1.5B计算公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3D%5Csum_%7Ba%5Cin+A%7D%7B%5Cpi%5Cleft%28a%7Cs%5Cright%29q_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%7D%5C%5C%5C%5C%5Cleft%281.7%5Cright%29+%5C%5D)

图1.5B给出了状态值函数与状态-行为值函数的关系。图1.5C计算状态-行为值函数为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3DR_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%7D%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%27%5Cright%29%5C%5C%5C%5C%5Cleft%281.8%5Cright%29+%5C%5D)

将(1.8)式带入到(1.7)式可以得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3D%5Csum_%7Ba%5Cin+A%7D%7B%5Cpi%5Cleft%28a%7Cs%5Cright%29%5Cleft%28R_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%5Cin+S%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%27%5Cright%29%7D%5Cright%29%7D%5C%5C%5C%5C%5Cleft%281.9%5Cright%29+%5C%5D)

![img](https://pic1.zhimg.com/80/v2-a74a8efcf3199cef76b7e5d41b580934_hd.jpg)

图1.6 状态-行为值函数计算

在1.6C中，

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%27%5Cright%29%3D%5Csum_%7Ba%27%5Cin+A%7D%7B%5Cpi%5Cleft%28a%27%7Cs%27%5Cright%29q_%7B%5Cpi%7D%5Cleft%28s%27%2Ca%27%5Cright%29%7D%5C%5C%5C%5C%5Cleft%281.10%5Cright%29+%5C%5D)

将(1.10)带入到(1.8)中，得到行为状态-行为值函数：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3DR_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%5Cin+S%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%5Csum_%7Ba%27%5Cin+A%7D%7B%5Cpi%5Cleft%28a%27%7Cs%27%5Cright%29q_%7B%5Cpi%7D%5Cleft%28s%27%2Ca%27%5Cright%29%7D%7D%5C%5C%5C%5C%5Cleft%281.11%5Cright%29+%5C%5D)

公式(1.9)可以在图1.4中进行验证。选择状态![[公式]](https://www.zhihu.com/equation?tex=s_4)处。由图1.4知道![[公式]](https://www.zhihu.com/equation?tex=%5Cupsilon%5Cleft%28s_4%5Cright%29%3D7.4)，由公式(1.9)得：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon%5Cleft%28s_4%5Cright%29%3D0.5%2A%5Cleft%281%2B0.2%2A%5Cleft%28-1.3%5Cright%29%2B0.4%2A2.7%2B0.4%2A7.4%5Cright%29%2B0.5%2A10%3D7.39+%5C%5D)

保留一位小数为7.4

计算状态值函数的目的是为了构建学习算法从数据中得到最优策略。每个策略对应着一个状态值函数，最优策略自然对应着最优状态值函数。

定义：最优状态值函数![[公式]](https://www.zhihu.com/equation?tex=%5Cupsilon%5E%2A%5Cleft%28s%5Cright%29),为在所有策略中值最大的值函数即：![[公式]](https://www.zhihu.com/equation?tex=%5Cupsilon%5E%2A%5Cleft%28s%5Cright%29%3D%5Cmax_%7B%5Cpi%7D%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29)，最优状态-行为值函数![[公式]](https://www.zhihu.com/equation?tex=q%5E%2A%5Cleft%28s%2Ca%5Cright%29)为在所有策略中最大的状态-行为值函数，即：

![[公式]](https://www.zhihu.com/equation?tex=q%5E%2A%5Cleft%28s%2Ca%5Cright%29%3D%5Cmax_%7B%5Cpi%7Dq_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29)

我们由(1.9)式和(1.11)式分别得到最优状态值函数和最优状态-行动值函数的贝尔曼最优方程：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon%5E%2A%5Cleft%28s%5Cright%29%3D%5Cmax_aR_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%5Cin+S%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%5Cupsilon%5E%2A%5Cleft%28s%27%5Cright%29%5C%5C%5C%5C%5Cleft%281.12%5Cright%29%7D+%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q%5E%2A%5Cleft%28s%2Ca%5Cright%29%3DR_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%5Cin+S%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%5Cmax_%7Ba%27%7Dq%5E%2A%5Cleft%28s%27%2Ca%27%5Cright%29%7D%5C%5C%5C%5C%5Cleft%281.13%5Cright%29+%5C%5D)

若已知最优状态-动作值函数，最优策略可通过直接最大化![[公式]](https://www.zhihu.com/equation?tex=q%5E%2A%5Cleft%28s%2Ca%5Cright%29) 来决定。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi_%2A%5Cleft%28a%7Cs%5Cright%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bc%7D+1%5C+if%5C+a%3D%5Cunderset%7Ba%5Cin+A%7D%7Barg%5Cmax%7Dq_%2A%5Cleft%28s%2Ca%5Cright%29%5C%5C+0%5C+otherwis%5C%5C+%5Cend%7Barray%7D%5Cright.+%5C%5D) (1.14)

![img](https://pic3.zhimg.com/80/v2-f7fb66a3f1838ed36d060d807717c35e_hd.jpg)

图1.7 最优值函数和最优策略



图1.7为最优状态值函数示意图，图中红色箭头所示的动作为最优策略。

至此，我们将强化学习的基本理论介绍完毕。现在是时候该对强化学习算法进行形式化描述了。

我们定义一个离散时间有限范围的折扣马尔科夫决策过程![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+M%3D%5Cleft%28S%2CA%2CP%2Cr%2C%5Crho_0%2C%5Cgamma+%2CT%5Cright%29+%5C%5D)，其中![[公式]](https://www.zhihu.com/equation?tex=S)为状态集，![[公式]](https://www.zhihu.com/equation?tex=A)为动作集，![[公式]](https://www.zhihu.com/equation?tex=P%3AS%5Ctimes+A%5Ctimes+S%5Crightarrow+R)是转移概率，![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+r%3AS%5Ctimes+A%5Crightarrow%5Cleft%5B-R_%7B%5Cmax%7D%2CR_%7B%5Cmax%7D%5Cright%5D+%5C%5D)为立即回报函数，![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Crho_0%3AS%5Crightarrow+R+%5C%5D)是初始状态分布，![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cgamma%5Cin%5Cleft%5B0%2C1%5Cright%5D+%5C%5D)为折扣因子，![[公式]](https://www.zhihu.com/equation?tex=T)为水平范围（其实就是步数）. ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau)为一个轨迹序列，即![[公式]](https://www.zhihu.com/equation?tex=%5Ctau+%3D%5Cleft%28s_0%2Ca_0%2Cs_1%2Ca_1%2C%5Ccdots%5Cright%29)，累积回报为![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+R%3D%5Csum_%7Bt%3D0%7D%5ET%7B%5Cgamma%5Et%7Dr_t+%5C%5D)，强化学习的目标是：找到最优策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)，使得该策略下的累积回报期望最大，即：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmax_%7B%5Cpi%7D%5Cint%7BR%5Cleft%28%5Ctau%5Cright%29%7Dp_%7B%5Cpi%7D%5Cleft%28%5Ctau%5Cright%29d%5Ctau+%5C%5D) 。

根据策略最优定理知道，当值函数最优时采取的策略也是最优的。反过来，策略最优时值函数也最优。



![img](https://pic2.zhimg.com/80/v2-6da51de9c97aa14de84c27c22cdc6895_hd.jpg)

图1.8 强化学习算法分类

如图1.8所示，强化学习算法根据以策略为中心还是以值函数最优可以分为两大类，策略优化方法和动态规划方法。其中策略优化方法又分为进化算法和策略梯度方法；动态规划方法分为策略迭代算法和值迭代算法。策略迭代算法和值迭代算法可以利用广义策略迭代方法进行统一描述。

另外，强化学习算法根据策略是否是随机的，分为确定性策略强化学习和随机性策略强化学习。根据转移概率是否已知可以分为基于模型的强化学习算法和无模型的强化学习算法。另外，强化学习算法中的回报函数![[公式]](https://www.zhihu.com/equation?tex=r)十分关键，根据回报函数是否已知，可以分为强化学习和逆向强化学习。逆向强化学习是根据专家实例将回报函数学出来。

# 强化学习入门 第二讲 基于模型的动态规划方法


  强化学习基础 第二讲 基于模型的动态规划算法

上一讲我们将强化学习的问题纳入到马尔科夫决策过程的框架下进行解决。一个完整的已知模型的马尔科夫决策过程可以利用元组![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28+%7BS%2CA%2CP%2Cr%2C%5Cgamma+%7D+%5Cright%29)来表示。其中![[公式]](https://www.zhihu.com/equation?tex=S) 为状态集，![[公式]](https://www.zhihu.com/equation?tex=A)为动作集，![[公式]](https://www.zhihu.com/equation?tex=P) 为转移概率，也就是对应着环境和智能体的模型，![[公式]](https://www.zhihu.com/equation?tex=r)为回报函数，![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma)为折扣因子用来计算累积回报![[公式]](https://www.zhihu.com/equation?tex=R)。累积回报公式为![[公式]](https://www.zhihu.com/equation?tex=R+%3D+%5Csum%5Climits_%7Bt+%3D+0%7D%5ET+%7B%7B%5Cgamma+%5Et%7D%7D+%7Br_t%7D)，其中![[公式]](https://www.zhihu.com/equation?tex=0+%5Cle+%5Cgamma+%5Cle+1)，![[公式]](https://www.zhihu.com/equation?tex=T)为有限值时，强化学习过程称为有限范围强化学习，当![[公式]](https://www.zhihu.com/equation?tex=T+%3D+%5Cinfty+) 时，称为无穷范围强化学习。我们以有限范围强化学习为例进行讲解。

强化学习的目标是找到最优策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)使得累积回报的期望最大。所谓策略是指状态到动作的映射![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%3A+s+%5Cto+a) ，用![[公式]](https://www.zhihu.com/equation?tex=%5Ctau)表示从状态![[公式]](https://www.zhihu.com/equation?tex=s) 到最终状态的一个序列![[公式]](https://www.zhihu.com/equation?tex=%5Ctau%3A+%7Bs_t%7D%2C%7Bs_%7Bt+%2B+1%7D%7D+%5Ccdots+%2C%7Bs_T%7D)，则累积回报![[公式]](https://www.zhihu.com/equation?tex=R%28%5Ctau%29) 是个随机变量，随机变量无法进行优化，无法当成是目标函数，采用随机变量的期望作为目标函数，即![[公式]](https://www.zhihu.com/equation?tex=%5Cint+%7BR%5Cleft%28+%5Ctau+%5Cright%29%7D+%7Bp_%5Cpi+%7D%5Cleft%28+%5Ctau+%5Cright%29d%5Ctau+)作为目标函数。用公式来表示强化学习的目标为：![[公式]](https://www.zhihu.com/equation?tex=%5Cmathop+%7B%5Cmax+%7D%5Climits_%5Cpi+%5Cint+%7BR%5Cleft%28+%5Ctau+%5Cright%29%7D+%7Bp_%5Cpi+%7D%5Cleft%28+%5Ctau+%5Cright%29d%5Ctau+)。强化学习的最终目标是找到最优策略为：![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cpi+%5E%2A%7D%3As+%5Cto+%7Bu%5E%2A%7D)，我们看一下这个表达式的直观含义。

![img](https://pic3.zhimg.com/80/v2-effb0dd62af4993999c18a7fca746642_hd.jpg)

图2.1 序列决策示意图

如图2.1所示，最优策略的目标是找到决策序列![[公式]](https://www.zhihu.com/equation?tex=u_0%5E%2A+%5Cto+u_1%5E%2A+%5Cto+%5Ccdots+u_T%5E%2A)，因此从广义上来讲，强化学习可以归结为序列决策问题。即找到一个决策序列，使得目标函数最优。这里的目标函数![[公式]](https://www.zhihu.com/equation?tex=%5Cint+%7BR%5Cleft%28+%5Ctau+%5Cright%29%7D+%7Bp_%5Cpi+%7D%5Cleft%28+%5Ctau+%5Cright%29d%5Ctau+)是累积回报的期望值，所谓的累积回报其背后的含义是评价策略完成任务的回报，所以目标函数等价于**任务**。强化学习的直观目标是找到最优策略，其目的是更好地完成任务。**回报函数对应着具体的任务**，所以强化学习所学到的最优策略是跟具体的任务相对应的。从这个意义上来说，强化学习并不是万能的，它无法利用一个算法实现所有的任务。

![img](https://pic4.zhimg.com/80/v2-29f8f8a836901cc000373f1e3dcf13f7_hd.jpg)

图2.2 强化学习分类

从广义上讲，强化学习是序贯决策问题。但序贯决策问题包含的内容更丰富。它不仅包含马尔科夫过程的决策，而且包括非马尔科夫过程的决策。在上一节，我们已经将强化学习纳入到马尔科夫决策过程MDP的框架之内。马尔科夫决策过程可以利用元组![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28+%7BS%2CA%2CP%2Cr%2C%5Cgamma+%7D+%5Cright%29)来描述，根据转移概率![[公式]](https://www.zhihu.com/equation?tex=P)是否已知，可以分为基于模型的动态规划方法和基于无模型的强化学习方法，如图2.2。两种类别都包括策略迭代算法，值迭代算法和策略搜索算法。不同的是无模型的强化学习方法每类算法又分为online和offline两种。Online和offline的具体含义，下一节会详细介绍。

基于模型的强化学习可以利用动态规划的思想来解决。顾名思义，动态规划中的**动态**蕴含着序列和状态的变化；**规划**蕴含着优化，如线性优化，二次优化或者非线性优化。利用动态规划可以解决的问题需要满足两个条件：（1）整个优化问题可以分解为多个子优化问题，子优化问题的解可以被存储和重复利用。上节已经讲过，强化学习可以利用马尔科夫决策过程来描述，利用贝尔曼最优性原理得到贝尔曼最优化方程：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Cupsilon+%5E%2A%7D%5Cleft%28+s+%5Cright%29+%3D+%5Cmathop+%7B%5Cmax+%7D%5Climits_a+R_s%5Ea+%2B+%5Cgamma+%5Csum%5Climits_%7Bs%27+%5Cin+S%7D+%7BP_%7Bss%27%7D%5Ea%7B%5Cupsilon+%5E%2A%7D%7D+%5Cleft%28+%7Bs%27%7D+%5Cright%29%5C%5C+%7Bq%5E%2A%7D%5Cleft%28+%7Bs%2Ca%7D+%5Cright%29+%3D+R_s%5Ea+%2B+%5Cgamma+%5Csum%5Climits_%7Bs%27+%5Cin+S%7D+%7BP_%7Bss%27%7D%5Ea%7B%7B%5Cmax+%7D_%7Ba%27%7D%7D%7Bq%5E%2A%7D%7D+%5Cleft%28+%7Bs%27%2Ca%27%7D+%5Cright%29+%5Cend%7Barray%7D) (2.1)

从方程（2.1）中可以看到，马尔科夫决策问题符合使用动态规划的两个条件，因此可以利用动态规划解决马尔科夫决策过程的问题。

贝尔曼方程（2.1）指出，动态规划的核心是找到最优值函数。那么，第一个问题是：给定一个策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)，如何计算在策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)下的值函数？其实上节已经讲过，此处再重复一遍：

![img](https://pic4.zhimg.com/80/v2-e029708460ee8e4f7d3a1174e6fb5dff_hd.jpg)

图2.3 值函数计算过程

如图2.3，红色方框内的计算公式为：

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cupsilon+_%5Cpi+%7D%5Cleft%28+s+%5Cright%29+%3D+%5Csum%5Climits_%7Ba+%5Cin+A%7D+%7B%5Cpi+%5Cleft%28+%7Ba%7Cs%7D+%5Cright%29%7Bq_%5Cpi+%7D%7D+%5Cleft%28+%7Bs%2Ca%7D+%5Cright%29%7B%5Ckern+1pt%7D+) (2.2)

该方程表示，在状态![[公式]](https://www.zhihu.com/equation?tex=s)处的值函数等于采用策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)时，所有状态-行为值函数的总和。蓝色方框的计算公式为状态-行为值函数的计算：

![[公式]](https://www.zhihu.com/equation?tex=%7Bq_%5Cpi+%7D%5Cleft%28+%7Bs%2Ca%7D+%5Cright%29+%3D+R_s%5Ea+%2B+%5Cgamma+%5Csum%5Climits_%7Bs%27%7D+%7BP_%7Bss%27%7D%5Ea%7B%5Cupsilon+_%5Cpi+%7D%7D+%5Cleft%28+%7Bs%27%7D+%5Cright%29%7B%5Ckern+1pt%7D+)![[公式]](https://www.zhihu.com/equation?tex=%7Bq_%5Cpi+%7D%5Cleft%28+%7Bs%2Ca%7D+%5Cright%29+%3D+R_s%5Ea+%2B+%5Cgamma+%5Csum%5Climits_%7Bs%27%7D+%7BP_%7Bss%27%7D%5Ea%7B%5Cupsilon+_%5Cpi+%7D%7D+%5Cleft%28+%7Bs%27%7D+%5Cright%29) (2.3)

该方程表示，在状态![[公式]](https://www.zhihu.com/equation?tex=s)采用动作![[公式]](https://www.zhihu.com/equation?tex=a)的状态值函数等于回报加上后续状态值函数。将方程（2.3）带入方程（2.2）便得到状态值函数的计算公式：

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cupsilon+_%5Cpi+%7D%5Cleft%28+s+%5Cright%29+%3D+%5Csum%5Climits_%7Ba+%5Cin+A%7D+%7B%5Cpi+%5Cleft%28+%7Ba%7Cs%7D+%5Cright%29%7D+%5Cleft%28+%7BR_s%5Ea+%2B+%5Cgamma+%5Csum%5Climits_%7Bs%27+%5Cin+S%7D+%7BP_%7Bss%27%7D%5Ea%7B%5Cupsilon+_%5Cpi+%7D%5Cleft%28+%7Bs%27%7D+%5Cright%29%7D+%7D+%5Cright%29%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+) (2.4)

状态![[公式]](https://www.zhihu.com/equation?tex=s)处的值函数![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cupsilon+_%5Cpi+%7D%5Cleft%28+s+%5Cright%29)，可以利用后继状态的值函数![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cupsilon+_%5Cpi+%7D%5Cleft%28+%7Bs%27%7D+%5Cright%29)来表示。可是有人会说，后继状态的值函数![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cupsilon+_%5Cpi+%7D%5Cleft%28+%7Bs%27%7D+%5Cright%29)也是不知道的，那怎么计算当前状态的值函数？这不是自己抬起自己吗？如图2.4所示。没错，这正是bootstrapping算法(自举算法)。

![img](https://pic3.zhimg.com/80/v2-44a947426948bb957d711dd44dcf5c16_hd.jpg)



2.4 自举示意图

[Bootstrapping and Artificial Intelligence](https://link.zhihu.com/?target=https%3A//omarsbrain.wordpress.com/2010/01/22/bootstrapping-and-artificial-intelligence/)

如何求解（2.4）所示的方程？首先，我们从数学的角度去解释方程(2.4)。对于模型已知的强化学习算法，方程(2.4)中的![[公式]](https://www.zhihu.com/equation?tex=P_%7Bss%27%7D%5Ea)，![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma)和![[公式]](https://www.zhihu.com/equation?tex=%7BR_s%7D%5Ea)都是已知数，![[公式]](https://www.zhihu.com/equation?tex=%5Cpi+%5Cleft%28+%7Ba%7Cs%7D+%5Cright%29)为要评估的策略是指定的，也是已知值。方程（2.4）中唯一的未知数是值函数，从这个角度去理解方程（2.4）可知，方程（2.4）是关于值函数的线性方程组，其未知数的个数为状态的总数，用![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%7C+S+%5Cright%7C)来表示。此处，我们使用高斯-赛德尔迭代算法进行求解。即：

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cupsilon+_%7Bk+%2B+1%7D%7D%5Cleft%28+s+%5Cright%29+%3D+%5Csum%5Climits_%7Ba+%5Cin+A%7D+%7B%5Cpi+%5Cleft%28+%7Ba%7Cs%7D+%5Cright%29%7D+%5Cleft%28+%7BR_s%5Ea+%2B+%5Cgamma+%5Csum%5Climits_%7Bs%27+%5Cin+S%7D+%7BP_%7Bss%27%7D%5Ea%7B%5Cupsilon+_k%7D%5Cleft%28+%7Bs%27%7D+%5Cright%29%7D+%7D+%5Cright%29) (2.5)

下面我们给出策略评估算法的伪代码：

![img](https://pic4.zhimg.com/80/v2-d1efb5d39530ea1db507e340496cdb5b_hd.jpg)

图2.5 策略评估算法

如图2.5所示为策略评估算法的伪代码。需要注意的是，在每次迭代中都需要对状态集进行一次遍历（扫描）以便评估每个状态的值函数。

接下来，我们举个策略评估的例子：

如图2.6所示为网格世界，其状态空间为：![[公式]](https://www.zhihu.com/equation?tex=S+%3D+%5Cleft%5C%7B+%7B1%2C2%2C+%5Ccdots+%2C14%7D+%5Cright%5C%7D)，动作空间 A={东，南，西，北}，回报函数为![[公式]](https://www.zhihu.com/equation?tex=r%5Cequiv+-1),需要评估的策略为均匀随机策略：π(东|⋅)= 0.25， π(南|⋅)= 0.25， π(西|⋅)=0.25， π(北|⋅)=
0.25.

![img](https://pic2.zhimg.com/80/v2-66b17296cc95efc463e840733ff09fe1_hd.jpg)

图2.6 网格世界

![img](https://pic2.zhimg.com/80/v2-b4f557fcc5ecf0e84d3510c7f2ac63a9_hd.jpg)

图2.7 值函数迭代中间图

图2.7为值函数迭代过程值函数的变化。为了进一步说明，我们举个具体地例子，如从K=1到K=2时，状态1处的值函数计算过程。

由公式2.5得到：

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cupsilon+_2%7D%5Cleft%28+1+%5Cright%29%7B%5Crm%7B+%3D+%7D%7D0.25%7B%5Crm%7B%2A%7D%7D%5Cleft%28+%7B%7B%5Crm%7B+-+%7D%7D1%7B%5Crm%7B+-+%7D%7D1%7D+%5Cright%29%7B%5Crm%7B+%2B+%7D%7D0.25%7B%5Crm%7B%2A%7D%7D%5Cleft%28+%7B%7B%5Crm%7B+-+%7D%7D1%7B%5Crm%7B+-+%7D%7D1%7D+%5Cright%29+%2B+0.25%7B%5Crm%7B%2A%7D%7D%5Cleft%28+%7B%7B%5Crm%7B+-+%7D%7D1%7B%5Crm%7B+-+%7D%7D1%7D+%5Cright%29+%2B+0.25%7B%5Crm%7B%2A%7D%7D%5Cleft%28+%7B0%7B%5Crm%7B+-+%7D%7D1%7D+%5Cright%29%7B%5Crm%7B+%3D+%7D%7D0.25%7B%5Crm%7B%2A%7D%7D%5Cleft%28+%7B%7B%5Crm%7B+-+%7D%7D7%7D+%5Cright%29%7B%5Crm%7B+%3D+-+%7D%7D1.75)

保留两位有效数字便是-1.7。

计算值函数的目的是利用值函数找到最优策略。第二个要解决的问题是：如何利用值函数进行策略改善，从而得到最优策略？

一个很自然的方法是当已知当前策略的值函数时，在每个状态采用贪婪策略对当前策略进行改进，即：![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cpi+_%7Bl+%2B+1%7D%7D%5Cleft%28+s+%5Cright%29+%5Cin+%5Cmathop+%7B%5Carg+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%5Cmax+%7D%5Climits_a+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7Bq%5E%7B%7B%5Cpi+_l%7D%7D%7D%5Cleft%28+%7Bs%2Ca%7D+%5Cright%29)

![img](https://pic4.zhimg.com/80/v2-598893f12bdc2ed65d409d1df79294fb_hd.jpg)

图2.8 贪婪策略计算

如图2.8给出了贪婪策略示意图。图中红线为最优策略选择。

![img](https://pic3.zhimg.com/80/v2-9070e6f49495a0b1c4c551cd90fa3896_hd.jpg)

图2.9 方格世界贪婪策略选取

如图2.9所示为方格世界贪婪策略的示意图。我们仍然以状态1为例得到改进的贪婪策略：

![img](https://pic4.zhimg.com/80/v2-ee1147d66d9c27f637c75d8b3f97c9bf_hd.jpg)

至此，我们已经给出了策略评估算法和策略改进算法。万事已具备，将策略评估算法和策略改进算合起来便组成了策略迭代算法。

![img](https://pic1.zhimg.com/80/v2-54619a6b75d3c82860f12d5ad14e2c68_hd.jpg)

图2.10 策略迭代算法

策略迭代算法包括策略评估和策略改进两个步骤。在策略评估中，给定策略，通过数值迭代算法不断计算该策略下每个状态的值函数，利用该值函数和贪婪策略得到新的策略。如此循环下去，最终得到最优策略。这是一个策略收敛的过程。

![img](https://pic1.zhimg.com/80/v2-86ecd409577876582ead5154570dee18_hd.jpg)

图2.11 值函数收敛

如图2.11 所示为值函数收敛过程，通过策略评估和策略改进得到最优值函数。从策略迭代的伪代码我们看到，进行策略改进之前需要得到收敛的值函数。值函数的收敛往往需要很多次迭代，现在的问题是进行策略改进之前一定要等到策略值函数收敛吗？

对于这个问题，我们还是先看一个例子：

![img](https://pic1.zhimg.com/80/v2-461b10fdff95f2e830f4c359d99a0590_hd.jpg)

图2.12 策略改进

如图2.12所示，策略评估迭代10次和迭代无穷次所得到的贪婪策略是一样的。因此，对于上面的问题，我们的回答是不一定等到策略评估算法完全收敛。如果我们在进行一次评估之后就进行策略改善，则称为值函数迭代算法。

值函数迭代算法的伪代码为：

![img](https://pic1.zhimg.com/80/v2-165d1458f90d34652dbd3d6fd7134e48_hd.jpg)

图2.13 值函数迭代算法

如图2.13为值函数迭代算法。需要注意的是在每次迭代过程，需要对状态空间进行一次扫描，同时在每个状态对动作空间进行扫描以便得到贪婪的策略。

值函数迭代是动态规划算法最一般的计算框架，我们阐述最优控制理论与值函数迭代之间的联系。解决最优控制的问题往往有三种思路：变分法原理、庞特里亚金最大值原理和动态规划的方法。三种方法各有优缺点。

基于变分法的方法是最早的方法，其局限性是无法求解带有约束的优化问题。基于庞特里亚金最大值原理的方法在变分法基础上进行发展，可以解决带约束的优化问题。相比于这两种经典的方法，动态规划的方法相对独立，主要是利用贝尔曼最优性原理。

对于一个连续系统，往往有一组状态方程来描述：

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Crm%7B%5Cdot+X+%3D+%7D%7Df%5Cleft%28+%7Bt%2CX%2CU%7D+%5Cright%29%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+X%5Cleft%28+%7B%7Bt_0%7D%7D+%5Cright%29+%3D+%7BX_0%7D) (2.6)

性能指标往往由积分给出：

![[公式]](https://www.zhihu.com/equation?tex=J%5Cleft%5B+%7Bx%5Cleft%28+%7B%7Bt_0%7D%7D+%5Cright%29%2C%7Bt_0%7D%7D+%5Cright%5D+%3D+%5Cphi+%5Cleft%5B+%7BX%5Cleft%28+%7B%7Bt_f%7D%7D+%5Cright%29%2C%7Bt_f%7D%7D+%5Cright%5D+%2B+%5Cint_%7B%7Bt_0%7D%7D%5E%7B%7Bt_f%7D%7D+%7BL%5Cleft%28+%7Bx%5Cleft%28+t+%5Cright%29%2Cu%5Cleft%28+t+%5Cright%29%2Ct%7D+%5Cright%29%7D+) (2.7)

最优控制的问题是：

![[公式]](https://www.zhihu.com/equation?tex=V%5Cleft%28+%7BX%2Ct%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bu+%5Cin+%5COmega+%7D+%5Cleft%5C%7B+%7B%5Cphi+%5Cleft%5B+%7BX%5Cleft%28+%7B%7Bt_f%7D%7D+%5Cright%29%2C%7Bt_f%7D%7D+%5Cright%5D+%2B+%5Cint_%7B%7Bt_0%7D%7D%5E%7B%7Bt_f%7D%7D+%7BL%5Cleft%28+%7Bx%5Cleft%28+t+%5Cright%29%2Cu%5Cleft%28+t+%5Cright%29%2Ct%7D+%5Cright%29%7D+%7D+%5Cright%5C%7D) (2.8)

由贝尔曼最优性原理得到哈密尔顿-雅克比-贝尔曼方程：

![[公式]](https://www.zhihu.com/equation?tex=+-+%7B%7B%5Cpartial+V%7D+%5Cover+%7B%5Cpartial+t%7D%7D+%3D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bu%5Cleft%28+t+%5Cright%29+%5Cin+U%7D+%5Cleft%5C%7B+%7BL%5Cleft%28+%7Bx%5Cleft%28+t+%5Cright%29%2Cu%5Cleft%28+t+%5Cright%29%2Ct%7D+%5Cright%29+%2B+%7B%7B%5Cpartial+V%7D+%5Cover+%7B%5Cpartial+%7BX%5ET%7D%7D%7Df%5Cleft%5B+%7Bx%5Cleft%28+t+%5Cright%29%2Cu%5Cleft%28+t+%5Cright%29%2Ct%7D+%5Cright%5D%7D+%5Cright%5C%7D) (2.9)

方程(2.9)是一个偏微分方程，一般不存在解析解。对于偏微分方程(2.9)，有三种解决思路：（1）将值函数进行离散，求解方程（2.9）的数值解，然后利用贪婪策略得到最优控制。这对应于求解微分方程的数值求解方法。（2）第二个思路是利用变分法，将微分方程转化成变分代数方程，在标称轨迹展开，得到微分动态规划DDP。（3）利用函数逼近理论，对方程中的回报函数和控制函数进行函数逼近，利用优化方法得到逼近系数，这类方法称为伪谱的方法。

前两种方法都是以值函数为中心，其思路与值函数迭代类似，我们在此介绍前两种方法。



HJB方程的数值算法：

![img](https://pic4.zhimg.com/80/v2-650f07f5c4e49e00382ee2365162410f_hd.jpg)

图2.14 平面移动机器人移动规划 [1]

如图2.14为平面移动机器人的移动规划部分的代码，从代码中我们看到，如图2.14红色框内节点处的值进行计算时选取的也是最小值函数。



第二个思路是采用变分法。下面我们给出DDP方法的具体推导公式和伪代码：

由贝尔曼最优性原理得：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgathered%7D+V%5Cleft%28+%7BX%2Ct%7D+%5Cright%29+%3D+%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bu+%5Cin+%5COmega+%7D+%5Cleft%5C%7B+%7B%5Cphi+%5Cleft%5B+%7BX%5Cleft%28+%7B%7Bt_f%7D%7D+%5Cright%29%2C%7Bt_f%7D%7D+%5Cright%5D+%2B+%5Cint_%7B%7Bt_0%7D%7D%5E%7B%7Bt_f%7D%7D+%7BL%5Cleft%28+%7Bx%5Cleft%28+t+%5Cright%29%2Cu%5Cleft%28+t+%5Cright%29%2Ct%7D+%5Cright%29dt%7D+%7D+%5Cright%5C%7D+%5Chfill+%5C%5C+%7B%5Ctext%7B+%3D+%7D%7D%5Cmathop+%7B%5Cmin+%7D%5Climits_%7Bu+%5Cin+%5COmega+%7D+%5Cleft%5C%7B+%7B%5Cint_%7B%7Bt_0%7D%7D%5E%7B%7Bt_0%7D+%2B+dt%7D+%7BL%5Cleft%28+%7Bx%5Cleft%28+%5Ctau+%5Cright%29%2Cu%5Cleft%28+%5Ctau+%5Cright%29%2C%5Ctau+%7D+%5Cright%29d%5Ctau+%2B+V%5Cleft%28+%7BX+%2B+%5CDelta+X%2Ct+%2B+dt%7D+%5Cright%29%7D+%7D+%5Cright%5C%7D+%5Chfill+%5C%5C+%5Cend%7Bgathered%7D+) (2.10)

假设：![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgathered%7D+%7Bx_%7Bk+%2B+1%7D%7D+%3D+f%5Cleft%28+%7B%7Bx_k%7D%2C%7Bu_k%7D%7D+%5Cright%29+%5Chfill+%5C%5C+%7BV_k%7D+%3D+%5Cmathop+%7B%5Cmin+%7D%5Climits_u+%5Cleft%5B+%7Bl%5Cleft%28+%7B%7Bx_k%7D%2C%7Bu_k%7D%7D+%5Cright%29+%2B+%7BV_%7Bk+%2B+1%7D%7D%5Cleft%28+%7B%7Bx_%7Bk+%2B+1%7D%7D%7D+%5Cright%29%7D+%5Cright%5D+%5Chfill+%5C%5C+%5Cend%7Bgathered%7D+) 令![[公式]](https://www.zhihu.com/equation?tex=Q%5Cleft%28+%7B%5Cdelta+x%2C%5Cdelta+u%7D+%5Cright%29+%3D+V%5Cleft%28+%7Bx+%2B+%5Cdelta+x%7D+%5Cright%29+-+V%5Cleft%28+x+%5Cright%29)，则![[公式]](https://www.zhihu.com/equation?tex=Q)

在标称轨迹![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28+%7B%7Bx_k%7D%2C%7Bu_k%7D%7D+%5Cright%29)展开：![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgathered%7D+Q%5Cleft%28+%7B%5Cdelta+x%2C%5Cdelta+u%7D+%5Cright%29+%3D+V%5Cleft%28+%7Bx+%2B+%5Cdelta+x%7D+%5Cright%29+-+V%5Cleft%28+x+%5Cright%29+%5Chfill+%5C%5C+%7B%5Ctext%7B+%3D+%7D%7Dl%5Cleft%28+%7B%7Bx_k%7D+%2B+%5Cdelta+%7Bx_k%7D%2C%7Bu_k%7D+%2B+%5Cdelta+%7Bu_k%7D%7D+%5Cright%29+%2B+%7BV_%7Bk+%2B+1%7D%7D%5Cleft%28+%7B%7Bx_%7Bk+%2B+1%7D%7D+%2B+%5Cdelta+%7Bx_%7Bk+%2B+1%7D%7D%7D+%5Cright%29+-+%5Cleft%28+%7Bl%5Cleft%28+%7B%7Bx_k%7D%2C%7Bu_k%7D%7D+%5Cright%29+%2B+%7BV_%7Bk+%2B+1%7D%7D%5Cleft%28+%7B%7Bx_%7Bk+%2B+1%7D%7D%7D+%5Cright%29%7D+%5Cright%29+%5Chfill+%5C%5C+%5Capprox+%5Cdelta+x_k%5ET%7Bl_%7B%7Bx_k%7D%7D%7D+%2B+%5Cdelta+u_k%5ET%7Bl_%7B%7Bu_k%7D%7D%7D+%2B+%5Cfrac%7B1%7D%7B2%7D%5Cleft%28+%7B%5Cdelta+x_k%5ET%7Bl_%7Bx%7Bx_k%7D%7D%7D%5Cdelta+%7Bx_k%7D+%2B+2%5Cdelta+x_k%5ET%7Bl_%7Bx%7Bu_k%7D%7D%7D%5Cdelta+%7Bu_k%7D+%2B+%5Cdelta+u_k%5ET%7Bl_%7Bu%7Bu_k%7D%7D%7D%5Cdelta+%7Bu_k%7D%7D+%5Cright%29+%2B+%5Cdelta+x_%7Bk+%2B+1%7D%5ET%7BV_%7B%7Bx_%7Bk+%2B+1%7D%7D%7D%7D+%2B+%5Cfrac%7B1%7D%7B2%7D%5Cdelta+x_%7Bk+%2B+1%7D%5ET%7BV_%7Bx%7Bx_%7Bk+%2B+1%7D%7D%7D%7D%5Cdelta+%7Bx_%7Bk+%2B+1%7D%7D+%5Chfill+%5C%5C+%5Cend%7Bgathered%7D+)(2.11)

又![[公式]](https://www.zhihu.com/equation?tex=%7Bx_%7Bk+%2B+1%7D%7D+%3D+f%5Cleft%28+%7B%7Bx_k%7D%2C%7Bu_k%7D%7D+%5Cright%29)，得到：![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta+%7Bx_%7Bk+%2B+1%7D%7D+%3D+%5Cdelta+f%5Cleft%28+%7B%7Bx_k%7D%2C%7Bu_k%7D%7D+%5Cright%29+%3D+%7Bf_%7B%7Bx_k%7D%7D%7D%5Cdelta+%7Bx_k%7D+%2B+%7Bf_%7B%7Bu_k%7D%7D%7D%5Cdelta+%7Bu_k%7D+%2B+%5Cfrac%7B1%7D%7B2%7D%5Cleft%28+%7B%5Cdelta+x_k%5ET%7Bf_%7Bx%7Bx_k%7D%7D%7D%5Cdelta+x+%2B+2%5Cdelta+x_k%5ET%7Bf_%7Bx%7Bu_k%7D%7D%7D%5Cdelta+%7Bu_k%7D+%2B+%5Cdelta+u_k%5ET%7Bf_%7Bu%7Bu_k%7D%7D%7D%5Cdelta+u%7D+%5Cright%29)（2.12）将(2.12)带入到（2.11）可以得到：

![[公式]](https://www.zhihu.com/equation?tex=Q%5Cleft%28+%7B%5Cdelta+x%2C%5Cdelta+u%7D+%5Cright%29+%3D+%5Cfrac%7B1%7D%7B2%7D%5Cleft%5B+%7B%5Cdelta+%7Bu%5ET%7D%7BQ_%7Buu%7D%7D%5Cdelta+u+%2B+%5Cleft%28+%7B%5Cdelta+%7Bu%5ET%7D%7BQ_%7Bux%7D%7D%5Cdelta+x+%2B+%5Cdelta+%7Bx%5ET%7D%7BQ_%7Bxu%7D%7D%5Cdelta+u%7D+%5Cright%29+%2B+Q_u%5ET%5Cdelta+u+%2B+%5Cdelta+%7Bu%5ET%7D%7BQ_u%7D+%2B+%5Cdelta+%7Bx%5ET%7D%7BQ_%7Bxx%7D%7D%5Cdelta+x+%2B+%5Cdelta+%7Bx%5ET%7D%7BQ_x%7D+%2B+Q_x%5ET%5Cdelta+x%7D+%5Cright%5D)
（2.13）

其中：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgathered%7D+%7BQ_x%7D+%3D+%7Bl_%7B%7Bx_k%7D%7D%7D+%2B+f_%7B%7Bx_k%7D%7D%5ET%7BV_%7B%7Bx_%7Bk+%2B+1%7D%7D%7D%7D+%5Chfill+%5C%5C+%7BQ_u%7D+%3D+%7Bl_%7B%7Bu_k%7D%7D%7D+%2B+f_%7B%7Bu_k%7D%7D%5ET%7BV_%7B%7Bx_%7Bk+%2B+1%7D%7D%7D%7D+%5Chfill+%5C%5C+%7BQ_%7Bxx%7D%7D+%3D+%7Bl_%7Bx%7Bx_k%7D%7D%7D+%2B+f_%7B%7Bx_k%7D%7D%5ET%7BV_%7Bx%7Bx_k%7D%7D%7D%7Bf_%7B%7Bx_k%7D%7D%7D+%2B+%7BV_%7B%7Bx_%7Bk+%2B+1%7D%7D%7D%7D%7Bf_%7B%7Bx_k%7D%7Bx_k%7D%7D%7D+%5Chfill+%5C%5C+%7BQ_%7Buu%7D%7D+%3D+%7Bl_%7Bu%7Bu_k%7D%7D%7D+%2B+f_%7B%7Bu_k%7D%7D%5ET%7BV_%7Bx%7Bx_%7Bk+%2B+1%7D%7D%7D%7D%7Bf_%7B%7Bu_k%7D%7D%7D+%2B+%7BV_%7B%7Bx_%7Bk+%2B+1%7D%7D%7D%7D%7Bf_%7B%7Bu_k%7D%7Bu_k%7D%7D%7D+%5Chfill+%5C%5C+%7BQ_%7Bux%7D%7D+%3D+%7Bl_%7Bu%7Bx_k%7D%7D%7D+%2B+f_%7B%7Bu_k%7D%7D%5ET%7BV_%7Bx%7Bx_%7Bk+%2B+1%7D%7D%7D%7D%7Bf_%7B%7Bx_k%7D%7D%7D+%2B+%7BV_%7B%7Bx_%7Bk+%2B+1%7D%7D%7D%7D%7Bf_%7Bu%7Bx_k%7D%7D%7D+%5Chfill+%5C%5C+%5Cend%7Bgathered%7D+) （2.14）

将（2.13）视为![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta+u)的函数，则![[公式]](https://www.zhihu.com/equation?tex=Q%5Cleft%28+%7B%5Cdelta+x%2C%5Cdelta+u%7D+%5Cright%29)是![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta+u)的二次函数。

![img](https://pic3.zhimg.com/80/v2-2c11cb56eaa225eb1c397d0e9f001f8a_hd.jpg)

图2.15 值函数变分函数

如图2.15所示，![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta+%7Bu%5E%2A%7D+%3D+%5Cmathop+%7B%5Carg+%5Cmin+%7D%5Climits_%7B%5Cdelta+u%7D+Q%5Cleft%28+%7B%5Cdelta+x%2C%5Cdelta+u%7D+%5Cright%29+%3D+-+Q_%7Buu%7D%5E%7B+-+1%7D%5Cleft%28+%7B%7BQ_u%7D+%2B+%7BQ_%7Bux%7D%7D%5Cdelta+x%7D+%5Cright%29)。我们令

![[公式]](https://www.zhihu.com/equation?tex=k+%3D+-+Q_%7Buu%7D%5E%7B+-+1%7D%7BQ_u%7D%2C%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+%7B%5Ckern+1pt%7D+K+%3D+-+Q_%7Buu%7D%5E%7B+-+1%7D%7BQ_%7Bux%7D%7D) （2.15）

则![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta+%7Bu%5E%2A%7D+%3D+%5Cmathop+%7B%5Carg+%5Cmin+%7D%5Climits_%7B%5Cdelta+u%7D+Q%5Cleft%28+%7B%5Cdelta+x%2C%5Cdelta+u%7D+%5Cright%29+%3D+k+%2B+K%5Cdelta+x)

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgathered%7D+%5CDelta+V+%3D+-+%5Cfrac%7B1%7D%7B2%7D%7BQ_u%7DQ_%7Buu%7D%5E%7B+-+1%7D%7BQ_u%7D+%5Chfill+%5C%5C+%7BV_%7B%7Bx_k%7D%7D%7D+%3D+%7BQ_x%7D+-+%7BQ_u%7DQ_%7Buu%7D%5E%7B+-+1%7D%7BQ_%7Bux%7D%7D+%5Chfill+%5C%5C+%7BV_%7Bx%7Bx_k%7D%7D%7D+%3D+%7BQ_%7Bxx%7D%7D+-+%7BQ_%7Bxu%7D%7DQ_%7Buu%7D%5E%7B+-+1%7D%7BQ_%7Bux%7D%7D+%5Chfill+%5C%5C+%5Cend%7Bgathered%7D+) （2.16）

微分动态规划的伪代码为：

1.
前向迭代：给定初始控制序列![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cbar+u_k%7D)，正向迭代计算标称轨迹

![[公式]](https://www.zhihu.com/equation?tex=%7B%5Cbar+x_%7Bk+%2B+1%7D%7D+%3D+f%5Cleft%28+%7B%7B%7B%5Cbar+x%7D_k%7D%2C%7B%7B%5Cbar+u%7D_k%7D%7D+%5Cright%29%2C%7Bl_%7B%7Bx_k%7D%7D%7D%2C%7Bf_%7B%7Bu_k%7D%7D%7D%2C%7Bl_%7Bx%7Bx_k%7D%7D%7D%2C%7Bl_%7Bx%7Bu_k%7D%7D%7D%2C%7Bl_%7Bu%7Bu_k%7D%7D%7D)
2.
反向迭代：由代价函数边界条件![[公式]](https://www.zhihu.com/equation?tex=%7BV_%7B%7Bx_N%7D%7D%7D%2C%7BV_%7Bx%7Bx_N%7D%7D%7D)，反向迭代计算（2.14），（2.15）和（2.16）得到贪婪策略![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta+%7Bu%5E%2A%7D)

3.
正向迭代新的控制序列：

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Bgathered%7D+%7Bx_1%7D+%3D+%5Cbar+x%5Cleft%28+1+%5Cright%29+%5Chfill+%5C%5C+%7Bu_k%7D+%3D+%7B%7B%5Cbar+u%7D_k%7D+%2B+%7Bk_k%7D+%2B+%7BK_k%7D%5Cleft%28+%7B%7Bx_k%7D+-+%7B%7B%5Cbar+x%7D_k%7D%7D+%5Cright%29+%5Chfill+%5C%5C+%7Bx_%7Bk+%2B+1%7D%7D+%3D+f%5Cleft%28+%7B%7Bx_k%7D%2C%7Bu_k%7D%7D+%5Cright%29+%5Chfill+%5C%5C+%5Cend%7Bgathered%7D+)

从第二步反向迭代计算贪婪策略 的过程我们可以看到，贪婪策略通过最小化值函数得到。

# 强化学习入门 第三讲 蒙特卡罗方法


  强化学习基础 第三讲 蒙特卡罗方法

上一节课我们讲了已知模型时，利用动态规划的方法求解马尔科夫决策问题。从这节课开始，我们讲无模型的强化学习算法。

![img](https://pic2.zhimg.com/80/v2-046b7405293b6a6957ac0ab212df4a41_hd.jpg)

图3.1 强化学习方法分类

解决无模型的马尔科夫决策问题是强化学习算法的精髓。如图3.1所示，无模型的强化学习算法主要包括蒙特卡罗方法和时间差分方法。这一节我们先讲蒙特卡罗的方法。

在讲解蒙特卡罗方法之前，先梳理一下整个强化学习研究思路。首先强化学习问题可以纳入到马尔科夫决策过程中，这方面的知识已经在第一讲给出。若已知模型时，马尔科夫决策过程可以利用动态规划的方法来解决。之所以讲动态规划的方法是因为动态规划的思想是无模型强化学习研究的根源。在上一讲中，我们讲了动态规划的方法包括策略迭代、值迭代。这两种方法可以用广义策略迭代方法来统一：即先对当前策略进行策略评估，也就是说计算出当前策略所对应的值函数；然后，利用值函数改进当前策略。无模型的强化学习基本思想也是如此，即：策略评估和策略改善。

![img](https://pic3.zhimg.com/80/v2-2e9aa7291e0c67448a29029d5780ceaa_hd.jpg)

图3.2 值函数计算图

在动态规划的方法中，值函数的计算方法如图3.2为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3D%5Csum_%7Ba%5Cin+A%7D%7B%5Cpi%5Cleft%28a%7Cs%5Cright%29%7D%5Cleft%28R_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%5Cin+S%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%27%5Cright%29%7D%5Cright%29%5C%5C%5C%5C%5Cleft%283.1%5Cright%29+%5C%5D)

动态规划方法计算状态处的值函数时利用了模型![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+P_%7Bss%27%7D%5E%7Ba%7D+%5C%5D) 而在无模型强化学习中，模型![[公式]](https://www.zhihu.com/equation?tex=P_%7Bss%27%7D%5E%7Ba%7D+)是未知的。无模型的强化学习算法要想利用策略评估和策略改善的框架，必须采用其他的方法对当前策略进行评估（计算值函数）。

我们回到值函数最原始的定义公式，在第一讲中，我们已经给出了值函数的定义：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3DE_%7B%5Cpi%7D%5Cleft%5BG_t%7CS_t%3Ds%5Cright%5D%3DE_%7B%5Cpi%7D%5Cleft%5B%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EkR_%7Bt%2Bk%2B1%7D%7CS_t%3Ds%7D%5Cright%5D%5C%5C%5C+%5Cleft%283.2%5Cright%29+%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3DE_%7B%5Cpi%7D%5Cleft%5B%5Csum_%7Bk%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EkR_%7Bt%2Bk%2B1%7D%7CS_t%3Ds%2CA_t%3Da%7D%5Cright%5D%5C%5C%5C+%5Cleft%283.3%5Cright%29+%5C%5D)

状态值函数和行为值函数的计算实际上是计算返回值的期望。如图3.2，动态规划的方法是利用模型对该期望进行计算。在没有模型时，我们可以采用蒙特卡罗的方法计算该期望，即利用随机样本来估计期望。在计算值函数时，蒙特卡罗方法是利用**经验平均**代替随机变量的期望。此处，我们要理解两个词，何为经验？何为平均。

首先看何为经验：

当要评估智能体的当前策略时，我们可以利用策略产生很多次试验，每次试验都是从任意的初始状态开始直到终止状态，比如一次试验(an
episode)为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+S_1%2CA_1%2CR_2%2C%5Ccdots+%2CS_T+%5C%5D) 计算一次试验中状态![[公式]](https://www.zhihu.com/equation?tex=s)处的折扣回报返回值为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_t%5Cleft%28s%5Cright%29%3DR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Ccdots+%2B%5Cgamma%5E%7BT-1%7DR_T+%5C%5D)

所谓经验，是指利用该策略做很多次试验，产生很多幕数据。这里一幕是一次试验的意思。如图3.3所示

![img](https://pic1.zhimg.com/80/v2-0de32a6bd8453cb7e56d434373eee9cc_hd.jpg)

图3.3
蒙特卡罗中的经验

再来看什么是平均：

这个概念很简单，平均就是求均值。不过，利用蒙特卡罗方法求状态![[公式]](https://www.zhihu.com/equation?tex=s)处的值函数时，又可以分为第一次访问蒙特卡罗方法和每次访问蒙特卡罗方法。

第一次访问蒙特卡罗方法是指，在计算状态s处值函数时，只利用每次试验中第一次访问到状态s时的返回值。如图3.3中第一次试验所示，计算状态![[公式]](https://www.zhihu.com/equation?tex=s)处的均值时只利用![[公式]](https://www.zhihu.com/equation?tex=G_%7B11%7D) 。因此第一次访问蒙特卡罗方法的计算公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon%5Cleft%28s%5Cright%29%3D%5Cfrac%7BG_%7B11%7D%5Cleft%28s%5Cright%29%2BG_%7B21%7D%5Cleft%28s%5Cright%29%2B%5Ccdots%7D%7BN%5Cleft%28s%5Cright%29%7D+%5C%5D)

每次访问蒙特卡罗方法是指，在计算状态![[公式]](https://www.zhihu.com/equation?tex=s)处的值函数时，利用所有访问到状态![[公式]](https://www.zhihu.com/equation?tex=s)时的回报返回值，即：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon%5Cleft%28s%5Cright%29%3D%5Cfrac%7BG_%7B11%7D%5Cleft%28s%5Cright%29%2BG_%7B12%7D%5Cleft%28s%5Cright%29%2B%5Ccdots+%2BG_%7B21%7D%5Cleft%28s%5Cright%29%2B%5Ccdots%7D%7BN%5Cleft%28s%5Cright%29%7D+%5C%5D)

根据大数定律： ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cupsilon%5Cleft%28s%5Cright%29%5Crightarrow%5Cupsilon_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%5C+as%5C+N%5Cleft%28s%5Cright%29%5Crightarrow%5Cinfty+%5C%5D)

**探索的必要性：**

由于不知道智能体与环境交互的模型，蒙特卡罗方法是利用经验平均来估计值函数。能否得到正确的值函数，取决于经验。**如何获得充足的经验是无模型强化学习的核心所在**。

在动态规划方法中，为了保证值函数的收敛性，算法会对状态空间中的状态进行逐个扫描。无模型的方法充分评估策略值函数的前提是每个状态都能被访问到。因此，在蒙特卡洛方法中必须采用一定的方法保证每个状态都能被访问到。其中一种方法是**探索性初始化**。

**探索性初始化：**

所谓探索性初始化是指每个状态都有一定的几率作为初始状态。在给出基于探索性初始化的蒙特卡罗方法前，我们还需要给出策略改进方法，以及便于进行迭代计算的平均方法。

**蒙特卡罗策略改进：**

蒙特卡罗方法利用经验平均对策略值函数进行估计。当值函数被估计出来后，对于每个状态![[公式]](https://www.zhihu.com/equation?tex=s) ，通过最大化动作值函数，来进行策略的改进。即： ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28s%5Cright%29%3D%5Cunderset%7Ba%7D%7Barg%5Cmax%5Ctextrm%7B%5C+%7D%7Dq%5Cleft%28s%2Ca%5Cright%29+%5C%5D)

**递增计算平均的方法：**

![[公式]](https://www.zhihu.com/equation?tex=+%5Cupsilon_k%5Cleft%28s%5Cright%29%3D%5Cfrac%7B1%7D%7Bk%7D%5Csum_%7Bj%3D1%7D%5Ek%7BG_j%7D%5Cleft%28s%5Cright%29+%5C%5C%5C%5C%5C+%3D%5Cfrac%7B1%7D%7Bk%7D%5Cleft%28G_k%5Cleft%28s%5Cright%29%2B%5Csum_%7Bj%3D1%7D%5E%7Bk-1%7D%7BG_j%7D%5Cleft%28s%5Cright%29%5Cright%29%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C+%5C%5C+%5C%5C%5C%5C+%3D%5Cfrac%7B1%7D%7Bk%7D%5Cleft%28G_k%5Cleft%28s%5Cright%29%2B%5Cleft%28k-1%5Cright%29%5Cupsilon_%7Bk-1%7D%5Cleft%28s%5Cright%29%5Cright%29+%5C%5C+%5C%5C%5C%5C+%3D%5Cupsilon_%7Bk-1%7D%5Cleft%28s%5Cright%29%2B%5Cfrac%7B1%7D%7Bk%7D%5Cleft%28G_k%5Cleft%28s%5Cright%29-%5Cupsilon_%7Bk-1%7D%5Cleft%28s%5Cright%29%5Cright%29+%5C%5D)(3.4)

![img](https://pic3.zhimg.com/80/v2-08b4dbca6cb3464522f7f9bb04c67d66_hd.jpg)

图3.4 探索性初始化蒙特卡罗方法

如图3.4为探索性初始化蒙特卡罗方法的伪代码。需要注意的是：

（1）第2步中，每次试验的初始状态和动作都是随机的，以保证每个状态行为对都有机会作为初始化。在进行状态行为对值函数评估时，需要对每次试验中所有的状态行为对进行估计。

（2）第3步完成策略评估，第4步完成策略改进。

我们再讨论一下探索性初始化：

在探索性初始化中，迭代每一幕时，初始状态是随机分配的，这样可以保证迭代过程中每个状态行为对都能被选中。它蕴含着一个假设，即：假设所有的动作都被无限频繁选中。对于这个假设，有时很难成立，或无法完全保证。

我们会问，如何保证初始状态不变的同时，又能保证报个状态行为对可以被访问到？

答案是：精心地设计你的探索策略，以保证每个状态都能被访问到。

可是如何精心地设计探索策略？符合要求的探索策略是什么样的？

答案是：策略必须是温和的，即对所有的状态![[公式]](https://www.zhihu.com/equation?tex=s) 和![[公式]](https://www.zhihu.com/equation?tex=a) 满足：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28a%7Cs%5Cright%29%3E0+%5C%5D) 。也就是说，温和的探索策略是指在任意状态下，采用动作集中每个动作的概率都大于零。典型的温和策略是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cvarepsilon+-soft+%5C%5D) 策略，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28a%7Cs%5Cright%29%5Cgets%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bc%7D+1-%5Cvarepsilon+%2B%5Cfrac%7B%5Cvarepsilon%7D%7B%5Cleft%7C+A%5Cleft%28s%5Cright%29%5Cright%7C%7D%5C+if%5C+a%3Darg%5Cmax_aQ%5Cleft%28s%2Ca%5Cright%29%5C%5C%5C%5C+%5Cfrac%7B%5Cvarepsilon%7D%7B%5Cleft%7C+A%5Cleft%28s%5Cright%29%5Cright%7C%7D%5C+if%5C+a%5Cne+arg%5Cmax_aQ%5Cleft%28s%2Ca%5Cright%29%5C%5C+%5Cend%7Barray%7D%5Cright.%5C%5C%5C%5C%5Cleft%283.5%5Cright%29+%5C%5D)

根据探索策略（行动策略）和评估的策略是否是同一个策略，蒙特卡罗方法又分为on-policy和off-policy.

若行动策略和评估及改善的策略是同一个策略，我们称之为on-policy,可翻译为同策略。

若行动策略和评估及改善的策略是不同的策略，我们称之为off-policy,
可翻译为异策略。

接下来我们重点理解这on-policy方法和off-policy方法。

**On-policy**: 同策略是指产生数据的策略与评估和要改善的策略是同一个策略。比如，要产生数据的策略和评估及要改进的策略都是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cvarepsilon+-soft+%5C%5D)策略。其伪代码如图3.5所示

![img](https://pic1.zhimg.com/80/v2-b1557717c43fed2f86f7254d59a14fbc_hd.jpg)

图3.5 同策略蒙特卡罗强化学习

如图3.5产生数据的策略以及进行评估和改进的策略都是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cvarepsilon+-soft+%5C%5D)策略。

**Off-policy:**

异策略是指产生数据的策略与评估和改善的策略不是同一个策略。我们用![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)表示用来评估和改进的策略，用![[公式]](https://www.zhihu.com/equation?tex=%5Cmu) 表示产生样本数据的策略。

异策略可以保证充分的探索性。例如用来评估和改进的策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)是贪婪策略，用于产生数据的探索性策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)为探索性策略，如![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cvarepsilon+-soft+%5C%5D)策略。

用于异策略的目标策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)和行动策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)并非任意选择的，而是必须满足一定的条件。这个条件是覆盖性条件即：行动策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)产生的行为覆盖或包含目标策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)产生的行为。利用式子表示即为：满足![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28a%7Cs%5Cright%29%3E0+%5C%5D) 的任何![[公式]](https://www.zhihu.com/equation?tex=%28s%2Ca%29) 均满足![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%5Cleft%28a%7Cs%5Cright%29%3E0) 。

利用行为策略产生的数据评估目标策略需要利用**重要性采样**方法。下面，我们来介绍重要性采样。

![img](https://pic1.zhimg.com/80/v2-6af7314d87e34ad90703a26742c2aa10_hd.jpg)

图3.6 重要性采样

我们利用图3.6来描述重要性采样的原理。重要性采样来源于求期望：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+E%5Cleft%5Bf%5Cright%5D%3D%5Cint%7Bf%5Cleft%28z%5Cright%29p%5Cleft%28z%5Cright%29dz%7D%5C%5C%5C%5C%5Cleft%283.6%5Cright%29+%5C%5D)

如图3.6, 当随机变量z的分布非常复杂时，无法利用解析的方法产生用于逼近期望的样本，这时，我们可以选用一个概率分布很简单，产生样本很容易的概率分布![[公式]](https://www.zhihu.com/equation?tex=q%28z%29) ，比如正态分布。原来的期望可变为：

![[公式]](https://www.zhihu.com/equation?tex=+E%5Cleft%5Bf%5Cright%5D%3D%5Cint%7Bf%5Cleft%28z%5Cright%29p%5Cleft%28z%5Cright%29dz%7D+%5C%5C+%5C%5C%5C%5C%5C+%3D%5Cint%7Bf%5Cleft%28z%5Cright%29%5Cfrac%7Bp%5Cleft%28z%5Cright%29%7D%7Bq%5Cleft%28z%5Cright%29%7D%7Dq%5Cleft%28z%5Cright%29dz%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C%5C+%5C%5C%5C%5C%5Capprox%5Cfrac%7B1%7D%7BN%7D%5Csum_n%7B%5Cfrac%7Bp%5Cleft%28z%5En%5Cright%29%7D%7Bq%5Cleft%28z%5En%5Cright%29%7Df%5Cleft%28z%5En%5Cright%29%2Cz%5En%5Cthicksim+q%5Cleft%28z%5Cright%29%7D+%5C%5D) (3.7)

定义重要性权重：![[公式]](https://www.zhihu.com/equation?tex=%5Comega%5En%3D%5Cdfrac%7Bp%5Cleft%28z%5En%5Cright%29%7D%7Bq%5Cleft%28z%5En%5Cright%29%7D) ，普通的重要性采样求积分如方程(3.7)所示为：

![[公式]](https://www.zhihu.com/equation?tex=E%5Cleft%5Bf%5Cright%5D%3D%5Cfrac%7B1%7D%7BN%7D%5Csum_n%7B%5Comega%5Enf%5Cleft%28z%5En%5Cright%29%7D%5C%5C%5C%5C) (3.8)

由式(3.7)可以知道，基于重要性采样的积分估计为无偏估计，即估计的期望值等于真实的期望。但是，基于重要性采样的积分估计的方差无穷大。这是因为，原来的被积函数乘上了一个重要性权重，这就改变了被积函数的形状及分布。尽管被积函数的均值没有发生变化，但方差明显发生改变。

在重要性采样中，使用的采样概率分布与原概率分布越接近，方差越小。然而，被积函数的概率分布往往很难求得，或很奇怪，没有简单地采样概率分布能与之相似，如果使用分布差别很大的采样概率对原概率分布进行采样，方差会趋近于无穷大。

一种减小重要性采样积分方差的方法是采用加权重要性采样：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+E%5Cleft%5Bf%5Cright%5D%5Capprox%5Csum_%7Bn%3D1%7D%5EN%7B%5Cfrac%7B%5Comega%5En%7D%7B%5CvarSigma_%7Bm%3D1%7D%5E%7BN%7D%5Comega%5Em%7D%7Df%5Cleft%28z%5En%5Cright%29%5C%5C%5C%5C+%5C%5D) (3.9)

在异策略方法中，行动策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)即用来产生样本的策略，所产生的轨迹概率分布相当于重要性采样中的![[公式]](https://www.zhihu.com/equation?tex=q%5Bz%5D)，用来评估和改进的策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)所对应的轨迹概率分布为![[公式]](https://www.zhihu.com/equation?tex=p%5Bz%5D)，因此利用行动策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)所产生的累积函数返回值来评估策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)时，需要在累积函数返回值前面乘以重要性权重。

在目标策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)下，一次试验的概率为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Pr%5Cleft%28A_t%2CS_%7Bt%2B1%7D%2C%5Ccdots+%2CS_T%5Cright%29%3D%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cpi%5Cleft%28A_k%7CS_k%5Cright%29p%5Cleft%28S_%7Bk%2B1%7D%7CS_k%2CA_k%5Cright%29%7D+%5C%5D)

在行动策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)下，相应的试验的概率为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Pr%5Cleft%28A_t%2CS_%7Bt%2B1%7D%2C%5Ccdots+%2CS_T%5Cright%29%3D%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cmu%5Cleft%28A_k%7CS_k%5Cright%29p%5Cleft%28S_%7Bk%2B1%7D%7CS_k%2CA_k%5Cright%29%7D+%5C%5D)

因此重要性权重为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Crho_%7Bt%7D%5E%7BT%7D%3D%5Cfrac%7B%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cpi%5Cleft%28A_k%7CS_k%5Cright%29p%5Cleft%28S_%7Bk%2B1%7D%7CS_k%2CA_k%5Cright%29%7D%7D%7B%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cmu%5Cleft%28A_k%7CS_k%5Cright%29p%5Cleft%28S_%7Bk%2B1%7D%7CS_k%2CA_k%5Cright%29%7D%7D%3D%5Cprod_%7Bk%3Dt%7D%5E%7BT-1%7D%7B%5Cfrac%7B%5Cpi%5Cleft%28A_k%7CS_k%5Cright%29%7D%7B%5Cmu%5Cleft%28A_k%7CS_k%5Cright%29%7D%7D%5C%5C%5C%5C%5Cleft%283.10%5Cright%29+%5C%5D)

普通重要性采样，值函数估计为如图3.7所示：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V%5Cleft%28s%5Cright%29%3D%5Cfrac%7B%5CvarSigma_%7Bt%5Cin%5Cmathcal%7BT%7D%5Cleft%28s%5Cright%29%7D%5Crho_%7Bt%7D%5E%7BT%5Cleft%28t%5Cright%29%7DG_t%7D%7B%5Cleft%7C%5Cmathcal%7BT%7D%5Cleft%28s%5Cright%29%5Cright%7C%7D%5C%5C%5C%5C%5Cleft%283.11%5Cright%29+%5C%5D)

![img](https://pic3.zhimg.com/80/v2-1b93d6f7d0536cb4002aa3db8f01566a_hd.jpg)

图3.7 普通重要性采样计算公式

现在举例说明公式(3.11)中各个符号的具体含义。

![img](https://pic1.zhimg.com/80/v2-fac4cf77a1559630d7f9611151a13078_hd.jpg)

图3.8 重要性采样公式举例解释

t是状态访问的时刻，![[公式]](https://www.zhihu.com/equation?tex=T%28t%29)是访问状态s相对应的那个试验的终止状态所对应的时刻。![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmathcal%7BT%7D%5Cleft%28s%5Cright%29+%5C%5D)是状态s发生的所有时刻集合。

加权重要性采样值函数估计为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V%5Cleft%28s%5Cright%29%3D%5Cfrac%7B%5CvarSigma_%7Bt%5Cin%5Cmathcal%7BT%7D%5Cleft%28s%5Cright%29%7D%5Crho_%7Bt%7D%5E%7BT%5Cleft%28t%5Cright%29%7DG_t%7D%7B%5CvarSigma_%7Bt%5Cin%5Cmathcal%7BT%7D%5Cleft%28s%5Cright%29%7D%5Crho_%7Bt%7D%5E%7BT%5Cleft%28t%5Cright%29%7D%7D%5C%5C%5C%5C%5Cleft%283.12%5Cright%29+%5C%5D)

最后给出异策略每次访问蒙特卡罗算法的伪代码：

![img](https://pic3.zhimg.com/80/v2-d89672f70cdf683866ba44c537011cfa_hd.jpg)

需要注意的是：此处的软策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)为![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cvarepsilon+-soft+%5C%5D)策略，需要改进的策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)为贪婪策略。

最后，总结一下：

本节重点讲解了如何利用MC的方法估计值函数。跟基于动态规划的方法相比，基于MC的方法只是在值函数估计上有所不同。两者在整个框架上是相同的，即对当前策略进行评估，然后利用学到的值函数进行策略改进。






重点理解on-policy 和off-policy的概念，学会利用重要性采样来评估目标策略的值函数

# 强化学习入门 第四讲 时间差分法（TD方法）

强化学习入门第四讲 时间差分方法

上一节我们已经讲了无模型强化学习最基本的方法蒙特卡罗方法。本节，我们讲另外一个无模型的方法时间差分的方法。

![img](https://pic2.zhimg.com/80/v2-6fbb5020d52942d6e42b9794a762d849_hd.jpg)

图4.1 强化学习算法分类



时间差分(TD)方法是强化学习理论中最核心的内容，是强化学习领域最重要的成果，没有之一。与动态规划的方法和蒙特卡罗的方法比，时间差分的方法主要不同点在值函数估计上面。

![img](https://pic2.zhimg.com/80/v2-efd6697aaa4f7d7ef8be9787af43bb99_hd.jpg)



图4.2 动态规划方法计算值函数

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V%5Cleft%28S_t%5Cright%29%5Cgets+E_%7B%5Cpi%7D%5Cleft%5BR_%7Bt%2B1%7D%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29%5Cright%5D%3D%5Csum_a%7B%5Cpi%5Cleft%28a%7CS_t%5Cright%29%5Csum_%7Bs%27%2Cr%7D%7Bp%5Cleft%28s%27%2Cr%7CS_t%2Ca%5Cright%29%5Cleft%5Br%2B%5Cgamma+V%5Cleft%28s%27%5Cright%29%5Cright%5D%7D%7D+%5C%5D) （4.1）

方程(4.1)给出了值函数估计的计算公式，从公式中可以看到，DP方法计算值函数时用到了当前状态s的所有后继状态s’处的值函数。值函数的计算用到了**bootstapping**的方法。所谓bootstrpping本意是指自举，此处是指当前值函数的计算用到了后继状态的值函数。即**用后继状态的值函数估计当前值函数**。特别注意，此处后继的状态是由模型公式![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+p%5Cleft%28s%27%2Cr%7CS_t%2Ca%5Cright%29+%5C%5D)

计算得到的。由模型公式和动作集，可以计算状态s所有的后继状态s’。当没有模型时，后继状态无法全部得到，只能通过试验和采样的方法每次试验得到一个后继状态s’。

无模型时，我们可以采用蒙特卡罗的方法利用经验平均来估计当前状态的值函数。其计算值函数示意图如图4.3所示。

![img](https://pic1.zhimg.com/80/v2-c45a6aefd3d8f735b82616037fc8b208_hd.jpg)

图4.3 蒙特卡罗方法计算值函数



蒙特卡罗方法利用经验平均估计状态的值函数，所谓的经验是指一次试验，而一次试验要等到终止状态出现才结束，如图4.3所示。公式(4.2)中的![[公式]](https://www.zhihu.com/equation?tex=G_t)是状态![[公式]](https://www.zhihu.com/equation?tex=S_t)后直到终止状态所有回报的返回值。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V%5Cleft%28S_t%5Cright%29%5Cgets+V%5Cleft%28S_t%5Cright%29%2B%5Calpha%5Cleft%28G_t-V%5Cleft%28S_t%5Cright%29%5Cright%29+%5C%5D) (4.2)

相比于动态规划的方法，蒙特卡罗的方法需要等到每次试验结束，所以学习速度慢，学习效率不高。从两者的比较我们很自然地会想，能不能借鉴动态规划中boot’strapping的方法，在不等到试验结束时就估计当前的值函数呢？

答案是肯定的，这就是时间差分方法的精髓。时间差分方法结合了蒙特卡罗的采样方法（即做试验）和动态规划方法的bootstrapping(利用后继状态的值函数估计当前值函数)，其示意图如图4.4所示。

![img](https://pic2.zhimg.com/80/v2-d1a2f656558676f3322f6482cf68ce19_hd.jpg)



图4.4 时间差分方法计算值函数



TD方法更新值函数的公式为(4.3)：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V%5Cleft%28S_t%5Cright%29%5Cgets+V%5Cleft%28S_t%5Cright%29%2B%5Calpha%5Cleft%28R_%7Bt%2B1%7D%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29-V%5Cleft%28S_t%5Cright%29%5Cright%29+%5C%5D) (4.3)

其中![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+R_%7Bt%2B1%7D%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29+%5C%5D)称为TD目标，与（4.2）中的![[公式]](https://www.zhihu.com/equation?tex=G_t)相对应，两者不同之处是TD目标利用了bootstrapping方法估计当前值函数。![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cdelta_t%3DR_%7Bt%2B1%7D%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29-V%5Cleft%28S_t%5Cright%29+%5C%5D) 称为TD偏差。

下面我们从原始公式给出动态规划(DP)，蒙特卡罗方法(MC)，和时间差分方法(TD)的不同之处。

![img](https://pic3.zhimg.com/80/v2-cf92820b336eee7746f04257693e998e_hd.jpg)



图4.5 DP,MC和TD方法的异同



图4.5给出了三种方法估计值函数时的异同点。从图中可以看到，蒙特卡罗的方法使用的是值函数最原始的定义，该方法利用所有回报的累积和估计值函数。DP方法和TD方法则利用一步预测方法计算当前状态值函数。其共同点是利用了bootstrapping方法，不同的是，DP方法利用模型计算后继状态，而TD方法利用试验得到后继状态。

从统计学的角度来看，蒙特卡罗方法（MC方法）和时间差分方法（TD方法）都是利用样本去估计值函数的方法，哪种估计方法更好呢？既然是统计方法，我们就可以从期望和方差两个指标对两种方法进行对比。

首先蒙特卡罗方法：

蒙特卡罗方法中的返回值![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_t%3DR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Ccdots+%2B%5Cgamma%5E%7BT-1%7DR_T+%5C%5D) ，其期望便是值函数的定义，因此蒙特卡罗方法是无偏估计。但是，蒙特卡罗方法每次得到的![[公式]](https://www.zhihu.com/equation?tex=G_t) 值要等到最终状态出现，在这个过程中要经历很多随机的状态和动作，因此每次得到的![[公式]](https://www.zhihu.com/equation?tex=G_t)随机性很大，所以尽管期望等于真值，但方差无穷大。

其次，时间差分方法：

时间差分方法的TD目标为![[公式]](https://www.zhihu.com/equation?tex=R_%7Bt%2B1%7D%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29)，若![[公式]](https://www.zhihu.com/equation?tex=V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29) 采用真实值，则TD估计也是无偏估计，然而在试验中![[公式]](https://www.zhihu.com/equation?tex=V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29) 用的也是估计值，因此时间差分估计方法属于有偏估计。然而，跟蒙特卡罗方法相比，时间差分方法只用到了一步随机状态和动作，因此TD目标的随机性比蒙特卡罗方法中的![[公式]](https://www.zhihu.com/equation?tex=G_t) 要小，因此其方差也比蒙特卡罗方法的方差小。

![img](https://pic3.zhimg.com/80/v2-1867e844fcf5b06b00e3593bf29f930e_hd.jpg)

图4.6 on-policy Sarsa强化学习算法



如图4.6为同策略Sarsa强化学习算法，主要注意的是方框中代码表示同策略中的行动策略和评估的策略都是 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon+) 贪婪策略。与蒙特卡罗方法不同的是，值函数更新不同。

![img](https://pic2.zhimg.com/80/v2-a3ee3d85fabac477a2681835f7c53d09_hd.jpg)

图4.7 off-policy Qlearning

如图4.7为异策略的Qlearning方法。与Sarsa方法的不同之处在于，Qlearning方法是异策略。即行动策略采用 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon+) 贪婪策略，而目标策略采用贪婪策略。

从图4.4我们看到，在更新当前值函数时，用到了下一个状态的值函数。那么我们可以以此推理，能不能利用后继第二个状态的值函数来更新当前状态的值函数呢？

答案是肯定的，那么如何利用公式计算呢？

我们用![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Cleft%281%5Cright%29%7D%3DR_%7Bt%2B1%7D%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29) 表示TD目标，则利用第二步值函数来估计当前值函数可表示为：![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Cleft%282%5Cright%29%7D%3DR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Cgamma%5E2V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29) 以此类推，利用第n步的值函数更新当前值函数可表示为：

![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Cleft%28n%5Cright%29%7D%3DR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Ccdots+%2B%5Cgamma%5E%7Bn-1%7DR_%7Bt%2Bn%7D%2B%5Cgamma%5EnV%5Cleft%28S_%7Bt%2Bn%7D%5Cright%29)

![img](https://pic1.zhimg.com/80/v2-1d087d9cff7205234f0bc4cb5810deb0_hd.jpg)

图4.8 n步预测估计值函数

如图4.8所示为利用n步值函数估计当前值函数的示意图。我们再审视一下刚刚的结论：可以利用n步值函数来估计当前值函数，也就是说当前值函数有n种估计方法。

哪种估计值更接近真实值呢？

我们不知道，但是我们是不是可以对这n个估计值利用加权的方法进行融合一下呢？这就是![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的方法。

我们在![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Cleft%28n%5Cright%29%7D)前乘以加权因子![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%281-%5Clambda%5Cright%29%5Clambda%5E%7Bn-1%7D)，为什么要乘这个加权呢？这是因为：

![[公式]](https://www.zhihu.com/equation?tex=+G_%7Bt%7D%5E%7B%5Clambda%7D%3D%5Cleft%281-%5Clambda%5Cright%29G_%7Bt%7D%5E%7B%5Cleft%281%5Cright%29%7D%2B%5Cleft%281-%5Clambda%5Cright%29%5Clambda+G_%7Bt%7D%5E%7B%5Cleft%282%5Cright%29%7D%2B%5Ccdots+%2B%5Cleft%281-%5Clambda%5Cright%29%5Clambda%5E%7Bn-1%7DG_%7Bt%7D%5E%7B%5Cleft%28n%5Cright%29%7D+%5C%5C+%5Capprox%5Cleft%5B%5Cleft%281-%5Clambda%5Cright%29%2B%5Cleft%281-%5Clambda%5Cright%29%5Clambda+%2B%5Ccdots+%2B%5Cleft%281-%5Clambda%5Cright%29%5Clambda%5E%7Bn-1%7D%5Cright%5DV%5Cleft%28S_t%5Cright%29+%5C%5C+%3DV%5Cleft%28S_t%5Cright%29) (4.4)

利用![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Clambda%7D)来更新当前状态的值函数的方法称为![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的方法。对于![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29) 的理解一般可以从两个视角进行解读。

第一个视角是前向视角，该视角也是![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的定义。

![img](https://pic2.zhimg.com/80/v2-c1ebe7e7ec69a34503f95c5e73c6022d_hd.jpg)

图4.9 ![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的前向视角

如图4.9所示为![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)方法的前向视角解释。假设一个人坐在状态流上拿着望远镜看向前方，前方是那些将来的状态。当估计当前状态的值函数时，![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的定义中可以看到，它需要用来将来时刻的值函数。也就是说，![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)前向观点通过观看将来状态的值函数来估计当前的值函数。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V%5Cleft%28S_t%5Cright%29%5Cgets+V%5Cleft%28S_t%5Cright%29%2B%5Calpha%5Cleft%28G_%7Bt%7D%5E%7B%5Cleft%28%5Clambda%5Cright%29%7D-V%5Cleft%28S_t%5Cright%29%5Cright%29+%5C%5D) (4.4)

其中![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_%7Bt%7D%5E%7B%5Clambda%7D%3D%5Cleft%281-%5Clambda%5Cright%29%5Csum_%7Bn%3D1%7D%5E%7B%5Cinfty%7D%7B%5Clambda%5E%7Bn-1%7D%7DG_%7Bt%7D%5E%7B%5Cleft%28n%5Cright%29%7D+%5C%5D) ，而![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_%7Bt%7D%5E%7B%5Cleft%28n%5Cright%29%7D%3DR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Ccdots+%2B%5Cgamma%5E%7Bn-1%7DR_%7Bt%2Bn%7D%2B%5Cgamma%5EnV%5Cleft%28S_%7Bt%2Bn%7D%5Cright%29+%5C%5D)

利用![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的前向观点估计值函数时，![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Clambda%7D) 的计算用到了将来时刻的值函数，因此需要等到整个试验结束之后。这跟蒙塔卡罗方法相似。那么有没有一种更新方法不需要等到试验结束就可以更新当前状态的值函数呢？

有，这种增量式的更新方法需要利用![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的后向观点。

![img](https://pic3.zhimg.com/80/v2-39b3757414f2d21d6e8c7337679967ea_hd.jpg)

图4.10 ![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的后向观点

如图4.10为![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)后向观点示意图，人骑坐在状态流上，手里拿着话筒，面朝已经经历过的状态流，获得当前回报并利用下一个状态的值函数得到TD偏差后，此人会向已经经历过的状态喊话，告诉这些已经经历过的状态处的值函数需要利用当前时刻的TD偏差进行更新。此时过往的每个状态值函数更新的大小应该跟距离当前状态的步数有关。假设当前状态为![[公式]](https://www.zhihu.com/equation?tex=s_t)，TD偏差为![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_t) ,那么![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt-1%7D)处的值函数更新应该乘以一个衰减因子![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma%5Clambda+)，状态![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt-2%7D) 处的值函数更新应该乘以![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28%5Cgamma%5Clambda%5Cright%29%5E2) ，以此类推。

![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)更新过程为：

（1）
首先计算当前状态的TD偏差：![[公式]](https://www.zhihu.com/equation?tex=%5Cdelta_t%3DR_%7Bt%2B1%7D%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29-V%5Cleft%28S_t%5Cright%29)

（2）
更新适合度轨迹：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+E_t%5Cleft%28s%5Cright%29%3D%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bl%7D+%5Cgamma%5Clambda+E_%7Bt-1%7D%2C%5C+if%5C+s%5Cne+s_t%5C%5C+%5Cgamma%5Clambda+E_%7Bt-1%7D%2B1%2C%5C+if%5C+s%3Ds_t%5C%5C+%5Cend%7Barray%7D%5Cright.+%5C%5D)

（3）
对于状态空间中的每个状态s, 更新值函数：![[公式]](https://www.zhihu.com/equation?tex=V%5Cleft%28s%5Cright%29%5Cgets+V%5Cleft%28s%5Cright%29%2B%5Calpha%5Cdelta_tE_t%5Cleft%28s%5Cright%29)

其中![[公式]](https://www.zhihu.com/equation?tex=E_t%5Cleft%28s%5Cright%29)称为适合度轨迹。

注意：现在我们比较一下![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)的前向观点和后向观点的异同：

（1）
前向观点需要等到一次试验之后再更新当前状态的值函数；而后向观点不需要等到值函数结束后再更新值函数，而是每一步都在更新值函数，是增量式方法。

（2）
前向观点在一次试验结束后更新值函数时，更新完当前状态的值函数后，此状态的值函数就不再改变。而后向观点，在每一步计算完当前的TD误差后，其他状态的值函数需要利用当前状态的TD误差进行更新。

（3）
在一次试验结束后，前向观点和后向观点每个状态的值函数的更新总量是相等的，都是![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Clambda%7D)。

为了说明前向观点和后向观点的等价性，我们从公式上对其进行严格地证明。

首先，当![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+%3D0) 时，只有当前状态值更新，此时等价于之前说的TD方法。所以TD方法又称为TD(0)方法.

其次，当![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+%3D1)时，状态s值函数总的更新与蒙特卡罗方法相同：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cbegin%7Barray%7D%7Bl%7D+%5Cdelta_t%2B%5Cgamma%5Cdelta_%7Bt%2B1%7D%2B%5Cgamma%5E2%5Cdelta_%7Bt%2B2%7D%2B%5Ccdots+%2B%5Cgamma%5E%7BT-1-t%7D%5Cdelta_%7BT-1%7D%5C%5C+%3DR_%7Bt%2B1%7D%2B%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29-V%5Cleft%28S_t%5Cright%29%5C%5C+%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Cgamma%5E2V%5Cleft%28S_%7Bt%2B2%7D%5Cright%29-%5Cgamma+V%5Cleft%28S_%7Bt%2B1%7D%5Cright%29%5C%5C+%2B%5Cgamma%5E2R_%7Bt%2B3%7D%2B%5Cgamma%5E3V%5Cleft%28S_%7Bt%2B3%7D%5Cright%29-%5Cgamma%5E2V%5Cleft%28S_%7Bt%2B2%7D%5Cright%29%5C%5C+%5Cvdots%5C%5C+%2B%5Cgamma%5E%7BT-1-t%7DR_T%2B%5Cgamma%5E%7BT-t%7DV%5Cleft%28S_T%5Cright%29-%5Cgamma%5E%7BT-1-t%7DV%5Cleft%28S_%7BT-1%7D%5Cright%29%5C%5C+%5Cend%7Barray%7D+%5C%5D)

对于一般的![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+)，前向观点等于后向观点：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+G_t%5E%5Clambda+-+V%5Cleft%28+%7B%7BS_t%7D%7D+%5Cright%29+%3D+%5C%5C+-+V%5Cleft%28+%7B%7BS_t%7D%7D+%5Cright%29+%2B+%5Cleft%28+%7B1+-+%5Clambda+%7D+%5Cright%29%7B%5Clambda+%5E0%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+V%5Cleft%28+%7B%7BS_%7Bt+%2B+1%7D%7D%7D+%5Cright%29%7D+%5Cright%29%5C%5C+%2B+%5Cleft%28+%7B1+-+%5Clambda+%7D+%5Cright%29%7B%5Clambda+%5E1%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+%7BR_%7Bt+%2B+2%7D%7D+%2B+%7B%5Cgamma+%5E2%7DV%5Cleft%28+%7B%7BS_%7Bt+%2B+2%7D%7D%7D+%5Cright%29%7D+%5Cright%29%5C%5C+%2B+%5Cleft%28+%7B1+-+%5Clambda+%7D+%5Cright%29%7B%5Clambda+%5E2%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+%7BR_%7Bt+%2B+2%7D%7D+%2B+%7B%5Cgamma+%5E2%7D%7BR_%7Bt+%2B+3%7D%7D+%2B+%7B%5Cgamma+%5E3%7DV%5Cleft%28+%7B%7BS_%7Bt+%2B+2%7D%7D%7D+%5Cright%29%7D+%5Cright%29+%2B+%5Ccdots+%5Cend%7Barray%7D%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm%7B+%3D+%7D%7D+-+V%5Cleft%28+%7B%7BS_t%7D%7D+%5Cright%29+%2B+%7B%5Cleft%28+%7B%5Cgamma+%5Clambda+%7D+%5Cright%29%5E0%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+V%5Cleft%28+%7B%7BS_%7Bt+%2B+1%7D%7D%7D+%5Cright%29+-+%5Cgamma+%5Clambda+V%5Cleft%28+%7B%7BS_%7Bt+%2B+1%7D%7D%7D+%5Cright%29%7D+%5Cright%29%5C%5C+%2B+%7B%5Cleft%28+%7B%5Cgamma+%5Clambda+%7D+%5Cright%29%5E1%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+2%7D%7D+%2B+%5Cgamma+V%5Cleft%28+%7B%7BS_%7Bt+%2B+2%7D%7D%7D+%5Cright%29+-+%5Cgamma+%5Clambda+V%5Cleft%28+%7B%7BS_%7Bt+%2B+2%7D%7D%7D+%5Cright%29%7D+%5Cright%29%5C%5C+%2B+%7B%5Cleft%28+%7B%5Cgamma+%5Clambda+%7D+%5Cright%29%5E2%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+3%7D%7D+%2B+%5Cgamma+V%5Cleft%28+%7B%7BS_%7Bt+%2B+3%7D%7D%7D+%5Cright%29+-+%5Cgamma+%5Clambda+V%5Cleft%28+%7B%7BS_%7Bt+%2B+3%7D%7D%7D+%5Cright%29%7D+%5Cright%29+%2B+%5Ccdots+%5Cend%7Barray%7D%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%5Cbegin%7Barray%7D%7Bl%7D+%7B%5Crm%7B+%3D+%7D%7D%7B%5Cleft%28+%7B%5Cgamma+%5Clambda+%7D+%5Cright%29%5E0%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+1%7D%7D+%2B+%5Cgamma+V%5Cleft%28+%7B%7BS_%7Bt+%2B+1%7D%7D%7D+%5Cright%29+-+V%5Cleft%28+%7B%7BS_t%7D%7D+%5Cright%29%7D+%5Cright%29%5C%5C+%7B%5Crm%7B+%2B+%7D%7D%7B%5Cleft%28+%7B%5Cgamma+%5Clambda+%7D+%5Cright%29%5E1%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+2%7D%7D+%2B+%5Cgamma+V%5Cleft%28+%7B%7BS_%7Bt+%2B+2%7D%7D%7D+%5Cright%29+-+V%5Cleft%28+%7B%7BS_%7Bt+%2B+1%7D%7D%7D+%5Cright%29%7D+%5Cright%29%5C%5C+%7B%5Crm%7B+%2B+%7D%7D%7B%5Cleft%28+%7B%5Cgamma+%5Clambda+%7D+%5Cright%29%5E2%7D%5Cleft%28+%7B%7BR_%7Bt+%2B+3%7D%7D+%2B+%5Cgamma+V%5Cleft%28+%7B%7BS_%7Bt+%2B+3%7D%7D%7D+%5Cright%29+-+V%5Cleft%28+%7B%7BS_%7Bt+%2B+2%7D%7D%7D+%5Cright%29%7D+%5Cright%29+%2B+%5Ccdots+%5Cend%7Barray%7D%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%7B%5Crm%7B+%3D+%7D%7D%7B%5Cdelta+_t%7D+%2B+%5Cgamma+%5Clambda+%7B%5Cdelta+_%7Bt+%2B+1%7D%7D+%2B+%7B%5Cleft%28+%7B%5Cgamma+%5Clambda+%7D+%5Cright%29%5E2%7D%7B%5Cdelta+_%7Bt+%2B+2%7D%7D+%2B+%5Ccdots+%5C%5D)

注意，所谓等价是指更新总量相等。

最后，我们给出![[公式]](https://www.zhihu.com/equation?tex=Sarsa%5Cleft%28%5Clambda%5Cright%29)算法的伪代码：

![img](https://pic3.zhimg.com/80/v2-dd1e1263d3c33e506041656c5296056a_hd.jpg)

图4.11 Sarsa算法的伪代码



#   强化学习入门 第五讲 值函数逼近


  

前面已经讲了强化学习的基本方法：基于动态规划的方法，基于蒙特卡罗的方法和基于时间差分的方法。这些方法有一个基本的前提条件，那就是**状态空间和动作空间是离散的**，而且**状态空间和动作空间不能太大**。

我们回想一下已经介绍的强化学习方法的基本步骤是：首先评估值函数，接着利用值函数改进当前的策略。其中**值函数的评估是关键**。

对于模型已知的系统，值函数可以利用动态规划的方法得到；对于模型未知的系统，可以利用蒙特卡罗的方法或者时间差分的方法得到。

注意，这时的**值函数其实是一个表格**。对于状态值函数，其索引是状态；对于行为值函数，其索引是状态-行为对。值函数迭代更新的过程实际上就是对这张表进行迭代更新。因此，之前讲的强化学习算法又称为表格型强化学习。对于状态值函数，其表格的维数为状态的个数 ![[公式]](https://www.zhihu.com/equation?tex=%7CS%7C)，其中 ![[公式]](https://www.zhihu.com/equation?tex=S)为状态空间。若状态空间的维数很大，或者状态空间为连续空间，**此时值函数无法用一张表格来表示**。这时，我们需要利用**函数逼近的方法对值函数进行表示**。如图5.1所示。当值函数利用函数逼近的方法表示后，可以利用策略迭代和值迭代方法构建强化学习算法。

![img](https://pic1.zhimg.com/80/v2-6a4898fc4743d76c16d42c51b93c0be8_hd.jpg)

图5.1 强化学习分类



在表格型强化学习中，值函数对应着一张表。在值函数逼近方法中，值函数对应着一个逼近函数![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cupsilon%7D%5Cleft%28s%5Cright%29) 。从数学角度来看，函数逼近方法可以分为**参数逼近和非参数逼近**，因此强化学习值函数估计可以分为**参数化逼近和非参数化逼近**。其中参数化逼近又分为线性参数化逼近和非线性化参数逼近。

这一节，我们主要介绍参数化逼近。所谓参数化逼近，是指值函数可以由一组参数![[公式]](https://www.zhihu.com/equation?tex=%5CvarTheta) 来近似。我们将逼近的值函数写为：![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cupsilon%7D%5Cleft%28s%2C%5Ctheta%5Cright%29)。

当逼近的值函数结构确定时，（如线性逼近时选定了基函数，非线性逼近时选定了神经网络的结构），那么值函数的逼近就等价于参数的逼近。值函数的更新也就等价于参数的更新。也就是说，我们需要利用试验数据来更新参数值。**如何利用数据更新参数值呢？也就是说如何从数据中学到参数值呢？**

我们回顾一下表格型强化学习值函数更新的公式，以便从中得到启发：

蒙特卡罗方法，值函数更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Cleft%28s%2Ca%5Cright%29%5Cgets+Q%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cleft%28G_t-Q%5Cleft%28s%2Ca%5Cright%29%5Cright%29+%5C%5D) (5.1)

TD方法值函数更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Cleft%28s%2Ca%5Cright%29%5Cgets+Q%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cleft%5Br%2B%5Cgamma+Q%5Cleft%28s%27%2Ca%27%5Cright%29-Q%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5D) (5.2)

![[公式]](https://www.zhihu.com/equation?tex=TD%5Cleft%28%5Clambda%5Cright%29)方法值函数更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Cleft%28s%2Ca%5Cright%29%5Cgets+Q%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cleft%5BG_%7Bt%7D%5E%7B%5Clambda%7D-Q%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5D) (5.3)

从式（5.1）—（5.3）值函数的更新过程我们看到，值函数更新过程是向着目标值函数靠近。

如图5.2所示为TD方法更新值函数的过程。

![img](https://pic1.zhimg.com/80/v2-2b75f85ae71e2cac4f0208d785e979b8_hd.jpg)



图5.2 TD方法值函数更新

从表格型值函数的更新过程，我们不难总结出不管是蒙特卡罗方法还是TD方法，都是朝着一个目标值更新的，这个目标值在蒙特卡罗方法中是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_t+%5C%5D) ，在TD方法中是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+r%2B%5Cgamma+Q%5Cleft%28s%27%2Ca%27%5Cright%29+%5C%5D) ，在![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+TD%5Cleft%28%5Clambda%5Cright%29+%5C%5D)中是![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+G_%7Bt%7D%5E%7B%5Clambda%7D+%5C%5D) 。

将表格型强化学习值函数的更新过程**推广到值函数逼近过程**，有如下形式：

函数逼近![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Cupsilon%7D%5Cleft%28s%2C%5Ctheta%5Cright%29+%5C%5D) 的过程是一个监督学习的过程，其数据和标签对为：![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%28S_t%2CU_t%5Cright%29), 其中![[公式]](https://www.zhihu.com/equation?tex=U_t)
等价于蒙特卡罗方法中的![[公式]](https://www.zhihu.com/equation?tex=G_t)，TD方法中的 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+r%2B%5Cgamma+Q%5Cleft%28s%27%2Ca%27%5Cright%29+%5C%5D) ，以及 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+TD%5Cleft%28%5Clambda%5Cright%29+%5C%5D) 中的 ![[公式]](https://www.zhihu.com/equation?tex=G_%7Bt%7D%5E%7B%5Clambda%7D)。

训练的目标函数为：

![[公式]](https://www.zhihu.com/equation?tex=argmin_%7B%5Ctheta%7D%5Cleft%28q%5Cleft%28s%2Ca%5Cright%29-%5Chat%7Bq%7D%5Cleft%28s%2Ca%2C%5Ctheta%5Cright%29%5Cright%29%5E2+) (5.4)



现在我们比较一下表格型强化学习和函数逼近方法的强化学习值函数更新时的异同点：

（1）
表格型强化学习进行值函数更新时，只有当前状态![[公式]](https://www.zhihu.com/equation?tex=S_t)处的值函数在改变，其他地方的值函数不发生改变。

（2）
值函数逼近方法进行值函数更新时，因此更新的是参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)，而估计的值函数为![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cupsilon%7D%5Cleft%28s%2C%5Ctheta%5Cright%29)，所以当参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)发生改变时，任意状态处的值函数都会发生改变。

值函数更新可分为增量式学习方法和批学习方法。**我们先介绍增量式学习方法。随机梯度下降法是最常用的增量式学习方法。**

由（5.4）我们得到参数的随机梯度更新为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta_%7Bt%2B1%7D%3D%5Ctheta_t%2B%5Calpha%5Cleft%5BU_t-%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2C%5Ctheta_t%5Cright%29%5Cright%5D+%5C%5D) (5.5)

基于蒙特卡罗方法的函数逼近，具体的过程为：

给定要评估的策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)，产生一次试验：

![img](https://pic4.zhimg.com/80/v2-d4cdf4ebddb34dc3e7f348bff88228ff_hd.jpg)

值函数的更新过程实际是一个监督学习的过程，其中监督数据集从蒙塔卡罗的试验中得到，其数据集为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cleft%3C+s_1%2CG_1%5Cright%3E+%2C%5Cleft%3C+s_2%2CG_2%5Cright%3E+%2C%5Ccdots+%2C%5Cleft%3C+s_T%2CG_T%5Cright%3E+%5C%5D),

值函数的更新：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5CvarDelta%5Ctheta+%3D%5Calpha%5Cleft%28G_t-%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2C%5Ctheta%5Cright%29%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2C%5Ctheta%5Cright%29+%5C%5D) (5.6)



其中![[公式]](https://www.zhihu.com/equation?tex=%5Calpha+)值比较小。在随机梯度下降法中，**似乎并不清楚为什么每一步采用很小的更新**。难道我们不能在梯度的方向上移动很大的距离甚至完全消除误差吗？在很多情况下确实可以这样做，但是通常这并不是我们想要的。请记住，我们的目的并不是在所有的状态找到精确的值函数，而是一个**能平衡所有不同状态误差的值函数逼近**。如果我们在一步中完全纠正了偏差，那么我们就无法找到这样的一个平衡了。**因此![[公式]](https://www.zhihu.com/equation?tex=%5Calpha+)值取得比较小可以维持这种平衡**。

![img](https://pic2.zhimg.com/80/v2-f77b6f3957f5829540b4bd76463e555d_hd.jpg)

图5.3 基于梯度的蒙特卡罗值函数逼近

如图5.3所示为基于梯度的蒙塔卡罗值函数逼近更新过程。蒙特卡罗方法的目标值函数使用一次试验的整个回报返回值，我们再看下时间差分方法。根据方程（5.5），TD(0)方法中目标值函数为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+U_t%3DR_%7Bt%2B1%7D%2B%5Cgamma%5Chat%7B%5Cupsilon%7D%5Cleft%28S_%7Bt%2B1%7D%2C%5Ctheta%5Cright%29+%5C%5D) ，即目标值函数用到了bootstrapping的方法。此时，我们注意到此时要更新的参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 不仅出现在要估计的值函数![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2C%5Ctheta%5Cright%29+%5C%5D) 中，还出现在目标值函数![[公式]](https://www.zhihu.com/equation?tex=U_t) 中。若只考虑参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 对估计值函数 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2Ct%5Cright%29+%5C%5D)的影响而忽略对目标值函数![[公式]](https://www.zhihu.com/equation?tex=U_t)的影响，这种方法并非完全的梯度法，只有部分梯度，因此称为半梯度法：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta_%7Bt%2B1%7D%3D%5Ctheta_t%2B%5Calpha%5Cleft%5BR%2B%5Cgamma%5Chat%7B%5Cupsilon%7D%5Cleft%28S%27%2C%5Ctheta%5Cright%29-%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2C%5Ctheta_t%5Cright%29%5Cright%5D%5Cnabla%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2C%5Ctheta_t%5Cright%29+%5C%5D) (5.7)

![img](https://pic4.zhimg.com/80/v2-70c4e552c21a49b2f586b5412baff5eb_hd.jpg)

图5.4 基于半梯度的TD(0)值函数评估算法



如图5.4所示为基于半梯度的TD(0)值函数评估算法。

![img](https://pic4.zhimg.com/80/v2-4cc8b28fc99cfe8c34a78427d2e78df3_hd.jpg)



图5.5 基于半梯度的Sarsa算法

如图5.5为基于半梯度的Sarsa算法。与表格型强化学习相比，值函数逼近方法中对值函数的更新换成了对参数的更新，参数的学习过程为监督学习。

到目前为止，我们还没有讨论要逼近的值函数的形式。值函数可以采用线性逼近也可以采用非线性逼近。非线性逼近常用的是神经网络。

下面我们仅讨论线性逼近：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Cupsilon%7D%5Cleft%28s%2C%5Ctheta%5Cright%29%3D%5Ctheta%5ET%5Cphi%5Cleft%28s%5Cright%29+%5C%5D)

相比于非线性逼近，线性逼近的好处是只有一个最优值，因此可以收敛到全局最优。其中![[公式]](https://www.zhihu.com/equation?tex=%5Cphi+%28s%29) 为状态s处的特征函数，或者称为基函数。

常用的基函数的类型为：

多项式基函数，如![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cleft%281%2Cs_1%2Cs_2%2Cs_1s_2%2Cs_%7B1%7D%5E%7B2%7D%2Cs_%7B2%7D%5E%7B2%7D%2C%5Ccdots%5Cright%29+%5C%5D)。

傅里叶基函数，![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cphi_i%5Cleft%28s%5Cright%29%3D%5Ccos%5Cleft%28i%5Cpi+s%5Cright%29%2C%5C+s%5Cin%5Cleft%5B0%2C1%5Cright%5D+%5C%5D)

径向基函数：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cphi_i%5Cleft%28s%5Cright%29%3D%5Cexp%5Cleft%28-%5Cfrac%7B%5ClVert+s-c_i%5CrVert%5E2%7D%7B2%5Csigma_%7Bi%7D%5E%7B2%7D%7D%5Cright%29+%5C%5D)

将线性逼近值函数带入随机梯度下降法和半梯度下降法中，可以得到参数的更新公式，如下：

蒙特卡罗方法值函数更新为：

![[公式]](https://www.zhihu.com/equation?tex=+%5CvarDelta%5Ctheta+%3D%5Calpha%5Cleft%5BU_t%5Cleft%28s%5Cright%29-%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2C%5Ctheta_t%5Cright%29%5Cright%5D%5Cnabla%5Chat%7B%5Cupsilon%7D%5Cleft%28S_t%2C%5Ctheta_t%5Cright%29+%5C%5C+%5C%5C+%3D%5Calpha%5Cleft%5BG_t-%5Ctheta%5ET%5Cphi%5Cright%5D%5Cphi+)

TD(0)线性逼近值函数更新为：

![[公式]](https://www.zhihu.com/equation?tex=+%5CvarDelta%5Ctheta+%3D%5Calpha%5Cleft%5BR%2B%5Cgamma%5Ctheta%5ET%5Cphi%5Cleft%28s%27%5Cright%29-%5Ctheta%5ET%5Cphi%5Cleft%28s%5Cright%29%5Cright%5D%5Cphi%5Cleft%28s%5Cright%29+%5C%5C%5C+%3D%5Calpha%5Cdelta%5Cphi%5Cleft%28s%5Cright%29+%5D)

正向视角的![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+TD%5Cleft%28%5Clambda%5Cright%29+%5C%5D)更新为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5CvarDelta%5Ctheta+%3D%5Calpha%5Cleft%28G_%7Bt%7D%5E%7B%5Clambda%7D-%5Ctheta%5ET%5Cphi%5Cright%29%5Cphi+%5C%5D)

后向视角的![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+TD%5Cleft%28%5Clambda%5Cright%29+%5C%5D)更新为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cdelta_t%3DR_%7Bt%2B1%7D%2B%5Cgamma%5Ctheta%5ET%5Cphi%5Cleft%28s%27%5Cright%29-%5Ctheta%5ET%5Cphi%5Cleft%28s%5Cright%29+%5C%5C+E_t%3D%5Cgamma%5Clambda+E_%7Bt-1%7D%2B%5Cphi%5Cleft%28s%5Cright%29+%5C%5C+%5CvarDelta%5Ctheta+%3D%5Calpha%5Cdelta_tE_t+%5C%5D)

前面讨论的是增量式方法更新。增量式方法参数更新过程随机性比较大，尽管计算简单，但样本数据的利用效率并不高。而批的方法，尽管计算复杂，但计算效率高。

所谓批的方法是指给定经验数据集![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+D%3D%5Cleft%5C%7B%5Cleft%3C+s_1%2C%5Cupsilon_%7B1%7D%5E%7B%5Cpi%7D%5Cright%3E%5Ctextrm%7B%EF%BC%8C%7D%5Cleft%3C+s_2%2C%5Cupsilon_%7B2%7D%5E%7B%5Cpi%7D%5Cright%3E%5Ctextrm%7B%EF%BC%8C%7D%5Ccdots%5Ctextrm%7B%EF%BC%8C%7D%5Cleft%3C+s_T%2C%5Cupsilon_%7BT%7D%5E%7B%5Cpi%7D%5Cright%3E%5Cright%5C%7D+%5C%5D)，找到最好的拟合函数![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Cupsilon%7D%5Cleft%28s%2C%5Ctheta%5Cright%29+%5C%5D)，使得：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+LS%5Cleft%28%5Ctheta%5Cright%29%3D%5Csum_%7Bt%3D1%7D%5ET%7B%5Cleft%28%5Cupsilon_%7Bt%7D%5E%7B%5Cpi%7D-%5Chat%7B%5Cupsilon%7D_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28s_t%2C%5Ctheta%5Cright%29%5Cright%29%7D%5E2+%5C%5D)最小可利用线性最小二乘逼近：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5CvarDelta%5Ctheta+%3D%5Calpha%5Csum_%7Bt%3D1%7D%5ET%7B%5Cleft%5B%5Cupsilon_%7Bt%7D%5E%7B%5Cpi%7D-%5Ctheta%5ET%5Cphi%5Cleft%28s_t%5Cright%29%5Cright%5D%7D%5Cphi%5Cleft%28s_t%5Cright%29%3D0+%5C%5D)
最小二乘蒙特卡罗方法参数为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta+%3D%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET%7B%5Cphi%5Cleft%28s_t%5Cright%29%5Cphi%5Cleft%28s_t%5Cright%29%5ET%7D%5Cright%29%5E%7B-1%7D%5Csum_%7Bt%3D1%7D%5ET%7B%5Cphi%5Cleft%28s_t%5Cright%29G_t%7D+%5C%5D)
最小二乘差分方法为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta+%3D%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET%7B%5Cphi%5Cleft%28s_t%5Cright%29%5Cleft%28%5Cphi%5Cleft%28s_t%5Cright%29-%5Cgamma%5Cphi%5Cleft%28%5Ctextrm%7Bs%7D_%7Bt%2B1%7D%5Cright%29%5Cright%29%5ET%7D%5Cright%29%5E%7B-1%7D%5Csum_%7Bt%3D1%7D%5ET%7B%5Cphi%5Cleft%28s_t%5Cright%29R_%7Bt%2B1%7D%7D+%5C%5D)
最小二乘![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+TD%5Cleft%28%5Clambda%5Cright%29+%5C%5D)方法为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta+%3D%5Cleft%28%5Csum_%7Bt%3D1%7D%5ET%7B%5Ctextrm%7BE%7D_t%5Cleft%28%5Cphi%5Cleft%28s_t%5Cright%29-%5Cgamma%5Cphi%5Cleft%28%5Ctextrm%7Bs%7D_%7Bt%2B1%7D%5Cright%29%5Cright%29%5ET%7D%5Cright%29%5E%7B-1%7D%5Csum_%7Bt%3D1%7D%5ET%7BE_tR_%7Bt%2B1%7D%7D+%5C%5D)

# 强化学习进阶 第六讲 策略梯度方法

说明：从这讲开始，我们进入强化学习的进阶课程学习。进阶课程以强化学习入门第一讲到第五讲为基础，所以请读者先读前面的课程讲义。该进阶课程也有五讲，主要讲解直接策略搜索方法。内容涉及到近十几年比较主流的直接策略搜索方法。本课程参考资料是Pieter Abbeel 在NIPS2016给的tutorial，视频网址为：

[Deep Reinforcement Learning Through Policy Optimization](https://link.zhihu.com/?target=https%3A//channel9.msdn.com/Events/Neural-Information-Processing-Systems-Conference/Neural-Information-Processing-Systems-Conference-NIPS-2016/Deep-Reinforcement-Learning-Through-Policy-Optimization)

![img](https://pic3.zhimg.com/80/v2-1f19fca1bc928851796289d380eaffde_hd.jpg)



图6.1 强化学习分类

如图6.1所示为强化学习的分类示意图，我们这一节讲解策略搜索方法。

我们首先要弄清楚，什么是策略搜索？

回忆第一讲到第五讲的内容，前面讲的是值函数的方法。广义值函数的方法包括两个步骤：策略评估+策略改善。当值函数最优时，策略是最优的。此时最优策略是贪婪策略。我们回忆下什么是贪婪策略。贪婪策略是指![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+arg%5C%5Cmax_a%5C+Q_%7B%5Ctheta%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D) ，即在状态s，对应最大行为值函数的动作，是一个状态空间向动作空间的映射，该映射就是最有策略。利用这种方法得到的策略往往是状态空间向有限集动作空间的映射。

策略搜索是将策略进行参数化即![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29+%5C%5D) ，利用线性或非线性（如神经网络）对策略进行表示，寻找最优的参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)使得强化学习的目标：累积回报的期望![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+E%5Cleft%5B%5Csum_%7Bt%3D0%7D%5EH%7BR%5Cleft%28s_t%5Cright%29%7C%5Cpi_%7B%5Ctheta%7D%7D%5Cright%5D+%5C%5D)最大。

在值函数的方法中，我们迭代计算的是值函数，然后根据值函数对策略进行改进；而在策略搜索方法中，我们直接对策略进行迭代计算，也就是迭代更新参数值，直到累积回报的期望最大，此时的参数所对应的策略为最优策略。

在正式讲解策略搜索方法之前，我们先比较一下值函数方法和直接策略搜索方法的优缺点。其实正因为直接策略搜索方法比值函数方法拥有更多的优点，我们才有理由或才有动机去研究和学习及改进直接策略搜索方法：

（1）
直接策略搜索方法是对策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)进行参数化表示，与值函数方中对值函数进行参数化表示相比，策略参数化更简单，有更好的收敛性。

（2）
利用值函数方法求解最优策略时，策略改进需要求解![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+arg%5C%5Cmax_a%5C+Q_%7B%5Ctheta%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D)，当要解决的问题动作空间很大或者动作为连续集时，该式无法有效求解。

（3）
直接策略搜索方法经常采用的随机策略，能够学习随机策略。可以将探索直接集成到策略之中。

当然与值函数方法相比，策略搜索方法也普遍存在一些缺点，比如：

（1）
策略搜索的方法容易收敛到局部最小值。

（2）
评估单个策略时并不充分，方差较大。

对于这些缺点，最近十几年，学者们正在探索各种直接策略搜索的方法进行改进。

策略搜索方法已经成功应用的案例如图6.2所示

![img](https://pic1.zhimg.com/80/v2-c5a7dae62de981daaf57628a231b77fc_hd.jpg)

图6.2 直接策略搜索方法成功案例



从图6.2我们看出，直接策略搜索的方法主要应用在机器人领域。本节主要讲解策略梯度的方法，第七节会讲TRPO的方法，第八节会讲确定性策略搜索方法，第九节会讲GPS方法，第十节会讲逆向强化学习方法。这些方法之间的关系可用图6.3表示。

![img](https://pic1.zhimg.com/80/v2-1ba71cff16c4f9f320abcac65c06a92c_hd.jpg)



图6.3 策略搜索方法分类



策略搜索方法按照是否利用模型可以分为无模型的策略搜索方法和基于模型的策略搜索方法。其中无模型的策略搜索方法根据策略是采用随机策略还是确定性策略分为随机策略搜索方法和确定性策略搜索方法。随机策略搜索方法最先发展起来的是策略梯度方法；然而策略梯度方法存在着学习速率难以确定等问题，为了解决这个问题学者们提出了基于统计学习的方法，基于路径积分的方法，回避学习速率问题。而TRPO并没有回避这个问题，而是找到了替代损失函数，利用优化方法局部找到使得损失函数单调的步长，我会在下一讲重点讲解。



**基于似然率来推导策略梯度**

用![[公式]](https://www.zhihu.com/equation?tex=%5Ctau)表示一组状态-行为序列![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+s_0%2Cu_0%2C%5Ccdots+%2Cs_H%2Cu_H+%5C%5D), 符号![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+R%5Cleft%28%5Ctau%5Cright%29%3D%5Csum_%7Bt%3D0%7D%5EH%7BR%5Cleft%28s_t%2Cu_t%5Cright%29%7D+%5C%5D),表示轨迹![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 的回报，![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29) 表示轨迹![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 出现的概率；则强化学习的目标函数可表示为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+U%5Cleft%28%5Ctheta%5Cright%29%3DE%5Cleft%28%5Csum_%7Bt%3D0%7D%5EH%7BR%5Cleft%28s_t%2Cu_t%5Cright%29%3B%5Cpi_%7B%5Ctheta%7D%7D%5Cright%29%3D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5D)

强化学习的目标是找到最优参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 使得：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmax_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%3D%5Cmax_%7B%5Ctheta%7D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5D)

这时，策略搜索方法，实际上变成了一个优化问题。解决优化问题有很多种方法，比如：最速下降法，牛顿法，内点法等等。

其中最简单，也是最常用的是最速下降法，此处称为策略梯度的方法。

即![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta_%7Bnew%7D%3D%5Ctheta_%7Bold%7D%2B%5Calpha%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29+%5C%5D) 。

问题的关键是如何计算策略梯度![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29+%5C%5D)。

我们对目标函数进行求导：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%3D%5Cnabla_%7B%5Ctheta%7D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5C+%3D%5Csum_%7B%5Ctau%7D%7B%5Cnabla_%7B%5Ctheta%7DP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5C+%3D%5Csum_%7B%5Ctau%7D%7B%5Cfrac%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%7D%5Cnabla_%7B%5Ctheta%7DP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5C+%3D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7DP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%7D%7D+%5C%5C+%3D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5D) (6.1)



最终策略梯度变成了求![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29+%5C%5D) 的期望，我们可以利用经验平均来进行估算。因此，当利用当前策略![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi_%7B%5Ctheta%7D+%5C%5D) 采样m条轨迹后，可以利用这m条轨迹的经验平均对策略梯度进行逼近：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%5Capprox%5Chat%7Bg%7D%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5D) (6.2)

**从重要性采样的角度推导策略梯度：**

目标函数为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+U%5Cleft%28%5Ctheta%5Cright%29%3DE%5Cleft%28%5Csum_%7Bt%3D0%7D%5EH%7BR%5Cleft%28s_t%2Cu_t%5Cright%29%3B%5Cpi_%7B%5Ctheta%7D%7D%5Cright%29%3D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5D)

利用参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bold%7D)产生的数据去评估参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)的回报期望，由重要性采样得到：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+U%5Cleft%28%5Ctheta%5Cright%29%3D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%7C%5Ctheta_%7Bold%7D%5Cright%29%5Cfrac%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%7D%7BP%5Cleft%28%5Ctau+%7C%5Ctheta_%7Bold%7D%5Cright%29%7DR%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5C+%3DE_%7B%5Ctau+%5Csim%5Ctheta_%7Bold%7D%7D%5Cleft%5B%5Cfrac%7BP%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29%7D%7BP%5Cleft%28%5Ctau+%7C%5Ctheta_%7Bold%7D%5Cright%29%7DR%5Cleft%28%5Ctau%5Cright%29%5Cright%5D+%5C%5D) (6.3)

导数为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%3DE_%7B%5Ctau+%5Csim%5Ctheta_%7Bold%7D%7D%5Cleft%5B%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7DP%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29%7D%7BP%5Cleft%28%5Ctau+%7C%5Ctheta_%7Bold%7D%5Cright%29%7DR%5Cleft%28%5Ctau%5Cright%29%5Cright%5D+%5C%5D) (6.4)



令![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta+%3D%5Ctheta_%7Bold%7D+%5C%5D) ，我们可以得到当前策略的导数：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%7C_%7B%5Ctheta+%3D%5Ctheta_%7Bold%7D%7D+%5C%5C+%3DE_%7B%5Ctau+~%5Csim%5Ctheta_%7Bold%7D%7D%5Cleft%5B%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7DP%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29%7C_%7B%5Ctheta_%7Bold%7D%7D%7D%7BP%5Cleft%28%5Ctau+%7C%5Ctheta_%7Bold%7D%5Cright%29%7DR%5Cleft%28%5Ctau%5Cright%29%5Cright%5D+%5C%5C+%3DE_%7B%5Ctau+%5Csim%5Ctheta_%7Bold%7D%7D%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29%7C_%7B%5Ctheta_%7Bold%7D%7DR%5Cleft%28%5Ctau%5Cright%29%5Cright%5D+%5C%5D) (6.5)

从重要性采样的视角推导策略梯度，不仅得到了与似然率相同的结果，更重要的是得到了原来目标函数新的损失函数：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+U%5Cleft%28%5Ctheta%5Cright%29%3DE_%7B%5Ctau+%5Csim%5Ctheta_%7Bold%7D%7D%5Cleft%5B%5Cfrac%7BP%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29%7D%7BP%5Cleft%28%5Ctau+%7C%5Ctheta_%7Bold%7D%5Cright%29%7DR%5Cleft%28%5Ctau%5Cright%29%5Cright%5D+%5C%5D)

**似然率策略梯度的直观理解**

前面我们已经利用似然率的方法推导得到了策略梯度公式为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%5Capprox%5Chat%7Bg%7D%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5D)

对这个公式中的两项，我们分别进行理解。

![img](https://pic1.zhimg.com/80/v2-772dc73dd3d3520d0a6299c9b0b54e24_hd.jpg)



图6.4 策略梯度的直观理解示意图

其中第一项![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29+%5C%5D) 是轨迹![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 的概率随参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 变化最陡的方向，参数在该方向进行更新时，若沿着正方向，则该轨迹![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 的概率会变大，而沿着负方向进行更新时，该轨迹![[公式]](https://www.zhihu.com/equation?tex=%5Ctau)的概率会变大。再看第二项![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+R%5Cleft%28%5Ctau%5Cright%29+%5C%5D) ，该项控制了参数更新的方向和步长。![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+R%5Cleft%28%5Ctau%5Cright%29+%5C%5D)为正且越大则参数更新后该轨迹的概率越大；![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+R%5Cleft%28%5Ctau%5Cright%29+%5C%5D)为负，则降低该轨迹的概率，抑制该轨迹的发生。

因此，策略梯度从直观上进行理解时，我们发现策略梯度会增加高回报路径的概率，减小低回报路径的概率。

前面我们已经推导出策略梯度的求解公式为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%5Capprox%5Chat%7Bg%7D%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5Cright%29%7D+%5C%5D)

现在，我们解决如何求似然率的梯度：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%5Ctextrm%7B%EF%BC%9F%7D+%5C%5D)

已知![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctau+%3Ds_0%2Cu_0%2C%5Ccdots+%2Cs_H%2Cu_H+%5C%5D) ，则轨迹的似然率可写为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+P%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%3B%5Ctheta%5Cright%29%3D%5Cprod_%7Bt%3D0%7D%5EH%7BP%5Cleft%28s_%7Bt%2B1%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Cu_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%5Ccdot%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29+%5C%5D) (6.6)

式中，![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+P%5Cleft%28s_%7Bt%2B1%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Cu_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29+%5C%5D)表示动力学，式中无参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)，因此可在求导过程中消掉。具体推导，如式（6.7）

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%3B%5Ctheta%5Cright%29%3D%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cleft%5B%5Cprod_%7Bt%3D0%7D%5EH%7BP%5Cleft%28s_%7Bt%2B1%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Cu_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%5Ccdot%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%5Cright%5D+%5C%5C+%3D%5Cnabla_%7B%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5EH%7B%5Clog+P%5Cleft%28s_%7Bt%2B1%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Cu_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%2B%5Csum_%7Bt%3D0%7D%5EH%7B%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%5Cright%5D+%5C%5C+%3D%5Cnabla_%7B%5Ctheta%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5EH%7B%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%5Cright%5D+%5C%5C+%3D%5Csum_%7Bt%3D0%7D%5EH%7B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D+%5C%5D) (6.7)

从（6.7）的结果来看，似然率梯度转化为动作策略的梯度，与动力学无关。那么，如何求解策略的梯度呢？

下面，我们看一下常见的**策略表示方法**：

一般，随机策略可以写为确定性策略加随机部分，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi_%7B%5Ctheta%7D%3D%5Cmu_%7B%5Ctheta%7D%2B%5Cvarepsilon+%5C%5D)

对于高斯策略：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cvarepsilon+%5Csim+N%5Cleft%280%2C%5Csigma%5E2%5Cright%29+%5C%5D) ，是均值为零，标准差为![[公式]](https://www.zhihu.com/equation?tex=%5Csigma+)的高斯分布。像值函数逼近一样，确定性部分常见的表示为：

线性策略：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmu%5Cleft%28s%5Cright%29%3D%5Cphi%5Cleft%28s%5Cright%29%5ET%5Ctheta+%5C%5D)

径向基策略：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%3D%5Comega%5ET%5Cphi%5Cleft%28s%5Cright%29%2C%5C+%5C%5D)其中：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cphi_i%5Cleft%28s%5Cright%29%3D%5Cexp%5Cleft%28-%5Cfrac%7B1%7D%7B2%7D%5Cleft%28s-%5Cmu_i%5Cright%29%5ETD_i%5Cleft%28s-%5Cmu_i%5Cright%29%5Cright%29+%5C%5D)

参数为![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta+%3D%5Cleft%5C%7B%5Comega+%2C%5Cmu_i%2Cd_i%5Cright%5C%7D+%5C%5D)

我们以确定性部分策略为线性策略为例来说明![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29+%5C%5D) 如何计算。

首先![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28u%7Cs%5Cright%29%5Csim%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%5Cexp%5Cleft%28-%5Cfrac%7B%5Cleft%28u-%5Cphi%5Cleft%28s%5Cright%29%5ET%5Ctheta%5Cright%29%5E2%7D%7B2%5Csigma%5E2%7D%5Cright%29+%5C%5D) ，利用该分布进行采样，得到![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D+%5C%5D) ，然后将![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cleft%28s_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Cu_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29+%5C%5D)带入，得到：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%3D%5Cfrac%7B%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D-%5Cphi%5Cleft%28s_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%5ET%5Ctheta%5Cright%29%5Cphi%5Cleft%28s_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%7B%5Csigma%5E2%7D+%5C%5D)

其中方差参数![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csigma%5E2+%5C%5D) 用来控制策略的探索性。

由此，我们已经推导除了策略梯度的计算公式：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%5Capprox%5Chat%7Bg%7D%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Cleft%28%5Csum_%7Bt%3D0%7D%5EH%7B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7DR%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%5Cright%29%7D+%5C%5D) (6.8)

（6.8）式给出的策略梯度是无偏的，但是方差很大。我们在回报中引入常数基线b来减小方差。

**首先，证明当回报中引入常数b时，策略梯度不变即：**

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%5Capprox%5Chat%7Bg%7D%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%3B%5Ctheta%5Cright%29R%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D+%5C%5C+%3D%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%3B%5Ctheta%5Cright%29%5Cleft%28R%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29-b%5Cright%29%7D+%5C%5D)

**证明：**

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+E%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29b%5Cright%5D+%5C%5C+%3D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%7D%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29b+%5C%5C+%3D%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%7D%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7DP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29b%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%7D+%5C%5C+%3D%5Csum_%7B%5Ctau%7D%7B%5Cnabla_%7B%5Ctheta%7DP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29b%7D+%5C%5C+%3D%5Cnabla_%7B%5Ctheta%7D%5Cleft%28%5Csum_%7B%5Ctau%7D%7BP%5Cleft%28%5Ctau+%3B%5Ctheta%5Cright%29%7Db%5Cright%29+%5C%5C+%3D%5Cnabla_%7B%5Ctheta%7Db+%5C%5C+%3D0+%5C%5D)

**其次，我们求使得策略梯度的方差最小时的基线b**

令![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+X%3D%5Cnabla_%7B%5Ctheta%7D%5Clog+P%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%3B%5Ctheta%5Cright%29%5Cleft%28R%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29-b%5Cright%29+%5C%5D) ，则方差为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Var%5Cleft%28X%5Cright%29%3DE%5Cleft%28X-%5Cbar%7BX%7D%5Cright%29%5E2%3DEX%5E2-E%5Cbar%7BX%7D%5E2+%5C%5D)

方差最小处，方差对b的导数为零，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cfrac%7B%5Cpartial+Var%5Cleft%28X%5Cright%29%7D%7B%5Cpartial+b%7D%3DE%5Cleft%28X%5Cfrac%7B%5Cpartial+X%7D%7B%5Cpartial+b%7D%5Cright%29%3D0+%5C%5D)

其中![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cbar%7BX%7D%3DEX+%5C%5D)与b无关。

将X带入，得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+b%3D%5Cfrac%7B%5Csum_%7Bi%3D1%7D%5Em%7B%5Cleft%5B%5Cleft%28%5Csum_%7Bt%3D0%7D%5EH%7B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%5Cright%29%5E2R%5Cleft%28%5Ctau%5Cright%29%5Cright%5D%7D%7D%7B%5Csum_%7Bi%3D1%7D%5Em%7B%5Cleft%5B%5Cleft%28%5Csum_%7Bt%3D0%7D%5EH%7B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%5Cright%29%5E2%5Cright%5D%7D%7D+%5C%5D) (6.9)

**进一步减小方差的方法：修改回报函数：**

引入基线后，策略梯度公式变为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%5Capprox%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Cleft%28%5Csum_%7Bt%3D0%7D%5EH%7B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%5Cleft%28R%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29-b%5Cright%29%5Cright%29%7D+%5C%5D) (6.10)

其中，b取（6.9）式。我们对公式（6.10）进行进一步分析。在（6.10）中，每个动作![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D+%5C%5D)所对应的![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29+%5C%5D)都乘以相同的该轨迹的总回报![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cleft%28R%5Cleft%28%5Ctau%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29-b%5Cright%29+%5C%5D) ，如图6.5所示。

![img](https://pic2.zhimg.com/80/v2-80ba3ac1a6ee374aa23045cacf596d01_hd.jpg)

图6.5 REINFORCE方法

然而，当前的动作与过去的回报实际上是没有关系的，即![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+E_p%5Cleft%5B%5Cpartial_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_t%7Cx_t%2Ct%5Cright%29r_j%5Cright%5D%3D0%5C%5C+for%5C%5C+j%3Ct+%5C%5D)

因此，我们可以修改（6.10）中的回报函数。存在两种修改方法：

第一种称为（G(PO)MDP）:

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%5Capprox%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Csum_%7Bj%3D0%7D%5E%7BH-1%7D%7B%5Cleft%28%5Csum_%7Bt%3D0%7D%5Ej%7B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%5Cleft%28r_j-b_j%5Cright%29%7D%5Cright%29%7D%7D+%5C%5D)

![img](https://pic2.zhimg.com/80/v2-f847d83072c35f75fd2c4e9e4afe7c99_hd.jpg)



图6.6 G(PO)MDP方法



第二种称为策略梯度理论：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DU%5Cleft%28%5Ctheta%5Cright%29%5Capprox%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi%3D1%7D%5Em%7B%5Csum_%7Bt%3D0%7D%5E%7BH-1%7D%7B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%7Cs_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%7D%5Cleft%28%5Csum_%7Bk%3Dt%7D%5E%7BH-1%7D%7B%5Cleft%28R%5Cleft%28s_%7Bk%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29-b%5Cright%29%7D%5Cright%29%7D+%5C%5D)

![img](https://pic2.zhimg.com/80/v2-2d6966aa40b699c9f55a7d02f1718c19_hd.jpg)



图6.7 策略梯度理论



为了使得方差最小，可以利用前面的方法求解相应的基线b。

# 强化学习进阶 第七讲 TRPO

强化学习进阶 第七讲 TRPO



今天开始我们的第七讲，TRPO。先简短地介绍一下：TRPO是英文单词Trust region policy optimization的简称，翻译成中文是信赖域策略优化。提出这个算法的人是伯克利的博士生John Schulman，此人已于2016年博士毕业。Schulman的导师是强化学习领域的大神Pieter Abbeel, Abbeel是伯克利的副教授，同时也是OpenAI的研究科学家，是机器人强化学习领域最有影响力的人之一。

如果进一步追根溯源的话，Abbeel毕业于斯坦福大学，导师是Andrew Ng（吴恩达）。相信搞机器学习的人应该都听说过此大神或者听过他的课吧。有意思的是，吴恩达博士毕业于伯克利，之后在斯坦福任教，这跟Abbeel的经历正好相反。看来美国名校间人才互换的情况还是挺普遍的。Abbeel博士做的课题是逆向强化学习（学徒学习）。如果再进一步追根溯源，吴恩达的导师是伯克利的Michael I. Jordan，一个将统计学和机器学习联合起来的大师级人物……

扯的好像有点多了，其实不然。说那么多其实跟今天的主题有关系。从师承关系我们可以看到，这个学派由统计学大师Michael I. Jordan传下来，所以他们最有力的杀手锏是统计学习。从宏观意义上来讲，TRPO将统计玩到了一个新高度。在TRPO出来之前，大部分强化学习算法很难保证单调收敛，而TRPO却给出了一个单调的策略改善方法。所以，不管你从事什么行业，想用强化学习解决你的问题，TRPO是一个不错的选择。所以，这一节，很关键。好了，现在我们正式进入这一讲



![img](https://pic2.zhimg.com/80/v2-e519a12e0617dd0eb66de29db96af429_hd.jpg)

图7.1 策略搜索方法分类

**策略梯度的缺点**

上一节，我们已经讲了策略梯度的方法。当然策略梯度方法博大精深，上一讲只是给出一个入门的介绍，在策略梯度方法中还有很多有意思的课题，比如相容函数法，自然梯度法等等。但Shulman在博士论文中已证明，这些方法其实都是TRPO弱化的特例，说这些是再次强调TRPO的强大之处。

我们知道，根据策略梯度方法，参数更新方程式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctheta_%7Bnew%7D%3D%5Ctheta_%7Bold%7D%2B%5Calpha%5Cnabla_%7B%5Ctheta%7DJ+%5C%5D) （7.1）

策略梯度算法的硬伤就在更新步长 ![[公式]](https://www.zhihu.com/equation?tex=%5Calpha) ，当步长不合适时，更新的参数所对应的策略是一个更不好的策略，当利用这个更不好的策略进行采样学习时，再次更新的参数会更差，因此很容易导致越学越差，最后崩溃。所以，合适的步长对于强化学习非常关键。

什么叫合适的步长？

所谓合适的步长是指当策略更新后，回报函数的值不能更差。如何选择这个步长？或者说，如何找到新的策略使得新的回报函数的值单调增，或单调不减。这是TRPO要解决的问题。

用 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctau) 表示一组状态-行为序列 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+s_0%2Cu_0%2C%5Ccdots+%2Cs_H%2Cu_H+%5C%5D) ，强化学习的回报函数为：

![[公式]](https://www.zhihu.com/equation?tex=+%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%3DE_%7B%5Ctau+%7C%5Ctilde%7B%5Cpi%7D%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5Et%5Cleft%28r%5Cleft%28s_t%5Cright%29%5Cright%29%7D%5Cright%5D+)



这里，我们用 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cpi%7D) 表示策略。

刚才已经说过，TRPO是找到新的策略，使得回报函数单调不减，一个自然地想法是能不能将新的策略所对应的回报函数分解成旧的策略所对应的回报函数+其他项。只要新的策略所对应的其他项大于等于零，那么新的策略就能保证回报函数单调不减。其实是存在这样的等式，这个等式是2002年Sham Kakade提出来的。TRPO的起点便是这样一个等式：

![[公式]](https://www.zhihu.com/equation?tex=+%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%3D%5Ceta%5Cleft%28%5Cpi%5Cright%29%2BE_%7Bs_0%2Ca_0%2C%5Ccdots+~%5Ctilde%7B%5Cpi%7D%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EtA_%7B%5Cpi%7D%5Cleft%28s_t%2Ca_t%5Cright%29%7D%5Cright%5D+) (7.2)

这里我们用 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi) 表示旧的策略，用 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cpi%7D) 表示新的策略。其中， ![[公式]](https://www.zhihu.com/equation?tex=+A_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3DQ_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-V_%7B%5Cpi%7D%5Cleft%28s%5Cright%29+%5C%5C+%3DE_%7Bs%27~P%5Cleft%28s%27%7Cs%2Ca%5Cright%29%7D%5Cleft%5Br%5Cleft%28s%5Cright%29%2B%5Cgamma+V%5E%7B%5Cpi%7D%5Cleft%28s%27%5Cright%29-V%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29%5Cright%5D)

称为优势函数。

此处我们再花点笔墨介绍下 ![[公式]](https://www.zhihu.com/equation?tex=+Q_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-V_%7B%5Cpi%7D%5Cleft%28s%5Cright%29+) 为什么称为优势函数，这个优势到底跟谁比。还是以大家熟悉的树状图来讲解：



![img](https://pic3.zhimg.com/80/v2-05a037b139af7efc98e97e7b13bb7882_hd.jpg)

图7.2 优势函数示意图

如图7.2，值函数 ![[公式]](https://www.zhihu.com/equation?tex=V%28s%29) 可以理解为在该状态下所有可能动作所对应的动作值函数乘以采取该动作的概率的和。更通俗的讲，值函数 ![[公式]](https://www.zhihu.com/equation?tex=V%28s%29) 是该状态下所有动作值函数关于动作概率的平均值。而动作值函数 ![[公式]](https://www.zhihu.com/equation?tex=Q%28s%2Ca%29) 是单个动作所对应的值函数， ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-V_%7B%5Cpi%7D%5Cleft%28s%5Cright%29+%5C%5D) 能评价当前动作值函数相对于平均值的大小。所以，这里的优势指的是动作值函数相比于当前状态的值函数的优势。如果优势函数大于零，则说明该动作比平均动作好，如果优势函数小于零，则说明当前动作还不如平均动作好。

回到正题上来，我们给出公式（7.2）的证明：

证明：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+E_%7B%5Ctau+%7C%5Ctilde%7B%5Cpi%7D%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5EtA_%7B%5Cpi%7D%5Cleft%28s_t%2Ca_t%5Cright%29%7D%5Cright%5D+%5C%5C+%3DE_%7B%5Ctau+%7C%5Ctilde%7B%5Cpi%7D%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5Et%5Cleft%28r%5Cleft%28s%5Cright%29%2B%5Cgamma+V%5E%7B%5Cpi%7D%5Cleft%28s_%7Bt%2B1%7D%5Cright%29-V%5E%7B%5Cpi%7D%5Cleft%28s_t%5Cright%29%5Cright%29%7D%5Cright%5D+%5C%5C+%3DE_%7B%5Ctau+%7C%5Ctilde%7B%5Cpi%7D%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5Et%5Cleft%28r%5Cleft%28s_t%5Cright%29%5Cright%29%2B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5Et%5Cleft%28%5Cgamma+V%5E%7B%5Cpi%7D%5Cleft%28s_%7Bt%2B1%7D%5Cright%29-V%5E%7B%5Cpi%7D%5Cleft%28s_t%5Cright%29%5Cright%29%7D%7D%5Cright%5D+%5C%5C+%3DE_%7B%5Ctau+%7C%5Ctilde%7B%5Cpi%7D%7D%5Cleft%5B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Cgamma%5Et%5Cleft%28r%5Cleft%28s_t%5Cright%29%5Cright%29%7D%5Cright%5D%2BE_%7Bs_0%7D%5Cleft%5B-V%5E%7B%5Cpi%7D%5Cleft%28s_0%5Cright%29%5Cright%5D+%5C%5C+%3D%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29-%5Ceta%5Cleft%28%5Cpi%5Cright%29+%5C%5D)

我们详细讲解一下：

第一个等号是将优势函数的定义带入。

第二个等号是把第一项和后两项分开来写。

第三个等号是将第二项写开来，相消，只剩 ![[公式]](https://www.zhihu.com/equation?tex=+-V%5E%7B%5Cpi%7D%5Cleft%28s_0%5Cright%29+) ，而 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+s_0+%5Csim%5Ctilde%7B%5Cpi%7D+%5C%5D) 等价于 ![[公式]](https://www.zhihu.com/equation?tex=s_0~%5Csim%5Cpi+) ，因为两个策略都从同一个初始状态开始。而 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+V%5E%7B%5Cpi%7D%5Cleft%28s_0%5Cright%29%3D%5Ceta%5Cleft%28%5Cpi%5Cright%29+%5C%5D)



![img](https://pic1.zhimg.com/80/v2-0cdc230c7c8332bac24c5ef2b78ad140_hd.jpg)

图7.3 TRPO最重要的等式

为了在等式（7.2）中出现策略项，我们需要对公式（7.2）进一步加工转化。如图7.3，我们对新旧策略回报差进行转化。优势函数的期望可以写成如下式：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%3D%5Ceta%5Cleft%28%5Cpi%5Cright%29%2B%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Csum_s%7BP%5Cleft%28s_t%3Ds%7C%5Ctilde%7B%5Cpi%7D%5Cright%29%7D%7D%5Csum_a%7B%5Ctilde%7B%5Cpi%7D%5Cleft%28a%7Cs%5Cright%29%5Cgamma%5EtA_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%7D+%5C%5D) (7.3)

其中 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+P%5Cleft%28s_t%3Ds%7C%5Ctilde%7B%5Cpi%7D%5Cright%29%5Ctilde%7B%5Cpi%7D%5Cleft%28a%7Cs%5Cright%29+%5C%5D) 为 ![[公式]](https://www.zhihu.com/equation?tex=%28s%2Ca%29) 的联合概率， ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csum_a%7B%5Ctilde%7B%5Cpi%7D%5Cleft%28a%7Cs%5Cright%29%5Cgamma%5EtA_%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%7D+%5C%5D) 为求对动作 ![[公式]](https://www.zhihu.com/equation?tex=a) 的边际分布，也就是说在状态s对整个动作空间求和； ![[公式]](https://www.zhihu.com/equation?tex=+%5Csum_s%7BP%5Cleft%28s_t%3Ds%7C%5Ctilde%7B%5Cpi%7D%5Cright%29%7D+) 为求对状态 ![[公式]](https://www.zhihu.com/equation?tex=s) 的边际分布，即对整个状态空间求和； ![[公式]](https://www.zhihu.com/equation?tex=+%5Csum_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%7B%5Csum_s%7BP%5Cleft%28s_t%3Ds%7C%5Ctilde%7B%5Cpi%7D%5Cright%29%7D%7D+) 求整个时间序列的和。

我们定义 ![[公式]](https://www.zhihu.com/equation?tex=+%5Crho_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3DP%5Cleft%28s_0%3Ds%5Cright%29%2B%5Cgamma+P%5Cleft%28s_1%3Ds%5Cright%29%2B%5Cgamma%5E2P%5Cleft%28s_2%3Ds%5Cright%29%2B%5Ccdots+)

则：

![[公式]](https://www.zhihu.com/equation?tex=+%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%3D%5Ceta%5Cleft%28%5Cpi%5Cright%29%2B%5Csum_s%7B%5Crho_%7B%5Ctilde%7B%5Cpi%7D%7D%5Cleft%28s%5Cright%29%5Csum_a%7B%5Ctilde%7B%5Cpi%7D%5Cleft%28a%7Cs%5Cright%29A%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%7D%7D+) （7.4）

如图所示：



![img](https://pic1.zhimg.com/80/v2-a00549d1831123604c013842c353f23c_hd.jpg)

图7.4 代价函数推导

注意，这时状态s的分布由新的策略产生，对新的策略严重依赖。

**TRPO第一个技巧**

这时，我们引入TRPO的第一个技巧对状态分布进行处理。我们忽略状态分布的变化，依然采用旧的策略所对应的状态分布。这个技巧是对原代价函数的第一次近似。其实，当新旧参数很接近时，我们将用旧的状态分布代替新的状态分布也是合理的。这时，原来的代价函数变成了：

![[公式]](https://www.zhihu.com/equation?tex=+L_%7B%5Cpi%7D%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%3D%5Ceta%5Cleft%28%5Cpi%5Cright%29%2B%5Csum_s%7B%5Crho_%7B%5Cpi%7D%5Cleft%28s%5Cright%29%5Csum_a%7B%5Ctilde%7B%5Cpi%7D%5Cleft%28a%7Cs%5Cright%29A%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%7D%7D+) (7.5)

我们再看(7.5)式的第二项策略部分，这时的动作a是由新的策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cpi%7D) 产生。可是新的策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cpi%7D) 是带参数 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 的，这个参数是未知的，因此无法用来产生动作。这时，我们引入TRPO的第二个技巧。

**TRPO第二个技巧**

TRPO的第二个技巧是利用重要性采样对动作分布进行的处理。

![[公式]](https://www.zhihu.com/equation?tex=%5Csum_a%7B%5Ctilde%7B%5Cpi%7D_%7B%5Ctheta%7D%5Cleft%28a%7Cs_n%5Cright%29A_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28s_n%2Ca%5Cright%29%3DE_%7Ba~q%7D%5Cleft%5B%5Cfrac%7B%5Ctilde%7B%5Cpi%7D_%7B%5Ctheta%7D%5Cleft%28a%7Cs_n%5Cright%29%7D%7Bq%5Cleft%28a%7Cs_n%5Cright%29%7DA_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28s_n%2Ca%5Cright%29%5Cright%5D%7D+)

通过利用两个技巧，我们再利用 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cfrac%7B1%7D%7B1-%5Cgamma%7DE_%7Bs~%5Crho_%7B%5Ctheta_%7Bold%7D%7D%7D%5Cleft%5B%5Ccdots%5Cright%5D+) 代替 ![[公式]](https://www.zhihu.com/equation?tex=+%5Csum_s%7B%5Crho_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28s%5Cright%29%7D%5Cleft%5B%5Ccdots%5Cright%5D+) ；取 ![[公式]](https://www.zhihu.com/equation?tex=+q%5Cleft%28a%7Cs_n%5Cright%29%3D%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28a%7Cs_n%5Cright%29+) ；

替代回报函数变为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L_%7B%5Cpi%7D%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%3D%5Ceta%5Cleft%28%5Cpi%5Cright%29%2BE_%7Bs~%5Crho_%7B%5Ctheta_%7Bold%7D%7D%2Ca~%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%7D%5Cleft%5B%5Cfrac%7B%5Ctilde%7B%5Cpi%7D_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29%7D%7B%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28a%7Cs%5Cright%29%7DA_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5D) (7.6)

接下来，我们看一下**替代回报函数（7.6）和原回报函数（7.4）有什么关系**

通过比较我们发现，（7.4）和（7.6）唯一的区别是状态分布的不同。将 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L_%7B%5Cpi%7D%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%5Ctextrm%7B%EF%BC%8C%7D%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29+%5C%5D) 都看成是策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cpi%7D) 的函数，则 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L_%7B%5Cpi%7D%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%5Ctextrm%7B%EF%BC%8C%7D%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29+%5C%5D) 在策略 ![[公式]](https://www.zhihu.com/equation?tex=+%5Cpi_%7B%5Ctheta_%7Bold%7D%7D+) 处一阶近似，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L_%7B%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%7D%5Cleft%28%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cright%29%3D%5Ceta%5Cleft%28%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cright%29+%5C%5C+%5Cnabla_%7B%5Ctheta%7DL_%7B%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%7D%5Cleft%28%5Cpi_%7B%5Ctheta%7D%5Cright%29%7C_%7B%5Ctheta+%3D%5Ctheta_%7Bold%7D%7D%3D%5Cnabla_%7B%5Ctheta%7D%5Ceta%5Cleft%28%5Cpi_%7B%5Ctheta%7D%5Cright%29%7C_%7B%5Ctheta+%3D%5Ctheta_%7Bold%7D%7D+%5C%5D) (7.7)

用图来表示为：



![img](https://pic4.zhimg.com/80/v2-f8e72c18f3cb17bcd3c828c71c842d3f_hd.jpg)

图7.5 回报函数与替代回报函数示意图

在 ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta_%7Bold%7D) 附近，能改善L的策略也能改善原回报函数。问题是步长多大呢？

再次引入第二个重量级的不等式

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29%5Cgeqslant+L_%7B%5Cpi%7D%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29-CD_%7BKL%7D%5E%7B%5Cmax%7D%5Cleft%28%5Cpi+%2C%5Ctilde%7B%5Cpi%7D%5Cright%29+%5C%5C+where%5C+C%3D%5Cfrac%7B2%5Cvarepsilon%5Cgamma%7D%7B%5Cleft%281-%5Cgamma%5Cright%29%5E2%7D+%5C%5D)(7.8)

其中 ![[公式]](https://www.zhihu.com/equation?tex=D_%7BKL%7D%5Cleft%28%5Cpi+%2C%5Ctilde%7B%5Cpi%7D%5Cright%29) 是两个分布的KL散度。我们在这里看一看，该不等式给了我们什么启示。

首先，该不等式给了 ![[公式]](https://www.zhihu.com/equation?tex=%5Ceta%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29) 的下界，我们定义这个下界为 ![[公式]](https://www.zhihu.com/equation?tex=+M_i%5Cleft%28%5Cpi%5Cright%29%3DL_%7B%5Cpi_i%7D%5Cleft%28%5Cpi%5Cright%29-CD_%7BKL%7D%5E%7B%5Cmax%7D%5Cleft%28%5Cpi_i%2C%5Cpi%5Cright%29) ，

下面利用这个下界，我们证明策略的单调性：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ceta%5Cleft%28%5Cpi_%7Bi%2B1%7D%5Cright%29%5Cgeqslant+M_i%5Cleft%28%5Cpi_%7Bi%2B1%7D%5Cright%29+%5C%5D)

且 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ceta%5Cleft%28%5Cpi_i%5Cright%29%3DM_i%5Cleft%28%5Cpi_i%5Cright%29+%5C%5D)

则： ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ceta%5Cleft%28%5Cpi_%7Bi%2B1%7D%5Cright%29-%5Ceta%5Cleft%28%5Cpi_i%5Cright%29%5Cgeqslant+M_i%5Cleft%28%5Cpi_%7Bi%2B1%7D%5Cright%29-M%5Cleft%28%5Cpi_i%5Cright%29+%5C%5D)

如果新的策略 ![[公式]](https://www.zhihu.com/equation?tex=%5Cpi_%7Bi%2B1%7D) 能使得 ![[公式]](https://www.zhihu.com/equation?tex=M_i) 最大，那么有不等式 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+M_i%5Cleft%28%5Cpi_%7Bi%2B1%7D%5Cright%29-M%5Cleft%28%5Cpi_i%5Cright%29%5Cgeqslant+0+%5C%5D) ，则 ![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ceta%5Cleft%28%5Cpi_%7Bi%2B1%7D%5Cright%29-%5Ceta%5Cleft%28%5Cpi_i%5Cright%29%5Cgeqslant+0+%5C%5D) ，这个使得 ![[公式]](https://www.zhihu.com/equation?tex=M_i) 最大的新的策略就是我们一直在苦苦找的要更新的策略。那么这个策略如何得到呢？

该问题可形式化为：

![[公式]](https://www.zhihu.com/equation?tex=+maximize_%7B%5Ctheta%7D%5Cleft%5BL_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28%5Ctheta%5Cright%29-CD_%7BKL%7D%5E%7B%5Cmax%7D%5Cleft%28%5Ctheta_%7Bold%7D%2C%5Ctheta%5Cright%29%5Cright%5D+)



如果利用惩罚因子C则每次迭代步长很小，因此问题可转化为：

![[公式]](https://www.zhihu.com/equation?tex=+maximize_%7B%5Ctheta%7DE_%7Bs~%5Crho_%7B%5Ctheta_%7Bold%7D%7D%2Ca~%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%7D%5Cleft%5B%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29%7D%7B%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28a%7Cs%5Cright%29%7DA_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5C+subject%5C+to%5C%5C+D_%7BKL%7D%5E%7B%5Cmax%7D%5Cleft%28%5Ctheta_%7Bold%7D%2C%5Ctheta%5Cright%29%5Cle%5Cdelta+) (7.9)

需要注意的是，因为有无穷多的状态，因此约束条件 ![[公式]](https://www.zhihu.com/equation?tex=D_%7BKL%7D%5E%7B%5Cmax%7D%5Cleft%28%5Ctheta_%7Bold%7D%2C%5Ctheta%5Cright%29) 有无穷多个。问题不可解。

**TRPO第三个技巧**

在约束条件中，利用平均KL散度代替最大KL散度，即：

![[公式]](https://www.zhihu.com/equation?tex=+subject%5C+to~~~%5Cbar%7BD%7D_%7BKL%7D%5E%7B%5Crho_%7B%5Ctheta_%7Bold%7D%7D%7D%5Cleft%28%5Ctheta_%7Bold%7D%2C%5Ctheta%5Cright%29%5Cle%5Cdelta+)

TRPO第四个技巧：

![[公式]](https://www.zhihu.com/equation?tex=+s~%5Csim%5Crho_%7B%5Ctheta_%7Bold%7D%7D%5Crightarrow+s~%5Csim%5Cpi_%7B%5Ctheta_%7Bold%7D%7D)

最终TRPO问题化简为：

![[公式]](https://www.zhihu.com/equation?tex=+maximize_%7B%5Ctheta%7DE_%7Bs~%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%2Ca~%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%7D%5Cleft%5B%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29%7D%7B%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28a%7Cs%5Cright%29%7DA_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5C+subject%5C+to%5C+E_%7Bs~%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%7D%5Cleft%5BD_%7BKL%7D%5Cleft%28%5Cpi_%7B%5Ctheta_%7Bold%7D%7D%5Cleft%28%5Ccdot+%7Cs%5Cright%29%7C%7C%5Cpi_%7B%5Ctheta%7D%5Cleft%28%5Ccdot+%7Cs%5Cright%29%5Cright%29%5Cright%5D%5Cle%5Cdelta+) （7.10）

接下来就是利用采样得到数据，然后求样本均值，解决优化问题即可。至此，TRPO理论算法完成。关于如何求解，以及如何减小方差，我们之后的课程会再讲。

# 强化学习进阶 第八讲 确定性策略方法


  强化学习进阶 第八讲 确定性策略梯度方法

我们开始第八讲，确定性策略梯度的方法。要想完全理解本节课的内容，前面的内容一定要熟练掌握，另外本讲也假设读者对DQN网络很熟悉。若不熟的话可以看[深度强化学习系列 第一讲 DQN - 知乎专栏](https://zhuanlan.zhihu.com/p/26052182)。

![img](https://pic1.zhimg.com/80/v2-f5fc92abbfc92a5f8574ef8857594618_hd.jpg)



图8.1 策略搜索方法分类



我们还是从图8.1策略搜索方法的分类讲起。从图中我们可以看到，无模型的策略搜索方法可以分为随机策略搜索方法和确定性策略搜索方法。其中随机策略搜索方法又发展出了很多算法。可以说，差不多在2014年以前，学者们都在发展随机策略搜索的方法。因为，大家都认为确定性策略梯度是不存在的。直到2014年，强化学习算法大神Silver在论文《Deterministic Policy Gradient Algorithms》中提出了确定性策略理论，策略搜索方法中才出现确定性策略这个方法。2015年，DeepMind的大神们又将该理论跟DQN的成功经验结合起来，在论文《Continuous control with
deep reinforcement learning》中提出DDPG的算法。本讲以这两篇论文为素材，给大家讲讲确定性策略

首先，我们讲讲什么是随机策略，什么是确定性策略。

**随机策略**的公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29%3DP%5Cleft%5Ba%7Cs%3B%5Ctheta%5Cright%5D+%5C%5D) （8.1）



其含义是，在状态s时，动作符合参数为![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+)的概率分布。比如常用的高斯策略：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29%3D%5Cfrac%7B1%7D%7B%5Csqrt%7B2%5Cpi%7D%5Csigma%7D%5Cexp%5Cleft%28-%5Cfrac%7B%5Cleft%28a-f_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%5Cright%29%7D%7B2%5Csigma%5E2%7D%5Cright%29+%5C%5D)，当利用该策略进行采样时，在状态s处，采取的动作服从均值为![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+f_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29+%5C%5D) ，方差为![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csigma%5E2+%5C%5D) 的正态分布。因此，我们可以总结说，采用随机策略时，即使在相同的状态，每次所采取的动作也很可能不一样。当然了，当采用高斯策略的时候，相同的策略，在同一个状态s处，采样的动作总体上看就算是不同，也差别不是很大，因为它们符合高斯分布，在均值附近的概率很大。

确定性策略的公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+a%3D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29+%5C%5D) （8.2）

跟随机策略不同，相同的策略（即![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+)相同时），在状态s时，动作是唯一确定的。

下面我们比较一下随机策略和确定性策略的优缺点。

**确定性策略的优点：需要采样的数据少，算法效率高**

首先，我们看一下随机策略，其梯度的计算公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DJ%5Cleft%28%5Cpi_%7B%5Ctheta%7D%5Cright%29%3DE_%7Bs%5Csim%5Crho%5E%7B%5Cpi%7D%2Ca%5Csim%5Cpi_%7B%5Ctheta%7D%7D%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5D) (8.3)

（8.3）表明，策略梯度公式是关于状态和动作的期望，在求期望时，需要对状态分布和动作分布进行求积分。这就要求在状态空间和动作空间采集大量的样本，这样求均值才能近似期望。

然而，确定性策略的动作是确定的，所以如果确定性策略梯度存在的化，策略梯度的求解不需要在动作空间进行采样积分。因此，相比于随机策略方法，确定性策略需要的样本数据要小。尤其是对那些动作空间很大的智能体，比如多关节机器人等，其动作空间维数很大。如果用随机策略，需要在这些动作空间中进行大量的采样。

通常来说，确定性策略方法的效率比随机策略的效率高十倍，这也是确定性策略方法最主要的优点。

相比于确定性策略，随机策略也有它自身的优点：**随机策略将探索和改进集成到一个策略中**

强化学习领域中的各路大神在过去十几年中乐忠于发展随机策略搜索方法也是有原因的。其中最重要的原因是随机策略本身自带探索，通过探索产生各种各样的数据，有好的数据，也有坏的数据，强化学习算法通过在这些好的数据中学到知识从而改进当前的策略。如图8.2所示，正是因为随机策略才产生了三条轨迹，这三条轨迹中有好的轨迹，通过向这些好的轨迹学习，策略可以得到快速改善。另外随机策略梯度理论也相对比较成熟，计算过程简单。

![img](https://pic1.zhimg.com/80/v2-c79f72345d5634f086e87b1e774d53a8_hd.jpg)

图8.2 随机策略学习过程



我们再回到确定性策略算法中。如公式（8.2）所示，给定状态s和策略参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)时，动作是固定的。也就是说，当初试状态已知时，用确定性策略所产生的轨迹永远都是固定的，智能体无法探索其他的轨迹或访问其他的状态，从这个层面来说，智能体无法学习。我们知道，强化学习算法是通过智能体与环境交互来学习的。这里的交互是指探索性交互，即智能体会尝试很多动作，然后在这些动作中学到好的动作。

**确定性策略无法探索环境，那么如何学习呢？**

答案就是利用异策略学习方法，即off-policy.
异策略是指行动策略和评估策略不是一个策略。这里我们的行动策略是随机策略，以保证充足的探索。评估策略是确定性策略，即公式（8.2）。整个确定性策略的学习框架采用AC的方法。

这里再介绍下AC的方法，AC算法英文名为：Actor-Critic Algorithm. AC算法包括两个同等地位的元素，一个元素是Actor即行动策略，另一个元素是Critic, 即评估，这里是指利用函数逼近方法估计值函数。

**我们先看看随机策略AC的方法是怎样的。**

随机策略的梯度为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DJ%5Cleft%28%5Cpi_%7B%5Ctheta%7D%5Cright%29%3DE_%7Bs%5Csim%5Crho%5E%7B%5Cpi%7D%2Ca%5Csim%5Cpi_%7B%5Ctheta%7D%7D%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5D)

如图8.3所示，其中Actor方法用来调整![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)值； Critic方法逼近值函数：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Ew%5Cleft%28s%2Ca%5Cright%29%5Capprox+Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D)，其中![[公式]](https://www.zhihu.com/equation?tex=w)为待逼近的参数，可用TD学习的方法对其进行评估。

![img](https://pic4.zhimg.com/80/v2-c542e16a6db355235da7ad1d3e892d1b_hd.jpg)

图8.3 AC方法网络结构



异策略随机策略梯度：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DJ_%7B%5Cbeta%7D%5Cleft%28%5Cpi_%7B%5Ctheta%7D%5Cright%29%3DE_%7Bs%5Csim%5Crho%5E%7B%5Cbeta%7D%2Ca%5Csim%5Cbeta%7D%5Cleft%5B%5Cfrac%7B%5Cpi_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29%7D%7B%5Cbeta_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29%7D%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi_%7B%5Ctheta%7D%5Cleft%28a%7Cs%5Cright%29Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%5D+%5C%5D)

采样策略为![[公式]](https://www.zhihu.com/equation?tex=%5Cbeta+).

为了给出确定性策略AC的方法，我们首先给出确定性策略梯度：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DJ%5Cleft%28%5Cmu_%7B%5Ctheta%7D%5Cright%29%3DE_%7Bs%5Csim%5Crho%5E%7B%5Cmu%7D%7D%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%5Cnabla_aQ%5E%7B%5Cmu%7D%5Cleft%28s%2Ca%5Cright%29%7C_%7Ba%3D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%7D%5Cright%5D+%5C%5D) (8.5)

(8.5)即为确定性策略梯度。跟随机策略梯度（8.3）相比，少了对动作的积分，多了回报函数对动作的导数。

**异策略确定性策略梯度为**：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DJ_%7B%5Cbeta%7D%5Cleft%28%5Cmu_%7B%5Ctheta%7D%5Cright%29%3DE_%7Bs%5Csim%5Crho%5E%7B%5Cbeta%7D%7D%5Cleft%5B%5Cnabla_%7B%5Ctheta%7D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%5Cnabla_aQ%5E%7B%5Cmu%7D%5Cleft%28s%2Ca%5Cright%29%7C_%7Ba%3D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%7D%5Cright%5D+%5C%5D) (8.6)

比较（8.6）和（8.4）我们发现，确定性策略梯度求解时少了重要性权重，这是因为重要性采样是用简单的概率分布区估计复杂的概率分布，而确定性策略的动作是确定值不是概率分布，另外确定性策略的值函数评估用的是Qlearning的方法，即用TD(0)来估计动作值函数并忽略重要性权重。

有了（8.6）,我们便可以得到确定性策略异策略AC算法的更新过程了：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cdelta_t%3Dr_t%2B%5Cgamma+Q%5Ew%5Cleft%28s_%7Bt%2B1%7D%2C%5Cmu_%7B%5Ctheta%7D%5Cleft%28s_%7Bt%2B1%7D%5Cright%29%5Cright%29-Q%5Ew%5Cleft%28s_t%2Ca_t%5Cright%29%5C%5C%5C%5C%5C+w_%7Bt%2B1%7D%3Dw_t%2B%5Calpha_w%5Cdelta_t%5Cnabla_wQ%5Ew%5Cleft%28s_t%2Ca_t%5Cright%29+%5C%5C+%5Ctheta_%7Bt%2B1%7D%3D%5Ctheta_t%2B%5Calpha_%7B%5Ctheta%7D%5Cnabla_%7B%5Ctheta%7D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s_t%5Cright%29%5Cnabla_aQ%5Ew%5Cleft%28s_t%2Ca_t%5Cright%29%7C_%7Ba%3D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%7D+%5C%5D) (8.7)

（8.7）的第一行和第二行是利用值函数逼近的方法更新值函数参数，第三行是利用确定性策略梯度的方法更新策略参数。

以上介绍的是确定性策略梯度方法，可以称为DPG的方法。有了DPG，我们再讲DDPG。

DDPG是深度确定性策略，所谓深度是指利用深度神经网络逼近行为值函数![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Ew%5Cleft%28s%2Ca%5Cright%29+%5C%5D)和确定性策略![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29+%5C%5D)。

就像在DQN中讲的那样，当利用深度神经网络进行函数逼近的时候，强化学习算法常常不稳定。这是因为，对深度神经网络进行训练的时候往往假设输入的数据是独立同分布的，但强化学习的数据是顺序采集的，数据之间存在马尔科夫性，很显然这些数据并非独立同分布的。

为了打破数据之间的相关性，DQN用了两个技巧：经验回放和独立的目标网络。DDPG的算法便是将这两条技巧用到了DPG算法中。

DDPG的经验回放跟DQN完全相同，这里就不重复介绍了，忘记的同学可以去看之前的课程（[深度强化学习系列 第一讲 DQN - 知乎专栏](https://zhuanlan.zhihu.com/p/26052182)）。

这里我们重点介绍独立的目标网络。

DPG的更新过程如（8.7）所示，这里的目标值是（8.7）式中的第一行的前两项，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+r_t%2B%5Cgamma+Q%5Ew%5Cleft%28s_%7Bt%2B1%7D%2C%5Cmu_%7B%5Ctheta%7D%5Cleft%28s_%7Bt%2B1%7D%5Cright%29%5Cright%29+%5C%5D) (8.8)

我们需要修改的就是（8.8）中的![[公式]](https://www.zhihu.com/equation?tex=w)和![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+)，我们将这里的![[公式]](https://www.zhihu.com/equation?tex=w)和![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+)单独拿出来，利用独立的网络对其进行更新。

DDPG的更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cdelta_t%3Dr_t%2B%5Cgamma+Q%5E%7Bw%5E-%7D%5Cleft%28s_%7Bt%2B1%7D%2C%5Cmu_%7B%5Ctheta%5E-%7D%5Cleft%28s_%7Bt%2B1%7D%5Cright%29%5Cright%29-Q%5Ew%5Cleft%28s_t%2Ca_t%5Cright%29%5C%5C%5C%5C%5C+%5C%5C+w_%7Bt%2B1%7D%3Dw_t%2B%5Calpha_w%5Cdelta_t%5Cnabla_wQ%5Ew%5Cleft%28s_t%2Ca_t%5Cright%29+%5C%5C+%5Ctheta_%7Bt%2B1%7D%3D%5Ctheta_t%2B%5Calpha_%7B%5Ctheta%7D%5Cnabla_%7B%5Ctheta%7D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s_t%5Cright%29%5Cnabla_aQ%5Ew%5Cleft%28s_t%2Ca_t%5Cright%29%7C_%7Ba%3D%5Cmu_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%7D+%5C%5C+%5Ctheta%5E-%3D%5Ctau%5Ctheta+%2B%5Cleft%281-%5Ctau%5Cright%29%5Ctheta%5E-+%5C%5C+w%5E-%3D%5Ctau+w%2B%5Cleft%281-%5Ctau%5Cright%29w%5E-+%5C%5D)

最后，我们给出DDPG的伪代码：

![img](https://pic2.zhimg.com/80/v2-8fe2d057ef4f4cdfeca525735afe0535_hd.jpg)



图8.4 DDPG伪代码

# 强化学习进阶 第九讲 引导策略搜索


  强化学习进阶 第九讲 引导策略搜索（GPS）

本节是本系列的第九讲，也是本系列的倒数第二讲。下个系列是强化学习实战，还请继续关注和宣传本专栏。再次公布一遍qq交流群号：202570720,群里有更多资料分享。好了，言归正传。

引导策略搜索方法（Guided
Policy Search）最早是2013年Sergey
Levine在斯坦福读博士的时候提出来的。强化学习算法大神Levine，博士毕业后便去了伯克利跟着Pieter Abbeel做博后，出站后留在了伯克利任教。如果查他的教育经历，会发现。Levine博士时候的老板不是搞机器人的，而是搞计算机图形学的。Levine读博士时，要解决的是计算机仿真中人物的逼真运动。通过模仿学习或机器学习来完成。

![img](https://pic3.zhimg.com/80/v2-16190a65ae15322a0249834f1e600206_hd.jpg)

图9.1 策略搜索方法分类

如图9.1 为策略搜索方法的分类，从引导策略搜索提出到现在，引导策略搜索方法其实已经扩展到了有模型和无模型的情况。跟其他直接的策略搜索方法不同，GPS不会对策略进行直接搜索，而是将策略搜索方法分为两步：控制项和监督相。

第一个问题，引导策略搜索方法为什么这么做呢？

听我一一道来：

首先，我们先看看第七讲的TRPO方法和DDPG方法的局限性。从图9.1策略搜索方法的分类我们可以看到，这两种方法都是无模型的强化学习算法。无模型的强化学习算法有很多优点，可以不用对外界建模，尤其是当外界环境非常复杂，很难建模或根本无法建模时，该方法是唯一的方法。但是，无模型的强化学习算法也有其固有的缺点：因为没有模型，所以无模型的强化学习算法只能通过不断尝试来探索环境，这些算法只能处理参数最多数百个的策略网络，对于更大的策略网络，这些方法学习效率不高。仔细想想，原因很简单。当随机初始化数千个或上万个参数的网络时，随机尝试根本就产生不了好的数据。对于复杂的任务，随机探索几乎找不到成功的解或者好的解。没有成功的解或好的解，智能体就没办法从中学到好的动作，无法形成良性循环。也就是说，无模型的强化学习算法最大的缺点是数据效率低。

解决无模型随机探索问题的方法是利用模型进行探索。如何得到模型？或者从数据中学一个模型，或者人为地建立智能体探索环境的近似模型（建机器人模型是机器人学和机器人学家们一直乐于干的事情）。

有了模型，我们可以干哪些事呢?

第一， 利用模型和基于模型的优化算法，我们可以得到回报高的数据，也就是好的数据。有了好的数据，我们就可以对策略网络进行稳定的训练了。

第二， 有了模型，我们可以充分地利用示例（demonstration）学习。人的示例可以当成模型的初值。

所以Guided Policy Search方法将策略搜索方法分成两个相，控制相和监督相。其中控制相通过轨迹最优、传统控制器或随机最优控制产生好的数据。监督相，是利用从控制相产生的好数据进行监督学习。



我们从直观上对比下我们通常的强化学习方法和GPS的方法。如图9.2所示，之前我们讲过，强化学习是智能体通过与环境交互产生数据，从交互数据中学习如图9.2图A所示；而GPS是策略网络通过与控制相交互产生的数据，从控制相产生的数据中进行学习。换句话说，在无模型中，智能体（策略网络）在通过试错跟环境学习；而GPS的方法是跟逐渐迭代优化的控制器学习。

![img](https://pic4.zhimg.com/80/v2-3e8d4979e1ae2d5cde790d2df3b6dba3_hd.jpg)

图9.2 强化学习原理图和GPS原理图

我们已经知道了GPS的基本原理，那么该如何用数学公式进而用代码来实现呢？

在进入这个问题之前，我们先看看自从GPS算法提出来以后，这些年如何发展的。可以从三个方面来看GPS的发展。

**第一个方面**：从问题的构建来看gps经历了基于重要性采样的gps(ISGPS)à基于变分推理的gps(vgps)à基于约束的gps(cgps)。这些发展是Levine在做博士时的发展思路。

**第二个方面**：从优化方法看gps的发展。基于约束的gps被构建以后，关于它的优化最开始是Dual gps即基于对偶梯度下降法（2014年）àBADMM方法即布雷格曼交叉方向乘子法（2015年）àmirror descent gps即镜像下降优化算法。

**第三个方面**：从控制相来看，gps从基于轨迹最优（微分动态规划DDP、线性二次规划LQR和线性二次高斯LQG）发展到了随机最优控制PI2 gps.

感兴趣的同学可以将这些论文下载下来好好研究。

下面，我们具体讲解基于约束的引导策略搜索方法。参考文献是2014年Levine在ICML上的论文《Learning complex neural network policies with trajectory
optimization》.讲解此论文是因为该论文用到了比较全面的数学技巧，将这些数学技巧交给大家，大家学会后便可以看懂gps系列的其他论文了。

基于约束的引导策略搜索方法可形式化为：

![[公式]](https://www.zhihu.com/equation?tex=+min_%7B%5Ctheta+%2Cq%7D%5C+D_%7BKL%7D%5Cleft%28q%5Cleft%28%5Ctau%5Cright%29%7C%7C%5Crho%5Cleft%28%5Ctau%5Cright%29%5Cright%29+%5C%5C+s.t.%5C+q%5Cleft%28x_1%5Cright%29%3Dp%5Cleft%28x_1%5Cright%29%2C+%5C%5C+q%5Cleft%28x_%7Bt%2B1%7D%7Cx_t%2Cu_t%5Cright%29%3Dp%5Cleft%28x_%7Bt%2B1%7D%7Cx_t%2Cu_t%5Cright%29+%5C%5C%5C%5C+D_%7BKL%7D%5Cleft%28q%5Cleft%28x_t%5Cright%29%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_t%7Cx_t%5Cright%29%7C%7Cq%5Cleft%28x_t%2Cu_t%5Cright%29%5Cright%29%3D0+) (9.1)

其中目标函数为![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin_%7B%5Ctheta+%2Cq%5Cleft%28%5Ctau%5Cright%29%7D%5C+D_%7BKL%7D%5Cleft%28q%5Cleft%28%5Ctau%5Cright%29%7C%7C%5Crho%5Cleft%28%5Ctau%5Cright%29%5Cright%29+%5C%5D),![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Crho%5Cleft%28%5Ctau%5Cright%29%5Cpropto%5Cexp%5Cleft%28l%5Cleft%28%5Ctau%5Cright%29%5Cright%29+%5C%5D).

（9.1）式给人的第一印象是好复杂。强化学习的目标为什么变成了一个KL散度？

首先，我们先定义KL散度：设![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q%5Cleft%28%5Ctau%5Cright%29%2C%5Crho%5Cleft%28%5Ctau%5Cright%29+%5C%5D)是两个分布，则两个分布之间的KL散度为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+D_%7BKL%7D%5Cleft%28q%5Cleft%28%5Ctau%5Cright%29%7C%7C%5Crho%5Cleft%28%5Ctau%5Cright%29%5Cright%29%3D%5Cint%7Bq%5Cleft%28%5Ctau%5Cright%29%5Clog%5Cfrac%7Bq%5Cleft%28%5Ctau%5Cright%29%7D%7B%5Crho%5Cleft%28%5Ctau%5Cright%29%7D%7Dd%5Ctau+%5C%5D)

KL散度是衡量两个概率分布之间距离的度量。比如，当两个分布相等时，距离为0.

强化学习的目标，为什么可以表示成如（9.1）式的KL散度呢？

我们利用KL散度的公式将目标函数展开：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+D_%7BKL%7D%5Cleft%28q%5Cleft%28%5Ctau%5Cright%29%7C%7C%5Crho%5Cleft%28%5Ctau%5Cright%29%5Cright%29%3D%5Cint%7Bq%5Cleft%28%5Ctau%5Cright%29%5Clog%5Cfrac%7Bq%5Cleft%28%5Ctau%5Cright%29%7D%7B%5Crho%5Cleft%28%5Ctau%5Cright%29%7D%7Dd%5Ctau+%5C%5C+%3D%5Cint%7Bq%5Cleft%28%5Ctau%5Cright%29%5Clog+q%5Cleft%28%5Ctau%5Cright%29d%5Ctau+-%5Cint%7Bq%5Cleft%28%5Ctau%5Cright%29%7D%7D%5Clog%5Crho%5Cleft%28%5Ctau%5Cright%29d%5Ctau+%5C%5D) （9.2）



将![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Crho%5Cleft%28%5Ctau%5Cright%29%5Cpropto%5Cexp%5Cleft%28l%5Cleft%28%5Ctau%5Cright%29%5Cright%29+%5C%5D)带入（9.2）并忽略常数，可以得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+D_%7BKL%7D%5Cleft%28q%5Cleft%28%5Ctau%5Cright%29%7C%7C%5Crho%5Cleft%28%5Ctau%5Cright%29%5Cright%29+%5C%5C+%3D-H%5Cleft%28q%5Cright%29-E_q%5Cleft%28l%5Cleft%28%5Ctau%5Cright%29%5Cright%29%2Bconst+%5C%5D)

第一项为分布q的熵，第二项为累积回报的期望。（9.1）最小化目标函数相当于最大化累积回报和最大化熵。这里熵是不确定性的度量，不确定越大，熵越大。这里最大化熵是为了保证最优控制分布是一个分布，而非是一个确定值。第二项是最大化累积回报，跟常用的强化学习目标保持一致。

将约束利用拉格朗日乘子带入目标函数，cGPS整个问题的拉格朗日函数为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L%5Cleft%28%5Ctheta+%2Cq%2C%5Clambda%5Cright%29%3DD_%7BKL%7D%5Cleft%28q%5Cleft%28%5Ctau%5Cright%29%7C%7C%5Crho%5Cright%29%2B%5Csum_%7Bt%3D1%7D%5ET%7B%5Clambda_tD_%7BKL%7D%5Cleft%28q%5Cleft%28x_t%5Cright%29%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_t%7Cx_t%5Cright%29%7C%7Cq%5Cleft%28x_t%2Cu_t%5Cright%29%5Cright%29%7D+%5C%5D) （9.3）



其中**轨迹优化相**的优化问题为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L%5Cleft%28q%5Cright%29%3D-%5Csum_%7Bt%3D1%7D%5ET%7BE_%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29%7D%5Cleft%5Bl%5Cleft%28x_t%2Cu_t%5Cright%29%5Cright%5D-H%5Cleft%28q%5Cright%29%7D%2B%5Clambda_tE_%7Bq%5Cleft%28x_t%5Cright%29%7D%5Cleft%5BD_%7BKL%7D%5Cleft%28%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_t%7Cx_t%5Cright%29%7C%7Cq%5Cleft%28u_t%7Cx_t%5Cright%29%5Cright%29%5Cright%5D+%5C%5D) （9.4）



**监督相的优化问题为**：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L%5Cleft%28%5Ctheta%5Cright%29%3D%5Csum_%7Bt%3D1%7D%5ET%7B%5Clambda_t%5Csum_%7Bi%3D1%7D%5EN%7BD_%7BKL%7D%5Cleft%28%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_t%7Cx_%7Bti%7D%5Cright%29%7C%7Cq%5Cleft%28u_t%7Cx_%7Bti%7D%5Cright%29%5Cright%29%7D%7D+%5C%5D) （9.5）



对于轨迹最优相，也就是求解（9.4）时我们可以利用轨迹最优算法（DDP算法如第二讲[强化学习入门 第二讲 基于模型的动态规划方法 - 知乎专栏](https://zhuanlan.zhihu.com/p/25580624?refer=sharerl)，LQR算法如杨超同学的分享[引导策略搜索方法（GPS）中的轨迹优化_LQR - 知乎专栏](https://zhuanlan.zhihu.com/p/26531882)），要想利用轨迹最优算法，必须求出每一步的代价。如何从（9.4）中求出每步的代价呢？下面我们给出推导：

每个轨迹分布![[公式]](https://www.zhihu.com/equation?tex=q%28%5Ctau+%29)有均值![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Ctau%7D%3D%5Cleft%28%5Chat%7Bx%7D_%7B1..T%7D%2C%5Chat%7Bu%7D_%7B1..T%7D%5Cright%29+%5C%5D)，条件动作分布![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q%5Cleft%28u_t%7Cx_t%5Cright%29%3DN%5Cleft%28u_t%2BKx_t%2CA_t%5Cright%29+%5C%5D)为了不失一般性，每个![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7Bx%7D_t%2C%5Chat%7Bu%7D_t+%5C%5D)初值为0，策略网络![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28u_t%7Cx_t%5Cright%29%3DN%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7Dx_t%2C%5CSigma_%7Bt%7D%5E%7B%5Cpi%7D%5Cright%29+%5C%5D), ![[公式]](https://www.zhihu.com/equation?tex=S_t)是相对于分布![[公式]](https://www.zhihu.com/equation?tex=q%28x_t%29)的协方差。

（9.4）中的第一项进行二阶泰勒展开：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csum_%7Bt%3D1%7D%5ET%7BE_%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29%7D%5Cleft%5Bl%5Cleft%28x_t%2Cu_t%5Cright%29%5Cright%5D%7D%3D%5Csum_%7Bt%3D1%7D%5ET%7BE_%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29%7D%5Cleft%5Bl%5Cleft%280%2C0%5Cright%29%2B%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t%5C%5C+u_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxut%7D%2B%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t%5C%5C+u_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxu%2Cxut%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t%5C%5C+u_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5Cright%5D%7D+%5C%5C+%3D%5Csum_%7Bt%3D1%7D%5ET%7B%5Cint%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29l%5Cleft%280%2C0%5Cright%29d%5Cleft%28x_t%2Cu_t%5Cright%29%5ET%2B%5Cint%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t%5C%5C+u_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxut%7Dd%5Cleft%28x_t%2Cu_t%5Cright%29%5ET%7D%2B%5Cfrac%7B1%7D%7B2%7D%5Cint%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t%5C%5C+u_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxu%2Cxut%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t%5C%5C+u_t%5C%5C+%5Cend%7Barray%7D%5Cright%29d%5Cleft%28x_t%2Cu_t%5Cright%29%5ET%7D%7D%7D+%5C%5D)



忽略常数，（9.4）的第一项可写为：

![[公式]](https://www.zhihu.com/equation?tex=+%5Csum_%7Bt%3D1%7D%5ET%7BE_%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29%7D%5Cleft%5Bl%5Cleft%28x_t%2Cu_t%5Cright%29%5Cright%5D%7D%3D+%5C%5C+%3D%5Csum_%7Bt%3D1%7D%5ET%7B%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxut%7D%7D%2B%5Cfrac%7B1%7D%7B2%7D%5Cint%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29%5Cleft%28%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t-%5Chat%7Bx%7D_t%5C%5C+u_t-%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ET%2B%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ET%5Cright%29l_%7Bxu%2Cxut%7D%5Cleft%28%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t-%5Chat%7Bx%7D_t%5C%5C+u_t-%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%2B%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5Cright%29d%5Cleft%28x_t%2Cu_t%5Cright%29%5ET%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%3D%5Csum_%7Bt%3D1%7D%5ET%7B%5Cleft%28%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxut%7D%2B%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxu%2Cxut%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%2B%5Cfrac%7B1%7D%7B2%7D%5Cint%7Bq%5Cleft%28x_t%2Cu_t%5Cright%29%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t-%5Chat%7Bx%7D_t%5C%5C+u_t-%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxu%2Cxut%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+x_t-%5Chat%7Bx%7D_t%5C%5C+u_t-%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%7D%5Cright%29%7D+%5C%5C+%3D%5Csum_%7Bt%3D1%7D%5ET%7B%5Cleft%28%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxut%7D%2B%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxu%2Cxut%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5Cright%29%7D%2B%5Cfrac%7B1%7D%7B2%7DT_r%5Cleft%28%5CvarSigma_tl_%7Bxu%2Cxut%7D%5Cright%29+%5C%5D)(9.5)

（9.4）的第二项为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+H%5Cleft%28q%5Cright%29%3D-%5Cint%7Bq%5Clog+q%7D+%5C%5C+%3D-%5Cint%7Bq%5Clog%5Cfrac%7B1%7D%7B%5Cleft%282%5Cpi%5Cright%29%5E%7BN%2F2%7D%5Cleft%7C+A%5Cright%7C%5E%7B1%2F2%7D%7D%7D%5Cexp%5Cleft%28-%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cleft%5Bu-%5Chat%7Bu%7D%5Cright%5D%5Cright%29%5ETA%5E%7B-1%7D%5Cleft%28%5Cleft%5Bu-%5Chat%7Bu%7D%5Cright%5D%5Cright%29%5Cright%29+%5C%5C+%3D%5Cfrac%7B1%7D%7B2%7D%5Clog%5Cleft%7C+A%5Cright%7C%2Bconst+%5C%5D) (9.6)

在给出第三项之前，先给出两个高斯分布的KL散度公式：

首先给出高斯分布的基本公式：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cint%7Bp%5Cleft%28x%5Cright%29dx%7D%3D1+%5C%5C+%5Cint%7Bxp%5Cleft%28x%5Cright%29dx%3D%5Cmu%7D+%5C%5C+%5Cint%7Bp%5Cleft%28x%5Cright%29%5Cleft%28x-%5Cmu%5Cright%29%5E2dx%3D%5Csigma%5E2%7D+%5C%5D)

设![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+f%5Cleft%28x%5Cright%29%5Csim+N%5Cleft%28%5Cmu+%2C%5CvarSigma%5Cright%29%2C%5C+g%5Cleft%28x%5Cright%29+%5Csim+N%5Cleft%28%5Cnu+%2C%5CvarGamma%5Cright%29+%5C%5D),利用高斯分布的基本公式可以求得：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+D_%7BKL%7D%5Cleft%28f%7C%7Cg%5Cright%29%3D%5Cint%7Bf%5Cleft%28x%5Cright%29%5Clog%5Cfrac%7Bf%5Cleft%28x%5Cright%29%7D%7Bg%5Cleft%28x%5Cright%29%7Ddx%7D+%5C%5C+%3D%5Cint%7Bf%5Cleft%28x%5Cright%29%5Clog+f%5Cleft%28x%5Cright%29dx-%5Cint%7Bf%5Cleft%28x%5Cright%29%5Clog+g%5Cleft%28x%5Cright%29%7D%7Ddx+%5C%5C+%3D%5Cint%7Bf%5Cleft%28x%5Cright%29%5Clog%5Cleft%28%5Cfrac%7B1%7D%7B%5Cleft%282%5Cpi%5Cright%29%5E%7BN%2F2%7D%5Cleft%7C%5CSigma%5Cright%7C%5E%7B1%2F2%7D%7D%5Cexp%5Cleft%28-%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cleft%5Bx-%5Cmu%5Cright%5D%5Cright%29%5ET%5CSigma%5E%7B-1%7D%5Cleft%28%5Cleft%5Bx-%5Cmu%5Cright%5D%5Cright%29%5Cright%29%5Cright%29%7Ddx+%5C%5C+-%5Cint%7Bf%5Cleft%28x%5Cright%29%5Clog%5Cleft%28%5Cfrac%7B1%7D%7B%5Cleft%282%5Cpi%5Cright%29%5E%7BN%2F2%7D%5Cleft%7C%5CvarGamma%5Cright%7C%5E%7B1%2F2%7D%7D%5Cexp%5Cleft%28-%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cleft%5Bx-%5Cmu%2B%5Cmu-%5Cnu%5Cright%5D%5Cright%29%5ET%5CvarGamma%5E%7B-1%7D%5Cleft%28%5Cleft%5Bx-%5Cmu%2B%5Cmu-%5Cnu%5Cright%5D%5Cright%29%5Cright%29%5Cright%29%7Ddx+%5C%5C+%3D%5Cfrac%7B1%7D%7B2%7D%5Cleft%5C%7B%5Clog%5Cfrac%7B%5Cleft%7C%5CvarGamma%5Cright%7C%7D%7B%5Cleft%7C%5CvarSigma%5Cright%7C%7D%2BT_r%5Cleft%28%5CSigma%5Cleft%28%5CvarGamma%5E%7B-1%7D-%5CSigma%5E%7B-1%7D%5Cright%29%5Cright%29%2B%5Cleft%28%5Cmu+-%5Cnu%5Cright%29%5ET%5CvarGamma%5E%7B-1%7D%5Cleft%28%5Cmu+-%5Cnu%5Cright%29%5Cright%5C%7D+%5C%5D) (9.7)

其中第三个等号一定要配方，这样才能利用高斯分布的方差定义进行计算。有了任意两个高斯分布的KL散度，再计算（9.4）的第三项，去掉常数项得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+E_%7Bq%5Cleft%28x_t%5Cright%29%7D%5Cleft%5BD_%7BKL%7D%5Cleft%28%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_t%7Cx_t%5Cright%29%7C%7Cq%5Cleft%28u_t%7Cx_t%5Cright%29%5Cright%29%5Cright%5D+%5C%5C+%3DE_q%5Cleft%5B%5Cfrac%7B1%7D%7B2%7D%5Clog%5Cleft%7C+A_t%5Cright%7C%2B%5Cfrac%7B1%7D%7B2%7Dtr%5Cleft%28A_%7Bt%7D%5E%7B-1%7D%5CSigma_%7Bt%7D%5E%7B%5Cpi%7D%5Cright%29%2B%5Cleft%28%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7Dx_t%5Cright%29-%5Cleft%28u_t%2BKx_t%5Cright%29%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7Dx_t%5Cright%29-%5Cleft%28u_t%2BKx_t%5Cright%29%5Cright%29%5Cright%5D+%5C%5D) (9.8)

其中（9.8）的第三项可以写为：（说明，为了让式子显得短，下式中的X代表前面的一串，即![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+X%3D%5Cleft%28%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D%5Chat%7Bx%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7Dx-%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D%5Chat%7Bx%7D%5Cright%29-%5Cleft%28u_t%2BK%5Chat%7Bx%7D%2BKx_t-K%5Chat%7Bx%7D%5Cright%29%5Cright%29+%5C%5D)）：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cint%7Bq%5Cleft%28x%5Cright%29%7D%5Cleft%28%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7Dx_t%5Cright%29-%5Cleft%28u_t%2BKx_t%5Cright%29%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7Dx_t%5Cright%29-%5Cleft%28u_t%2BKx_t%5Cright%29%5Cright%29dx+%5C%5C+%3D%5Cint%7Bq%5Cleft%28x%5Cright%29%5Cleft%5B%5Cleft%28%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D%5Chat%7Bx%7D%2B%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7Dx-%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D%5Chat%7Bx%7D%5Cright%29-%5Cleft%28u_t%2BK%5Chat%7Bx%7D%2BKx_t-K%5Chat%7Bx%7D%5Cright%29%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7DX%5Cright%5D%7Ddx+%5C%5C+%3D%5Cint%7Bq%5Cleft%28x%5Cright%29%5Cleft%5B%5Cleft%28%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%2B%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5Cleft%28x-%5Chat%7Bx%7D%5Cright%29%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7DX%5Cright%5Ddx%7D+%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%3D%5Cint%7Bq%5Cleft%28x%5Cright%29%7D%5Cleft%5B%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%2B%5Cleft%28x-%5Chat%7Bx%7D%5Cright%29%5ET%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5Cleft%28x-%5Chat%7Bx%7D%5Cright%29%5Cright%5Ddx+%5C%5C+%3D%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%2BTr%5Cleft%28S%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5Cright%29+%5C%5D)



最后我们联合（9.5, 9.6，9.8）给出**轨迹优化相的目标函数**：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L%5Cleft%28q%5Cright%29%5Capprox%5Csum_%7Bt%3D1%7D%5ET%7B-%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxu%2Cxut%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29-%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxut%7D-%7D%5Cfrac%7B1%7D%7B2%7DT_r%5Cleft%28%5CvarSigma_tl_%7Bxu%2Cxut%7D%5Cright%29-%5Cfrac%7B1%7D%7B2%7D%5Clog%5Cleft%7C+A%5Cright%7C+%5C%5C+%2B%5Clambda_t%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%5Clog%5Cleft%7C+A_t%5Cright%7C%2B%5Cfrac%7B1%7D%7B2%7Dtr%5Cleft%28A_%7Bt%7D%5E%7B-1%7D%5CSigma_%7Bt%7D%5E%7B%5Cpi%7D%5Cright%29%2B%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%2BTr%5Cleft%28S%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5Cright%29%5Cright%29+%5C%5D)（9.9）



这时每步的代价函数为:

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+l%3D-%5Cfrac%7B1%7D%7B2%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxu%2Cxut%7D%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29-%5Cleft%28%5Cbegin%7Barray%7D%7Bc%7D+%5Chat%7Bx%7D_t%5C%5C+%5Chat%7Bu%7D_t%5C%5C+%5Cend%7Barray%7D%5Cright%29%5ETl_%7Bxut%7D-%5Cfrac%7B1%7D%7B2%7DT_r%5Cleft%28%5CvarSigma_tl_%7Bxu%2Cxut%7D%5Cright%29-%5Cfrac%7B1%7D%7B2%7D%5Clog%5Cleft%7C+A%5Cright%7C+%5C%5C+%2B%5Clambda_t%5Cleft%28%5Cfrac%7B1%7D%7B2%7D%5Clog%5Cleft%7C+A_t%5Cright%7C%2B%5Cfrac%7B1%7D%7B2%7Dtr%5Cleft%28A_%7Bt%7D%5E%7B-1%7D%5CSigma_%7Bt%7D%5E%7B%5Cpi%7D%5Cright%29%2B%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cmu_%7Bt%7D%5E%7B%5Cpi%7D%5Cleft%28%5Chat%7Bx%7D%5Cright%29-%5Chat%7Bu%7D_t%5Cright%29%2BTr%5Cleft%28S%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28%5Cmu_%7Bxt%7D%5E%7B%5Cpi%7D-K%5Cright%29%5Cright%29%5Cright%29+%5C%5D)

然后利用轨迹最优方法，DDP算法如第二讲[强化学习入门 第二讲 基于模型的动态规划方法 - 知乎专栏](https://zhuanlan.zhihu.com/p/25580624?refer=sharerl)，LQR算法如杨超同学的分享[引导策略搜索方法（GPS）中的轨迹优化_LQR - 知乎专栏](https://zhuanlan.zhihu.com/p/26531882)便可以得到最优的q；



**对于监督相**，利用两个正态分布的KL散度公式（9.7）可以容易得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+L%5Cleft%28%5Ctheta%5Cright%29%3D%5Csum_%7Bt%3D1%7D%5ET%7B%5Clambda_t%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Cfrac%7B1%7D%7B2%7D%7D%5Cleft%5C%7BT_r%5Cleft%28%5CSigma_%7Bt%7D%5E%7B%5Cpi%7DA_%7Bt%7D%5E%7B-1%7D%5Cright%29-%5Clog%5Cleft%7C%5CSigma%5E%7B%5Cpi%7D%5Cright%7C%2B%5Cleft%28K_tx_t%2Bk_t-%5Cmu%5E%7B%5Cpi%7D%5Cleft%28x_t%5Cright%29%5Cright%29%5ETA_%7Bt%7D%5E%7B-1%7D%5Cleft%28K_tx_t%2Bk_t-%5Cmu%5E%7B%5Cpi%7D%5Cleft%28x_t%5Cright%29%5Cright%29%5Cright%5C%7D+%5C%5D)（9.10）

利用随机梯度下降或者LBFGS方法对神经网络进行训练。

**最后更新![[公式]](https://www.zhihu.com/equation?tex=%5Clambda+)值**。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Clambda_t%5Cgets%5Clambda_t%2B%5Ceta+D_%7BKL%7D%5Cleft%28q%5Cleft%28x_t%5Cright%29%5Cpi_%7B%5Ctheta%7D%5Cleft%28u_t%7Cx_t%5Cright%29%7C%7Cq%5Cleft%28x_t%2Cu_t%5Cright%29%5Cright%29+%5C%5D) （9.11）

最后梳理一下，给出伪代码：



![img](https://pic3.zhimg.com/80/v2-47bcfd1747d49e63023491016ac02852_hd.jpg)

本讲完。终于写完这一讲了，好难写，查了n多资料，好多公式想了好久才推出来，希望对各位有帮助。

# 强化学习进阶 第十讲 逆向强化学习（第一节 概述篇）

强化学习进阶
第十讲 逆向强化学习（第一节 概述篇）

说明：这一讲是本系列的最后一讲。对于这一讲，最近一直在看相关的资料。发现看的资料越多，知识点越多，要学的东西也越多。本来的想法是将逆向强化学习作为一讲给出，但现在基本否定了这个想法。当然了，如果简简单单地忽略那些细节，聊聊逆向强化学习的思想，一节足矣。但是，如果这样做了，相信读者会将这一讲当成了一篇科普文，读了还是不知道什么是逆向强化学习，根本学不到专业上的东西，这也违背了我开这个系列的初衷。所以，现在的打算是每次更新一节，尽量将细节挖掘出来。今天第一节为概述篇，先聊一聊逆向强化学习的概念和分类。

![img](https://pic2.zhimg.com/80/v2-2edd55d3fc4cd96edd098eb1a997d281_hd.jpg)

图10.1 逆向强化学习的分类

**第一个概念，什么是逆向强化学习？**

前面我们已经讲过强化学习。强化学习是求累积回报期望最大时的最优策略，在求解过程中立即回报是人为给定的。然而，在很多任务中，尤其是复杂的任务中，立即回报很难指定。那么该如何得到这些回报函数呢？

我们人类在完成复杂的任务时，根本就没考虑回报函数。但是，这并不是说人在完成任务时就没有回报函数。可以这么说，其实人在完成具体任务时有隐形的回报函数。所以，一种指定回报函数的方法是从人的示例中学到隐形的回报函数。

可是，这种回报函数怎么学呢？

这就需要进行建模了。逆向强化学习的提出者Ng是这么想的：专家在完成某项任务时，其决策往往是最优的或接近最优的，那么可以这样假设，当所有的策略所产生的累积回报期望都不比专家策略所产生的累积回报期望大时，强化学习所对应的回报函数就是根据示例学到的回报函数。

因此，逆向强化学习可以定义为从专家示例中学到回报函数。

**第二个问题是逆向强化学习的分类**

如果将最开始逆向强化学习的思想用数学的形式表示出来，那么这个问题可以归结为最大边际化问题，（我们在下一节会具体介绍）。如图10.1所示，这是强化学习最早的思想。根据这个思想发展起来的算法包括：学徒学习（Apprenticeship
learning）,MMP方法（Maximum Margin
Planning）,结构化分类（SCIRL）和神经逆向强化学习（NIRL）.

最大边际形式化的最大缺点是很多时候不存在单独的回报函数使得专家示例行为既是最优的又比其他任何行为好很多，或者有很多不同的回报函数会导致相同的专家策略，也就是说这种方法无法解决歧义的问题。



基于概率模型的方法可以解决歧义性的问题。学者们利用概率模型又发展出了很多逆向强化学习算法，如最大熵逆向强化学习、相对熵逆向强化学习、最大熵深度逆向强化学习，基于策略最优的逆向强化学习等等。对于这些模型，我会在后面一一介绍，请大家多多关注。

# 强化学习进阶 第十讲 逆向强化学习（第二节 基于最大边际的方法）

前言：在概述篇中，我们讲了逆向强化学习的分类，如图10.1所示。这一节，我讲第一大类：基于最大边际的逆向强化学习。本节将介绍四种方法，参考论文为：

[1] Apprenticeship
learning via inverse reinforcement learning （2004年
学徒学习）

[2] Maximum Margin
Planning (2006年 最大边际规划)

[3] Inverse
reinforcement learning through structured classification(2012年 结构化分类)

[4] Neural inverse reinforcement learning in autonomous navigation(2016年 神经逆向强化学习)

![img](https://pic2.zhimg.com/80/v2-2edd55d3fc4cd96edd098eb1a997d281_hd.png)

图10.1 逆向强化学习的分类

从文献的发表年份来看，该四篇文献几乎涵盖了近十年来逆向强化学习在最大边际方向上的发展。其中第4篇论文，更是采用了神经网络。这也反应了当前机器学习研究方向的转移轨迹。闲话少说，现在正式进入本节的课程。

**第一个问题，为什么要提出逆向强化学习？**

从第一讲到第九讲，我们讲的都是强化学习算法。讲了如何进行策略评估，如何进行策略迭代。但是有一个很关键的因素没有讲，那就是回报函数。因为在利用强化学习算法的时候，我们都假设回报函数是人为给定的。回报函数如何给定呢？这有很强的主观性和经验性。回报函数的不同会导致最优策略的不同。所以回报函数非常重要。但是当任务很复杂时，我们往往难以人为给出回报函数。比如在自动驾驶中，回报函数可能是信号灯、前面汽车，周边环境等各个因素的函数，我们很难人为给定这个回报函数。而且，在执行不同的任务时，回报函数也不同。所以，回报函数是阻碍强化学习算法得到普遍应用的一大障碍。逆向强化学习就是为解决学习回报函数的问题而提出来的。只有解决了这个问题，强化学习算法才能得到大规模应用。

**第二个问题，如何学习回报函数？**

其实逆向强化学习来源于模仿学习。模仿学习本身是一个很大的主题。小孩子在学习走路的时候，模仿大人们进行学习。人在学习很多技能的时候都是从模仿开始的。但有人只模仿到了表面，而有人模仿到了精髓。最早的模仿学习是行为克隆，它只模仿到了表面。在行为克隆中，人的示例轨迹被记录下来，下次执行时恢复该轨迹。行为克隆的方法只能模仿轨迹，无法进行泛化。而逆向强化学习是从专家（人为）示例中学到背后的回报函数，能泛化到其他情况，因此属于模仿到了精髓。

**第一个方法：学徒学习**

学徒学习是Ng（吴恩达）和Abbeel提出来的。学徒学习是这样：智能体从专家示例中学到回报函数，使得在该回报函数下所得到的最优策略在专家示例策略附近。

未知的回报函数![[公式]](https://www.zhihu.com/equation?tex=R%28s%29)一般都是状态的函数，因为它是未知的，所以我们可以利用函数逼近的方法对其进行参数逼近，其逼近形式可设为：![[公式]](https://www.zhihu.com/equation?tex=R%5Cleft%28s%5Cright%29%3Dw%5Ccdot%5Cphi%5Cleft%28s%5Cright%29)，其中![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28s%29)为基函数，可以为多项式基底，也可以为傅里叶基底。逆向强化学习求的是回报函数中的系数![[公式]](https://www.zhihu.com/equation?tex=w)。

根据值函数的定义，策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)的值函数为：

![[公式]](https://www.zhihu.com/equation?tex=+E_%7Bs_0+~+D%7D%5Cleft%5BV%5E%7B%5Cpi%7D%5Cleft%28s_0%5Cright%29%5Cright%5D+%5C%5C%3DE%5Cleft%5B%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5EtR%5Cleft%28s_t%5Cright%29%7C%5Cpi%5Cright%5D+%5C%5C%3DE%5Cleft%5B%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5Etw%5Ccdot%5Cphi%5Cleft%28s_t%5Cright%29%7C%5Cpi%5Cright%5D%5C%5C%3Dw%5Ccdot+E%5Cleft%5B%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5Et%5Cphi%5Cleft%28s_t%5Cright%29%7C%5Cpi%5Cright%5D+) (10.1)

定义特征期望为：![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%5Cleft%28%5Cpi%5Cright%29%3DE%5Cleft%5B%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5Et%5Cphi%5Cleft%28s_t%5Cright%29%7C%5Cpi%5Cright%5D)。需要注意的是，特征期望跟策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi)有关，策略不同时，策略期望也不相同。

定义了特征期望之后，值函数可以写为：![[公式]](https://www.zhihu.com/equation?tex=E_%7Bs_0~D%7D%5Cleft%5BV%5E%7B%5Cpi%7D%5Cleft%28s_0%5Cright%29%5Cright%5D%3Dw%5Ccdot%5Cmu%5Cleft%28%5Cpi%5Cright%29).

当给定m条专家轨迹后，根据定义我们可以估计专家策略的特征期望为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Cmu%7D_E%3D%5Cfrac%7B1%7D%7Bm%7D%5CSigma_%7Bi%3D1%7D%5E%7Bm%7D%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5Et%5Cphi%5Cleft%28s_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29+%5C%5D) (10.2)

其中，专家状态序列为专家轨迹:![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cleft%5C%7Bs_%7B0%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Cs_%7B1%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2C%5Ccdots%5Cright%5C%7D_%7Bi%3D1%7D%5E%7Bm%7D+%5C%5D)

逆向强化学习可以归结为如下问题：

找到一个策略，使得该策略的表现与专家策略相近。我们可以利用特征期望来表示一个策略的好坏，找到一个策略，使其表现与专家策略相近，其实就是找到一个策略![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7B%5Cpi%7D)的特征期望与专家策略的特征期望相近，即使如下不等式成立：

![[公式]](https://www.zhihu.com/equation?tex=%5ClVert%5Cmu%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29-%5Cmu_E%5CrVert_2%5Cle%5Cepsilon+)

当该不等式成立时，对于任意的权重![[公式]](https://www.zhihu.com/equation?tex=%5ClVert+w%5CrVert_1%5Cle+1)，值函数满足如下不等式：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cleft%7C+E%5Cleft%5B%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5EtR%5Cleft%28s_t%5Cright%29%7C%5Cpi_E%5Cright%5D-E%5Cleft%5B%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5EtR%5Cleft%28s_t%5Cright%29%7C%5Ctilde%7B%5Cpi%7D%5Cright%5D%5Cright%7C+%5C%5C%3D%5Cleft%7C+w%5ET%5Cmu%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29-w%5ET%5Cmu_E%5Cright%7C+%5C%5C+%5Cle%5ClVert+w%5CrVert_2%5ClVert%5Cmu%5Cleft%28%5Ctilde%7B%5Cpi%7D%5Cright%29-%5Cmu_E%5CrVert_2+%5C%5C+%5Cle+1%5Ccdot%5Cepsilon+%3D%5Cepsilon+%5C%5D) (10.3)

将（10.3）写成伪代码，如图10.2所示：

![img](https://pic2.zhimg.com/80/v2-345e3c180325970ddf5b91110dd88875_hd.png)

图10.2 学徒学习伪代码

**其中第二行的目标函数为:**

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+t%5E%7B%5Cleft%28i%5Cright%29%7D%3D%5Cmax_%7Bw%3A%5ClVert+w%5CrVert_2%5Cle+1%7D%5Cmin_%7Bj%5Cin%5Cleft%5C%7B0%5Ccdots%5Cleft%28i-1%5Cright%29%5Cright%5C%7D%7Dw%5ET%5Cleft%28%5Cmu_E-%5Cmu%5E%7B%5Cleft%28j%5Cright%29%7D%5Cright%29+%5C%5D)

写成标准的优化形式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmax_%7Bt%2Cw%7D+t+%5C%5Cs.t.%5C+w%5ET%5Cmu_E%5Cgeqslant+w%5ET%5Cmu%5E%7B%5Cleft%28j%5Cright%29%7D%2Bt%2C%5C+j%3D0%2C%5Ccdots+%2Ci-1+%5C%5C%5C%5C%5ClVert+w%5CrVert_2%5Cle+1+%5C%5D)

注意，在进行第二行求解时，![[公式]](https://www.zhihu.com/equation?tex=%5Cmu%5E%7B%5Cleft%28j%5Cright%29%7D)中的![[公式]](https://www.zhihu.com/equation?tex=j%5Cin%5Cleft%5C%7B0%2C1%2C%5Ccdots+%2Ci-1%5Cright%5C%7D)是前![[公式]](https://www.zhihu.com/equation?tex=i-1)次迭代得到的最优策略。也就是说第i次求解参数时，i-1次迭代的策略是已知的。这时候的最优函数值t相当于专家策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_E)与i-1个迭代策略之间的最大边际。

![img](https://pic2.zhimg.com/80/v2-3b5f5903a42af813eecda7c409006fd9_hd.png)

图10.3 最大边际方法的直观理解

如图10.3为最大边际方法的直观理解。我们可以从支持向量机的角度去理解。专家策略为一类，其他策略为另一类，参数的求解其实就是找一条超曲面将专家策略和其他策略区分开来。这个超平面使得两类之间的边际最大。

第四行是在第二行求出参数后，便有了回报函数![[公式]](https://www.zhihu.com/equation?tex=R%3D%5Cleft%28w%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%5ET%5Cphi)，利用该回报函数进行强化学习，从而得到该回报函数下的最优策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%5E%7B%5Cleft%28i%5Cright%29%7D)。

总结：学徒逆向强化学习方法分为两步，第一步在已经迭代得到的最优策略中，利用最大边际方法求出当前的回报函数的参数值；第二步利用求出的回报函数的参数值进行正向强化学习方法求得当前最优的策略，然后重复第一步。

**第二个方法：最大边际规划（MMP）的方法**

MMP的方法将逆向强化学习问题建模为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+D%3D%5Cleft%5C%7B%5Cleft%28%5Cmathcal%7BX%7D_i%2C%5Cmathcal%7BA%7D_i%2Cp_i%2CF_i%2Cy_i%2C%5Cmathcal%7BL%7D_i%5Cright%29%5Cright%5C%7D_%7Bi%3D1%7D%5E%7Bn%7D+%5C%5D)，其中，每一项分别为：状态空间，动作空间，状态转移概率，回报函数的特征向量，专家轨迹和策略损失函数。

在MMP的框架下，学习者试图找到一个特征到回报的线性映射也就是参数![[公式]](https://www.zhihu.com/equation?tex=w)，在这个线性映射下最好的策略在专家示例策略附近。可形式化为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin_%7Bw%2C%5Czeta_i%7D%5Cfrac%7B1%7D%7B2%7D%5ClVert+w%5CrVert%5E2%2B%5Cfrac%7B%5Cgamma%7D%7Bn%7D%5Csum_i%7B%5Cbeta_i%5Czeta_%7Bi%7D%5E%7Bq%7D%7D+%5C%5C+s.t.%5Cforall+i%5C+w%5ETf_i%5Cleft%28y_i%5Cright%29%2B%5Czeta_i%5Cgeqslant%5Cmax_%7By%5Cin%5Cmathcal%7BY%7D%7D%5C+w%5ETf_i%5Cleft%28y%5Cright%29%2B%5Cmathcal%7BL%7D_i%5Cleft%28y%5Cright%29+%5C%5D) (10.4)

其中约束可用下面两条进行理解：

1.
约束只允许专家示例得到最好的回报的那些权值存在

2.
回报的边际差，即专家示例的值函数与其他策略的值函数的差值与策略损失函数成正比。

这里的策略损失函数![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BL%7D_i%5Cleft%28y%5Cright%29)是指策略![[公式]](https://www.zhihu.com/equation?tex=y)与第i条专家轨迹![[公式]](https://www.zhihu.com/equation?tex=y_i)之间的差，该差可以利用轨迹中两种策略选择不同动作的综合来衡量。

用![[公式]](https://www.zhihu.com/equation?tex=%5Cmu_i)表示第i个专家策略，用![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)表示任意的策略。回报函数可以利用特征的线性组合表示，则（10.4）中回报函数![[公式]](https://www.zhihu.com/equation?tex=f_i%5Cleft%28y_i%5Cright%29%3DF_i%5Cmu_i)，其中![[公式]](https://www.zhihu.com/equation?tex=F_i)为特征基底。

原问题（10.4）可形式化:

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin_%7Bw%2C%5Czeta_i%7D%5Cfrac%7B1%7D%7B2%7D%5ClVert+w%5CrVert%5E2%2B%5Cfrac%7B%5Cgamma%7D%7Bn%7D%5Csum_i%7B%5Cbeta_i%5Czeta_%7Bi%7D%5E%7Bq%7D%7D+%5C%5C+s.t.%5Cforall+i%5C+w%5ETF_i%5Cmu_i%2B%5Czeta_i%5Cgeqslant%5Cmax_%7B%5Cmu%5Cin%5Cmathcal%7BG%7D_i%7D%5C+w%5ETF_i%5Cmu+%2Bl_%7Bi%7D%5E%7BT%7D%5Cmu+%5C%5D) (10.5)

注意：这里的策略![[公式]](https://www.zhihu.com/equation?tex=%5Cmu)是指每个状态被访问的频次。用树状图表示为：

![img](https://pic4.zhimg.com/80/v2-f86070067605a65150e9702ea92aa0ff_hd.png)

图10.4 策略频次流

其中状态![[公式]](https://www.zhihu.com/equation?tex=s%27)处的频次应满足流入流出关系：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csum_%7Bx%2Ca%7D%7B%5Cmu%5E%7Bx%2Ca%7Dp_i%5Cleft%28x%27%7Cx%2Ca%5Cright%29%2Bs_%7Bi%7D%5E%7Bx%27%7D%3D%5Csum_a%7B%5Cmu%5E%7Bx%27%2Ca%7D%7D%7D+%5C%5D) (10.6)

注意，其中的![[公式]](https://www.zhihu.com/equation?tex=s_%7Bi%7D%5E%7Bx%27%7D)表示初始位置。

接下来我们需要处理不等式约束右侧的最大值，右侧的最大值等价于下面问题：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmax_%7B%5Cmu%5Cin%5Cmathcal%7BG%7D_i%7D%5C+w%5ETF_i%5Cmu+%2Bl_%7Bi%7D%5E%7BT%7D%5Cmu+%5C%5C+sub.to%3A%5Csum_%7Bx%2Ca%7D%7B%5Cmu%5E%7Bx%2Ca%7Dp_i%5Cleft%28x%27%7Cx%2Ca%5Cright%29%2Bs_%7Bi%7D%5E%7Bx%27%7D%3D%5Csum_a%7B%5Cmu%5E%7Bx%27%2Ca%7D%7D%7D+%5C%5D) (10.7)

根据拉格朗日对偶原理（凸优化中的知识点，不了解的同学可以去查），其对偶问题为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin_%7B%5Cupsilon%5Cin+V_i%7Ds_%7Bi%7D%5E%7BT%7D%5Cupsilon+%5C%5Csub.to%3A%5Cforall+x%2Ca+%2C%5Cupsilon%5Ex%5Cgeqslant%5Cleft%28w%5ETF_i%2Bl_i%5Cright%29%5E%7Bx%2Ca%7D%2B%5Csum_%7Bx%27%7D%7Bp_i%5Cleft%28x%27%7Cx%2Ca%5Cright%29%5Cupsilon%5E%7Bx%27%7D%7D+%5C%5D) (10.8)

联合（10.5）和（10.8）逆向强化学习为一个二次规划问题：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin_%7Bw%2C%5Czeta_i%2C%5Cupsilon_i%7D%5Cfrac%7B1%7D%7B2%7D%5ClVert+w%5CrVert%5E2%2B%5Cfrac%7B%5Cgamma%7D%7Bn%7D%5Csum_i%7B%5Cbeta_i%5Czeta_%7Bi%7D%5E%7Bq%7D%7D+%5C%5C+s.t.%5Cforall+i%5C+w%5ETF_i%5Cmu_i%2B%5Czeta_i%5Cgeqslant+s_%7Bi%7D%5E%7BT%7D%5Cupsilon_i+%5C%5C+%5Cforall+i%2Cx%2Ca%2C%5C%5C%5Cupsilon_%7Bi%7D%5E%7Bx%7D%5Cgeqslant%5Cleft%28w%5ETF_i%2Bl_i%5Cright%29%5E%7Bx%2Ca%7D%2B%5Csum_%7Bx%27%7D%7Bp_i%5Cleft%28x%27%7Cx%2Ca%5Cright%29%5Cupsilon_%7Bi%7D%5E%7Bx%27%7D%7D+%5C%5D) (10.9)

利用凸优化求解方法求解系数。当然也可以利用其它更有效的方法进行求解。关于PPM的具体细节和具体应用可参考文献Maximum Margin Planning (2006年 最大边际规划)。该文献中的理论部分已经给大家掰开讲解了，相信再看这篇论文会轻松很多了。今天的更新已完成，明天更新**结构化分类方法。**

**第三个方法：基于结构化分类的方法**

第二个方法PPM的方法，在约束不等式部分，变量是策略。需要迭代求解MDP的解，而一个MDP解的维数跟状态的维数相同，因此求解过程复杂，计算代价很高。**那么，有没有一种不用求解MDP的方法来学习回报函数呢？**

答案是有，接下来要介绍的结构化分类的方法就是其中的一种。

在学徒学习算法中我们已经介绍过，回报函数可以表示为：

![[公式]](https://www.zhihu.com/equation?tex=R_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%3D%5Ctheta%5ET%5Cphi%5Cleft%28s%5Cright%29)

行为值函数则可以写为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q_%7B%5Ctheta%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3D%5Ctheta%5ET%5Cmu%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D)

其中![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmu%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3DE%5Cleft%5B%5Csum_%7Bt%3E0%7D%7B%5Cgamma%5Et%5Cphi%5Cleft%28S_t%5Cright%29%7CS_0%3Ds%2CA_0%3Da%2C%5Cpi%7D%5Cright%5D+%5C%5D)称为特征函数。关于特征函数，第i个元素![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmu_%7Bi%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3DQ_%7B%5Cphi_i%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D) ，我们可以理解为立即回报函数为![[公式]](https://www.zhihu.com/equation?tex=%5Cphi_i%5Cleft%28s%5Cright%29) 时，所对应的值函数。最后我们得到的行为值函数其实是不同立即回报函数所对应的值函数的线性组合。

为了避免迭代计算MDP解，我们可以这样考虑问题：

对于一个行为空间很小的问题（比如网格世界，状态空间可以有很多，但行为空间只有上下左右四个动作），最终的策略其实是找到每个状态所对应的最优动作**。用分类的思想去考虑最优的策略：每个动作看做一个类标签，那么所谓的策略其实就是把所有的状态分成四类，分类的标准是值函数，正确的分类对应着最大的值函数。**

利用这个思想，逆向强化学习可以形式化为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin_%7B%5Ctheta+%2C%5Czeta%7D%5Cfrac%7B1%7D%7B2%7D%5ClVert%5Ctheta%5CrVert%5E2%2B%5Cfrac%7B%5Ceta%7D%7BN%7D%5Csum_%7Bi%3D1%7D%5EN%7B%5Czeta_i%7D+%5C%5C+s.t.%5Cforall+i%2C+%5Ctheta%5ET%5Chat%7B%5Cmu%7D%5E%7B%5Cpi_E%7D%5Cleft%28s_i%2Ca_i%5Cright%29%2B%5Czeta_i%5Cgeqslant%5Cmax_a%5Ctheta%5ET%5Chat%7B%5Cmu%7D%5E%7B%5Cpi_E%7D%5Cleft%28s_i%2Ca%5Cright%29%2B%5Cmathcal%7BL%7D%5Cleft%28s_i%2Ca%5Cright%29+%5C%5D) （10.10）

约束中的![[公式]](https://www.zhihu.com/equation?tex=%5Cleft%5C%7Bs_i%2Ca_i%5Cright%5C%7D)为专家轨迹，![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmu%7D%5E%7B%5Cpi_E%7D%5Cleft%28s_i%2Ca_i%5Cright%29) 可以利用蒙特卡罗的方法求解。而对于![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7B%5Cmu%7D%5E%7B%5Cpi_E%7D%5Cleft%28s_i%2Ca%5Cne+a_i%5Cright%29)，则可以利用启发的方法来得到。

比如对于专家轨迹![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D%3D%5Cleft%5C%7Bs_1%2Ca_1%2Cs_2%2C%5Ccdots+%2Cs_%7BN-1%7D%2Ca_%7BN-1%7D%2Cs_N%2Ca_N%5Cright%5C%7D)，![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cforall%5C+1%5Cle+i%5Cle+N%2C%5Chat%7B%5Cmu%7D%5E%7B%5Cpi_E%7D%5Cleft%28s_i%2Ca_i%5Cright%29%3D%5Csum_%7Bj%3Di%7D%5EN%7B%5Cgamma%5E%7Bj-i%7D%5Cphi%5Cleft%28s_j%5Cright%29%7D+%5C%5C+%5Chat%7B%5Cmu%7D%5E%7B%5Cpi_E%7D%5Cleft%28s_i%2Ca%5Cne+a_i%5Cright%29%3D%5Cgamma%5Chat%7B%5Cmu%7D%5E%7B%5Cpi_E%7D%5Cleft%28s_i%2Ca_i%5Cright%29+%5C%5D)

评论：从数学形式化来看，结构化分类方法（10.10）和最大边际规划方法（10.9）有很多相似的地方。但两者有本质的不同，（10.10）对每个状态处的每个动作进行约束，而（10.9）是对一个MDP解进行约束。从计算量来看，（10.10）要小很多。具体细节请参看论文Inverse reinforcement learning through structured classification。今天的更新已完成（2017.5.9），明天更新神经逆向强化学习，神经网络要出场了。

**第四个方法 神经逆向强化学习**

逆向强化学习要学习的是回报函数，以便避免人为设定回报函数。但是，在进行学习回报函数的时候又引入了需要人为指定的基底，即我们之前已经假设了回报函数的形式为：

![[公式]](https://www.zhihu.com/equation?tex=R_%7B%5Ctheta%7D%5Cleft%28s%5Cright%29%3D%5Ctheta%5ET%5Cphi%5Cleft%28s%5Cright%29)

其中![[公式]](https://www.zhihu.com/equation?tex=%5Cphi%28s%29)是人为指定的基底。对于大规模问题，人为指定的基底表示能力不足，只能覆盖到部分回报函数形式，也难以泛化到其他状态空间。

其中一种解决方法是利用神经网络表示回报函数的基底。

这时，回报函数可表示为：![[公式]](https://www.zhihu.com/equation?tex=r%5Cleft%28s%2Ca%5Cright%29%3D%5Ctheta%5ETf%5Cleft%28s%2Ca%5Cright%29),其中![[公式]](https://www.zhihu.com/equation?tex=f%5Cleft%28s%2Ca%5Cright%29)为神经网络。

神经逆向强化学习的整个框架仍然是最大边际法的框架，因此问题形式化为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin_%7B%5Ctheta%7D+%5Cfrac%7B1%7D%7B2%7D%5ClVert%5Ctheta%5CrVert_%7B2%7D%5E%7B2%7D%2BC%5Csum_%7Bi%3D1%7D%5E%7BN_T%7D%7B%5Cxi%5E%7B%5Cleft%28i%5Cright%29%7D%7D+%5C%5C+s.t.%5C+Q_%7B%5Ctheta%7D%5E%7B%5Cpi_E%7D%5Cleft%28s_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Ca_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%2B%5Cxi%5E%7B%5Cleft%28i%5Cright%29%7D%5Cgeqslant%5Cmax_%7B%5Cpi%7D%5C+Q%5E%7B%5Cpi%7D%5Cleft%28s_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Ca_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29%2Bl%5Cleft%28s_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%2Ca_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29+%5C%5D) （10.11）

如图10.5为神经逆向强化学习的伪代码。

![img](https://pic1.zhimg.com/80/v2-2f11bebaa46836b4181d85346a0a56a0_hd.png)

图10.5 神经逆向强化学习伪代码

# 强化学习进阶 第十讲 逆向强化学习（第三节 基于最大熵的方法）


  

强化学习进阶 第十讲 逆向强化学习（第三节 基于最大熵的方法）

在第十讲的概述部分，我们已经介绍过，基于最大边际的方法往往会产生歧义，比如或许存在很多不同的回报函数导致相同的专家策略。在这种情况下，所学到的回报函数往往具有随机的偏好。为了克服这个缺点，学者们利用概率模型，提出基于最大熵的逆向强化学习方法。

**第一个问题，最大熵方法为什么可以克服歧义性的问题呢？**

首先，我们先介绍下什么是最大熵。对于这个概念的理解，推荐大家读一读**吴军**老师的《数学之美》第20章。关于最大熵模型的公式推导，建议大家读一读**李航**老师的《统计学习方法》第6章。在概率论中，熵是不确定性的度量。不确定性越大，熵越大。比如，在区间固定时，所有的分布中均匀分布的熵最大。因为均匀分布在固定区间每一点取值的概率都相等，所以取哪个值的不确定性最大。

最大熵原理是指，在学习概率模型时，在所有满足约束的概率模型（分布）中，熵最大的模型是最好的模型。这是因为，通过熵最大所选取的模型，没有对未知（即除了约束已知外）做任何主观假设。也就是说，除了约束条件外，我们不知道任何其他信息。

比如当我们猜测一个筛子每个面朝上的概率是多少时，我们猜每个面朝上的概率都是1/6，其实这个解就是最大熵解。因为，我们除了知道每个面朝上的概率加起来等于1外，其他条件都是不知道的，这时猜测出的均匀分布就是最大熵解。

那么，对于我们的逆向强化学习问题，最大熵模型是什么呢？其实，在学徒学习中，我们在计算特征期望的时候，已经用到了概率模型，我们回忆下，特征期望可定义为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmu%5Cleft%28%5Cpi%5Cright%29%3DE%5Cleft%5B%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5Et%5Cphi%5Cleft%28s_t%5Cright%29%7C%5Cpi%5Cright%5D+%5C%5D)

给定m条专家轨迹时，我们可以估计专家的特征期望为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7B%5Cmu%7D_E%3D%5Cfrac%7B1%7D%7Bm%7D%5CSigma_%7Bi%3D1%7D%5E%7Bm%7D%5CSigma_%7Bt%3D0%7D%5E%7B%5Cinfty%7D%5Cgamma%5Et%5Cphi%5Cleft%28s_%7Bt%7D%5E%7B%5Cleft%28i%5Cright%29%7D%5Cright%29+%5C%5D)

从概率模型的角度建模逆向强化学习，我们可以这样考虑。存在一个要潜在的概率分布，在该概率分布下，产生了专家轨迹。这是典型的已知数据，求模型的问题。也就是说，知道专家轨迹，求解产生该轨迹分布的概率模型。此时，已知条件为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csum_%7BPath%5C%5Czeta_i%7D%7BP%5Cleft%28%5Czeta_i%5Cright%29f%7D%3D%5Ctilde%7Bf%7D+%5C%5D) (10.12)

这里用![[公式]](https://www.zhihu.com/equation?tex=f)表示特征期望，![[公式]](https://www.zhihu.com/equation?tex=%5Ctilde%7Bf%7D)表示专家特征期望。满足（10.12）约束条件的所有概率分布中，熵最大的概率分布是除了约束外对其他任何未知信息没有做任何假设的。所以，最大熵的方法可以避免歧义性的问题。

....................................................................................................................................................

**继续更新（2017.5.14）**

**第一种方法：基于最大熵的逆向强化学习[1]**

如何利用最大熵原理恢复满足约束条件的概率分布呢？熵最大，是最优问题。因此，我们将该问题转化成为标准的优化问题：



该优化问题可形式化为：

![[公式]](https://www.zhihu.com/equation?tex=%5Cmax-p%5Clog+p+%5C%5C+s.t.%5Csum_%7BPath%5Czeta_i%7D%7BP%5Cleft%28%5Czeta_i%5Cright%29f_%7B%5Czeta_i%7D%3D%5Ctilde%7Bf%7D%7D+%5C%5C+%5CvarSigma+P%3D1+) （10.13）



公式（10.13）是熵最大的形式化表述，利用拉格朗日乘子法，该优化问题可转化为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin%5Ctextrm%7B%5C+%7DL%3D%5Csum_%7B%5Czeta_i%7D%7Bp%5Clog+p-%5Csum_%7Bj%3D1%7D%5En%7B%5Clambda_j%5Cleft%28pf_j-%5Ctilde%7Bf%7D_j%5Cright%29%7D-%5Clambda_0%5Cleft%28%5CvarSigma+p-1%5Cright%29%7D+%5C%5D) （10.14）



将（10.14）对概率![[公式]](https://www.zhihu.com/equation?tex=p)进行微分，并令其导数为0，可以得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cfrac%7B%5Cpartial+L%7D%7B%5Cpartial+p%7D%3D%5Csum_%7B%5Czeta_i%7D%7B%5Clog+p%2B1-%7D%5Csum_%7Bj%3D1%7D%5En%7B%5Clambda_jf_j%7D-%5Clambda_0%3D0+%5C%5D)

最后得到拥有最大熵的概率为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+p%3D%5Cfrac%7B%5Cexp%5Cleft%28%5Csum_%7Bj%3D1%7D%5En%7B%5Clambda_jf_j%7D%5Cright%29%7D%7B%5Cexp%5Cleft%281-%5Clambda_0%5Cright%29%7D%3D%5Cfrac%7B1%7D%7BZ%7D%5Cexp%5Cleft%28%5Csum_%7Bj%3D1%7D%5En%7B%5Clambda_jf_j%7D%5Cright%29+%5C%5D) (10.15)

其中参数![[公式]](https://www.zhihu.com/equation?tex=%5Clambda_j)对应着回报函数中的参数。可以利用最大似然的方法进行求解。



一般而言，利用最大似然的方法对式（10.15）中的参数进行求解时，往往会遇到未知的配分函数项![[公式]](https://www.zhihu.com/equation?tex=Z)，因此不能直接求解。一种可行的方法是利用次梯度的方法。

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla+L%5Cleft%28%5Clambda%5Cright%29%3D%5Ctilde%7Bf%7D-%5Csum_%7B%5Czeta%7D%7BP%5Cleft%28%5Czeta+%7C%5Clambda+%2CT%5Cright%29f_%7B%5Czeta%7D%7D+%5C%5D) (10.16)

**第二种方法：基于相对熵的逆向强化学习[2]**

在第一种方法中，即基于最大熵的逆向强化学习中，最后求解参数时，需要利用次梯度（10.16），在（10.16）中需要利用轨迹的概率![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28%5Czeta%5Cright%29)。该轨迹的概率可表示为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Pr%5Cleft%28%5Ctau+%7C%5Ctheta+%2CT%5Cright%29%5Cpropto+d_0%5Cleft%28s_1%5Cright%29%5Cexp%5Cleft%28%5Csum_%7Bi%3D1%7D%5Ek%7B%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%7D%5Cright%29%5Cprod_%7Bt%3D1%7D%5EH%7BT%5Cleft%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%5Cright%29%7D+%5C%5D)

求解该式的前提是系统的状态转移概率![[公式]](https://www.zhihu.com/equation?tex=T%5Cleft%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%5Cright%29)是已知的。然而，在无模型的强化学习中，该模型是未知的。为了解决这个问题，我们可以将问题建模为求解相对熵最大。

设![[公式]](https://www.zhihu.com/equation?tex=Q)为利用均匀分布策略产生的轨迹分布，要求解的概率分布为![[公式]](https://www.zhihu.com/equation?tex=P%28%5Ctau%29)，问题可形式化为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin_P%5Csum_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D%7D%7BP%5Cleft%28%5Ctau%5Cright%29%5Cln%5Cfrac%7BP%5Cleft%28%5Ctau%5Cright%29%7D%7BQ%5Cleft%28%5Ctau%5Cright%29%7D%7D+%5C%5C+s.t.for~+all~~+i%5Cin%5Cleft%5C%7B1%2C%5Ccdots+%2Ck%5Cright%5C%7D%5Ctextrm%7B%EF%BC%9A%7D+%5C%5C+%5Cleft%7C%5Csum_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D%7D%7BP%5Cleft%28%5Ctau%5Cright%29f_%7Bi%7D%5E%7B%5Ctau%7D-%5Chat%7Bf%7D_i%7D%5Cright%7C%5Cle%5Cepsilon_i+%5C%5C+%5Csum_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D%7D%7BP%5Cleft%28%5Ctau%5Cright%29%3D1%7D+%5Cforall%5Ctau%5Cin%5Cmathcal%7BT%7D%5C+%3A%5C+P%5Cleft%28%5Ctau%5Cright%29%5Cgeqslant+0+%5C%5D) (10.17)

同样，利用拉格朗日乘子法和KKT条件，可以得到相对熵最大的解：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+P%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29%3D%5Cfrac%7B1%7D%7BZ%5Cleft%28%5Ctheta%5Cright%29%7DQ%5Cleft%28%5Ctau%5Cright%29%5Cexp%5Cleft%28%5Csum_%7Bi%3D1%7D%5Ek%7B%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%7D%5Cright%29+%5C%5D) (10.18)



跟最大熵逆向强化学习方法相同，参数的求解过程利用次梯度的方法：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla+L%5Cleft%28%5Ctheta%5Cright%29%3D%5Chat%7Bf%7D_i-%5Csum_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D%7D%7BP%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29f_%7Bi%7D%5E%7B%5Ctau%7D-%5Calpha_i%5Cepsilon_i%7D+%5C%5D) (10.19)

在利用次梯度的方法进行参数求解时，最关键的问题是估计（10.19）式中的概率![[公式]](https://www.zhihu.com/equation?tex=P%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29+)。由最大相对熵的求解可以得到该概率的计算公式，如（10.18）。



我们将Q显示表述出来，由定义知道，它是在策略为均匀策略时得到的轨迹分布，因此可将其分解为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5Cleft%28%5Ctau%5Cright%29%3DD%5Cleft%28%5Ctau%5Cright%29U%5Cleft%28%5Ctau%5Cright%29+%5C%5C+%5Cleft%5C%7B%5Cbegin%7Barray%7D%7Bc%7D+D%5Cleft%28%5Ctau%5Cright%29%3Dd_0%5Cleft%28s_1%5Cright%29%5CvarPi_%7Bt%3D1%7D%5E%7BH%7DT%5Cleft%28s_%7Bt%2B1%7D%7Cs_t%2Ca_t%5Cright%29%5C%5C+U%5Cleft%28%5Ctau%5Cright%29%3D%5Cfrac%7B1%7D%7B%5Cleft%7C%5Cmathcal%7BA%7D%5Cright%7C%5EH%7D%5C%5C+%5Cend%7Barray%7D%5Cright.+%5C%5D) (10.20)

将（10.20）带入到（10.18），可以得到最大相对熵解为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+P%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29%3D%5Cfrac%7BD%5Cleft%28%5Ctau%5Cright%29%5Cexp%5Cleft%28%5CvarSigma_%7Bi%3D1%7D%5E%7Bk%7D%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%5Cright%29%7D%7B%5CvarSigma_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D%7DD%5Cleft%28%5Ctau%5Cright%29%5Cexp%5Cleft%28%5CvarSigma_%7Bi%3D1%7D%5E%7Bk%7D%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%5Cright%29%7D+%5C%5D) (10.21)



这时，我们再利用重要性采样对（10.21）进行估计，得到次梯度为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Chat%7Bf%7D_i-%5Csum_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D%7D%7BP%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29f_%7Bi%7D%5E%7B%5Ctau%7D-%5Calpha_i%5Cepsilon_i%7D+%5C%5C+%3D%5Chat%7Bf%7D_i-%5Cfrac%7B1%7D%7BN%7D%5Csum_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D_%7BN%7D%5E%7B%5Cpi%7D%7D%7B%5Cfrac%7BP%5Cleft%28%5Ctau+%7C%5Ctheta%5Cright%29%7D%7BD%5Cleft%28%5Ctau%5Cright%29%5Cpi%5Cleft%28%5Ctau%5Cright%29%7D%7Df_%7Bi%7D%5E%7B%5Ctau%7D-%5Calpha_i%5Cepsilon_i+%5C%5C+%3D%5Chat%7Bf%7D_i-%5Cfrac%7B1%7D%7BN%7D%5Cfrac%7B%5CvarSigma_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D_%7BN%7D%5E%7B%5Cpi%7D%7D%5Cfrac%7BD%5Cleft%28%5Ctau%5Cright%29%5Cexp%5Cleft%28%5CvarSigma_%7Bi%3D1%7D%5E%7Bk%7D%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%5Cright%29%7D%7BD%5Cleft%28%5Ctau%5Cright%29%5Cpi%5Cleft%28%5Ctau%5Cright%29%7D%7D%7BD%5Cleft%28%5Ctau%5Cright%29%5Cexp%5Cleft%28%5CvarSigma_%7Bi%3D1%7D%5E%7Bk%7D%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%5Cright%29%7Df_%7Bi%7D%5E%7B%5Ctau%7D-%5Calpha_i%5Cepsilon_i+%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%3D%5Chat%7Bf%7D_i-%5Cfrac%7B%5Cfrac%7B1%7D%7BN%7D%5CvarSigma_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D_%7BN%7D%5E%7B%5Cpi%7D%7D%5Cfrac%7BD%5Cleft%28%5Ctau%5Cright%29%5Cexp%5Cleft%28%5CvarSigma_%7Bi%3D1%7D%5E%7Bk%7D%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%5Cright%29%7D%7BD%5Cleft%28%5Ctau%5Cright%29%5Cpi%5Cleft%28%5Ctau%5Cright%29%7D%7D%7B%5Cfrac%7B1%7D%7BN%7D%5CvarSigma_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D_%7BN%7D%5E%7B%5Cpi%7D%7D%5Cfrac%7BD%5Cleft%28%5Ctau%5Cright%29%5Cexp%5Cleft%28%5CvarSigma_%7Bi%3D1%7D%5E%7Bk%7D%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%5Cright%29%7D%7BD%5Cleft%28%5Ctau%5Cright%29%5Cpi%5Cleft%28%5Ctau%5Cright%29%7D%7Df_%7Bi%7D%5E%7B%5Ctau%7D-%5Calpha_i%5Cepsilon_i+%5C%5C+%3D%5Chat%7Bf%7D_i-%5Cfrac%7B%5CvarSigma_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D_%7BN%7D%5E%7B%5Cpi%7D%7D%5Cfrac%7B%5Cexp%5Cleft%28%5CvarSigma_%7Bi%3D1%7D%5E%7Bk%7D%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%5Cright%29%7D%7B%5Cpi%5Cleft%28%5Ctau%5Cright%29%7D%7D%7B%5CvarSigma_%7B%5Ctau%5Cin%5Cmathcal%7BT%7D_%7BN%7D%5E%7B%5Cpi%7D%7D%5Cfrac%7B%5Cexp%5Cleft%28%5CvarSigma_%7Bi%3D1%7D%5E%7Bk%7D%5Ctheta_if_%7Bi%7D%5E%7B%5Ctau%7D%5Cright%29%7D%7B%5Cpi%5Cleft%28%5Ctau%5Cright%29%7D%7Df_%7Bi%7D%5E%7B%5Ctau%7D-%5Calpha_i%5Cepsilon_i+%5C%5D) (10.22)

参考文献：

[1] Ziebart, B., Maas, A., Bagnell, A., and Dey, A. (2008). Maximum Entropy Inverse Reinforcement Learning. In Proceedings of The Twenty-third AAAI Conference on Arti cial Intelligence (AAAI'08), pages 1433-1438.

[2] Boularias, A., Kober, J., and Peters, J. Relative entropy inverse reinforcement learning. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2011.

# 强化学习前沿 第一讲 组合策略梯度和值函数方法（PGQ）

强化学习前沿 第一讲 组合策略梯度和值函数方法（PGQ）

前言：从本节开始，我们讲强化学习算法前沿，即2016年-2017年比较重要的强化学习算法。学习本系列的前提是熟悉本专栏的前面课程即：强化学习入门（1-5讲）和强化学习进阶（6-10讲）。

随着强化学习算法受到越来越多的学者的关注，这两年，强化学习算法在各个领域蓬勃发展。尤其是深度学习与强化学习结合而形成的深度强化学习算法，在各个领域不断崭露头角，挑战着相关领域的极限，甚至已经在某些领域取得了新的突破。强化学习算法不断发展，被认为是实现通用人工智能算法最有潜力的方法。

从已经发表的论文来看，强化学习算法的发展趋势为：

**1.强化学习算法与深度学习的结合会更加紧密。**

机器学习算法常被分为监督学习、非监督学习和强化学习，以前三类方法分得很清楚，而如今三类方法联合起来使用效果会更好。所以，强化学习算法其中一个趋势便是三类机器学习方法在逐渐走向统一的道路。谁结合的好，谁就会有更好的突破。该方向的代表作如基于深度强化学习的对话生成等。

**2.强化学习算法与专业知识结合得将更加紧密。**

如果将一般的强化学习算法，如qlearning算法直接套到专业领域中，很可能不work。这时候一定不能灰心，因为这是正常现象。这时需要把专业领域中的知识加入到强化学习算法中，如何加？没有统一的方法，根据每个专业而变化。通常来说你可以重新塑造回报函数，或修改网络结构（大家可以开心的炼丹灌水了）。该方向的代表作是NIPS2016最佳论文的值迭代网络（Value Iteration Networks）等。

**3.强化学习算法理论分析会更强，算法会更稳定和高效。**

强化学习算法大火之后，必定会吸引一大批理论功底很强的牛人。这些牛人不愁吃穿，

追求完美主义，又有很强的数学技巧，所以在强化学习这个理论性还几乎是空白的领域，他们必定会建功立业，名垂千史。该方向的代表作如基于深度能量的策略方法，值函数与策略方法的等价性等。

**4.强化学习算法跟脑科学、认知神经科学和记忆会更紧密**

这个流派应该是以deepmind和伦敦大学学院为首，因为这些团体里面不仅仅有很多人

工智能学家还有很多认知神经科学家。该方向的代表作如deepmind关于记忆的一列论文。


本系列教程强化学习前沿将在这四个方面为大家分享和探讨一些代表作.

本节和大家分享的是一篇理论分析的论文《Combining policy gradient and Q-learning》.在该论文中，作者分析了值函数和策略之间的关系。

我们知道，强化学习算法常分为值函数的方法和直接策略搜索的方法。那么我们可能会问，这两种方法到底有什么关系呢？

根据策略梯度的理论，策略梯度可表示为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%7DJ%5Cleft%28%5Cpi%5Cright%29%3D%5Cunderset%7Bs%2Ca%7D%7BE%7DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29+%5C%5D) (1.1)



从式（1.1）中，我们无法看出值函数和策略之间有什么直接的联系。为了建立他们之间的联系，我们还需要再看两个知识点。

**第一个知识点：策略梯度的熵正则化**

为了防止策略变成确定性策略，失去探索性，在实际应用中常用的一个技巧是在策略梯度上增加一个熵正则化项，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5CvarDelta%5Ctheta%5Cpropto%5Cunderset%7Bs%2Ca%7D%7BE%7DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cunderset%7Bs%7D%7BE%7D%5Cnabla_%7B%5Ctheta%7DH%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29+%5C%5D) (1.2)

其中![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+H%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3D-%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D+%5C%5D) 熵是不确定性的度量，不确定性越大熵越大，式（1.2）的第二项的意思是让参数朝着不确定性大的方向更新。当然了，第一项是向着值函数更大的方向更新，第二项的系数![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)是正则惩罚参数，该参数在值函数的收敛中起到重要的作用。

**第二个知识点：贝尔曼方程的不动点**

贝尔曼操作符![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D%5E%7B%5Cpi%7D)定义为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmathcal%7BT%7D%5E%7B%5Cpi%7DQ%5Cleft%28s%2Ca%5Cright%29%3D%5Cunderset%7Bs%27%2Cr%2Cb%7D%7BE%7D%5Cleft%28r%5Cleft%28s%2Ca%5Cright%29%2B%5Cgamma+Q%5Cleft%28s%27%2Cb%5Cright%29%5Cright%29+%5C%5D).该操作符为压缩映射，具体请看知乎专栏
[读者常见问题汇总及解答 - 知乎专栏](https://zhuanlan.zhihu.com/p/26214408)
，该压缩映射最后会收敛到一个不动点。联合上面两个知识点，我们可以得到，在不动点处行为值函数的参数![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta)在熵正则化的梯度方向上将不会再更新。

我们令![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+f%5Cleft%28%5Ctheta%5Cright%29%3D%5Cunderset%7Bs%2Ca%7D%7BE%7DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cunderset%7Bs%7D%7BE%7D%5Cnabla_%7B%5Ctheta%7DH%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29+%5C%5D)，同时随机策略本身需要满足概率为1的约束条件，即![[公式]](https://www.zhihu.com/equation?tex=g%5Cleft%28%5Ctheta%5Cright%29%3D%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D%3D1)

将约束条件带入不动点处正则化的梯度为零可以得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+f%5Cleft%28%5Ctheta%5Cright%29-%5Csum_s%7B%5Clambda_s%5Cnabla_%7B%5Ctheta%7Dg_s%5Cleft%28%5Cpi%5Cright%29%7D+%5C%5C+%3D%5Cunderset%7Bs%2Ca%7D%7BE%7DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Cunderset%7Bs%7D%7BE%7D%5Cnabla_%7B%5Ctheta%7D%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%7D%5Csum_s%7B%5Clambda_s%5Cnabla_%7B%5Ctheta%7D%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D%7D+%5C%5C+%3D%5Cunderset%7Bs%2Ca%7D%7BE%7DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Cunderset%7Bs%7D%7BE%7D%5Csum_a%7B%5Cnabla_%7B%5Ctheta%7D%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Cunderset%7Bs%7D%7BE%7D%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Csum_s%7B%5Clambda_s%5Csum_a%7B%5Cnabla_%7B%5Ctheta%7D%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D%7D%7D%7D+%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%3D%5Cunderset%7Bs%2Ca%7D%7BE%7DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Cunderset%7Bs%7D%7BE%7D%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7D%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Cunderset%7Bs%7D%7BE%7D%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Csum_s%7B%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Clambda_s%5Cfrac%7B%5Cnabla_%7B%5Ctheta%7D%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D%7D%7D%7D%7D+%5C%5C+%3D%5Cunderset%7Bs%2Ca%7D%7BE%7DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Clambda%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29+%5C%5C+%3D%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha+-%5Clambda%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29+%5C%5D)（1.3）

其中第3个等式到第4个等式用到了期望的定义即：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%7D%3D%5Cunderset%7Ba%7D%7BE%7D+%5C%5D)

由分析知道，在不动点处有：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+f%5Cleft%28%5Ctheta%5Cright%29-%5Csum_s%7B%5Clambda_s%5Cnabla_%7B%5Ctheta%7Dg_s%5Cleft%28%5Cpi%5Cright%29%7D%3D%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha+-%5Clambda%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%3D0+%5C%5D)

由于：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cnabla_%7B%5Ctheta%5Cleft%28t%2Cb%5Cright%29%7D%5Cpi%5Cleft%28s%2Ca%5Cright%29%3D1_%7B%5Cleft%28t%2Cb%5Cright%29%3D%5Cleft%28s%2Ca%5Cright%29%7D+%5C%5D)



上式变为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-c%3D0+%5C%5D) （1.4）

其中：![[公式]](https://www.zhihu.com/equation?tex=c%3D%5Calpha+%2B%5Clambda+)

式（1.4）给出了行为值函数![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D)和策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%5Cleft%28s%2Ca%5Cright%29+)之间的关系，为了消去常数![[公式]](https://www.zhihu.com/equation?tex=c)，我们利用策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%5Cleft%28s%2Ca%5Cright%29+)在状态s处采样，并对该状态处的动作a进行积分，得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%7D%7D%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29c%5Cleft%28s%5Cright%29%3D0%7D+%5C%5D) (1.4)

其中：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3DV%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29%7D+%5C%5D)，![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+-%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%3D%7DH%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29+%5C%5D)，



（1.5）式变为：

![[公式]](https://www.zhihu.com/equation?tex=c_s%3D%5Calpha+H%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29%2BV%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29) （1.6）

将（1.6）式带回到（1.4）式，我们可以得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3D%5Calpha%5Cleft%28%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%2BH%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29%5Cright%29%2BV%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29+%5C%5D) （1.7）

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28s%2Ca%5Cright%29%3D%5Cexp%5Cleft%28A%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%2F%5Calpha+-H%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29%5Cright%29+%5C%5D) （1.8）



（1.7）式和（1.8）式给出了动作值函数![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D)与策略![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi%5Cleft%28s%2Ca%5Cright%29+%5C%5D)之间的关系。

到目前为止，我们已经得到了动作值函数和策略之间的关系，即（1.7）式和（1.8）式。那么有人会问，得到了两者之间的关系有什么用呢？

**我们可以利用当前策略来估计动作值函数**

由（1.7）式，我们可以给出逼近动作值函数的方法：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%3D%5Ctilde%7BA%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%2BV%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29%3D%5Calpha%5Cleft%28%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%2BH%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29%5Cright%29%2BV%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29+%5C%5D) （1.9）



这时我们再看熵正则化的策略梯度更新公式（1.2），在（1.3）的推导中我们能得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cunderset%7Bs%2Ca%7D%7BE%7DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%2B%5Calpha%5Cunderset%7Bs%7D%7BE%7D%5Cnabla_%7B%5Ctheta%7DH%5E%7B%5Cpi%7D%5Cleft%28s%5Cright%29+%5C%5C+%3D%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29+%5C%5D)



有常数偏差时，更新不变，因此（1.2）式可以写为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cbigtriangleup%5Ctheta%5Cpropto%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29+%5C%5D) （1.10）

从优化的角度去理解不动点处值函数和策略之间的关系：

考虑优化问题：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmin+imize~~+E_%7Bs%2Ca%7D%5Cleft%28q%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%5Cright%29%5E2+%5C%5C+s.t.%5C%5C%5C%5C%5Csum_a%7B%5Cpi%5Cleft%28s%2Ca%5Cright%29%3D1%2C%5C%5C%5C%5C+s%5Cin+S%7D+%5C%5D) （1.11）



极值点处的条件为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28q%5Cleft%28s%2Ca%5Cright%29-%5Calpha%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%2Bc_s%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%3D0+%5C%5D)

若![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+q%5Cleft%28s%2Ca%5Cright%29%3DQ%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D)，则优化问题得到的等式与根据不动点得到的等式相同。因此，（1.11）式可以有这样的解释：熵正则化策略梯度时，在不动点处，动作值函数可以看作是策略对数的回归。

在给出PGQ算法之前，还需要解决的问题是当采用策略（1.8）时，动作值函数是否会收敛到最优值？



**结论是动作值函数的贝尔曼残差会随着熵惩罚系数下降而下降。**

证明：

由（1.8）式，策略![[公式]](https://www.zhihu.com/equation?tex=%5Cpi%5Cleft%28s%2Ca%5Cright%29)可以写为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cpi_%7B%5Calpha%7D%5Cleft%28s%2Ca%5Cright%29%3D%5Cfrac%7B%5Cexp%5Cleft%28Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29%2F%5Calpha%5Cright%29%7D%7B%5Csum_b%7B%5Cexp%5Cleft%28Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Cb%5Cright%29%2F%5Calpha%5Cright%29%7D%7D%5Cle%5Cfrac%7B%5Cexp%5Cleft%28Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29%2F%5Calpha%5Cright%29%7D%7B%5Cexp%5Cleft%28%5Cmax_cQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Cc%5Cright%29%2F%5Calpha%5Cright%29%7D+%5C%5D) （1.12）

设![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BT%7D%5E%2A)为贪婪变换，则贝尔曼残差为：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmathcal%7BT%7D%5E%2AQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5D)

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+0%5Cle%5Cmathcal%7BT%7D%5E%2AQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5C+%3D%5Cmathcal%7BT%7D%5E%2AQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29-%5Cmathcal%7BT%7D%5E%7B%5Cpi_%7B%5Calpha%7D%7DQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29+%5C%5C+%3DE_%7Bs%27%7D%5Cleft%28%5Cmax_cQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cc%5Cright%29-%5Csum_b%7B%5Cpi_%7B%5Calpha%7D%5Cleft%28s%27%2Cb%5Cright%29Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cb%5Cright%29%7D%5Cright%29+%5C%5C+%3DE_%7Bs%27%7D%5Cleft%28%5Csum_b%7B%5Cpi_%7B%5Calpha%7D%5Cleft%28s%27%2Cb%5Cright%29%7D%5Cleft%28%5Cmax_cQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cc%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cb%5Cright%29%5Cright%29%5Cright%29+%5C%5D) （1.13）

将（1.12）带入（1.13）缩放不等式，得到：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmathcal%7BT%7D%5E%2AQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29%5Cle+E_%7Bs%27%7D%5Csum_b%7B%5Cexp%5Cleft%28%5Cleft%28Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cb%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cb%5E%2A%5Cright%29%5Cright%29%2F%5Calpha%5Cright%29%5Cleft%28%5Cmax_cQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cc%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cb%5Cright%29%5Cright%29%7D+%5C%5D)

设函数：![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+f_%7B%5Calpha%7D%5Cleft%28x%5Cright%29%3Dx%5Cexp%5Cleft%28-x%2F%5Calpha%5Cright%29+%5C%5D)，令![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+x%3D%5Cmax_cQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cc%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%27%2Cb%5Cright%29+%5C%5D)

则不等式变为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmathcal%7BT%7D%5E%2AQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29%5Cle+E_%7Bs%27%7D%5Csum_b%7Bf_%7B%5Calpha%7D%5Cleft%28x%5Cright%29%7D+%5C%5D) （1.14）

根据函数![[公式]](https://www.zhihu.com/equation?tex=f_%7B%5Calpha%7D%5Cleft%28x%5Cright%29)的性质，![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+f_%7B%5Calpha%7D%5Cleft%28x%5Cright%29%5Cle+f_%7B%5Calpha%7D%5Cleft%28%5Calpha%5Cright%29%3D%5Calpha+e%5E%7B-1%7D+%5C%5D)

因此原不等式（1.14）变为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5Cmathcal%7BT%7D%5E%2AQ%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29-Q%5E%7B%5Cpi_%7B%5Calpha%7D%7D%5Cleft%28s%2Ca%5Cright%29%5Cle%5Cleft%7C%5Cmathcal%7BA%7D%5Cright%7C%5Calpha+e%5E%7B-1%7D+%5C%5D) （1.15）



因此我们可以看到，贝尔曼残差随着![[公式]](https://www.zhihu.com/equation?tex=%5Calpha)的降低而收敛到0.

有了上面的分析，我们便可以介绍PGQL学习算法了。

**PGQL算法**

PGQL学习算法是基于值函数的估计（1.9）式，组合了熵正则化的策略梯度更新和Qlearning的方法。

其中Qlearning的更新是为了减小贝尔曼残差其更新公式为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5CDelta%5Ctheta%5Cpropto%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28%5Cmathcal%7BT%7D%5E%2A%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%2C+%5C%5C+%5CDelta+w%5Cpropto%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28%5Cmathcal%7BT%7D%5E%2A%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%29%5Cnabla_wV%5Cleft%28s%5Cright%29+%5C%5D)



熵正则化的策略梯度更新为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5CDelta%5Ctheta%5Cpropto%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi%5Cleft%28s%2Ca%5Cright%29%2C+%5C%5C+%5CDelta+w%5Cpropto%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28Q%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cleft%28s%2Ca%5Cright%29%5Cright%29%5Cnabla_wV%5Cleft%28s%5Cright%29+%5C%5D)



PGQL方法是将两种更新进行加权组合，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B+%5CDelta%5Ctheta%5Cpropto%5Cleft%281-%5Ceta%5Cright%29%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28Q%5E%7B%5Cpi%7D-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi+%2B%5Ceta%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28%5Cmathcal%7BT%7D%5E%2A%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cright%29%5Cnabla_%7B%5Ctheta%7D%5Clog%5Cpi+%2C+%5C%5C+%5CDelta+w%5Cpropto%5Cleft%281-%5Ceta%5Cright%29%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28Q%5E%7B%5Cpi%7D-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cright%29%5Cnabla_wV%5Cleft%28s%5Cright%29%2B%5Ceta%5Cunderset%7Bs%2Ca%7D%7BE%7D%5Cleft%28%5Cmathcal%7BT%7D%5E%2A%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D-%5Ctilde%7BQ%7D%5E%7B%5Cpi%7D%5Cright%29%5Cnabla_wV+%5C%5D)  

# 强化学习前沿 第二讲 值迭代网络


  强化学习前沿 第二讲 值迭代网络

前言：在上一讲中，我总结了强化学习的四个发展方向，其中第二个发展方向是强化学习与专业知识结合得将更加紧密，举的例子是NIPS2016最佳论文 Value Iteration Networks, 即值迭代网络。今天，我跟大家分享一下什么是值迭代网络。参考文献为《Value Iteration Networks》以及伯克利深度强化学习课程 [CS 294 Deep Reinforcement Learning, Spring 2017](https://link.zhihu.com/?target=http%3A//rll.berkeley.edu/deeprlcourse/) 4月10号的课，这节课是值迭代网络的第一作者Aviv Tamar讲的。建议大家在看完本贴后，结合本贴看论文和Tamar的ppt。

本讲分为以下几个小节：2.1 节，我们会探讨为什么要提出值迭代网络； 2.2节，我们会讲值迭代网络模型； 2.3节，我们会简单地探讨下训练方法。

**第2.1节 为什么要提出值迭代网络**

我们知道，深度学习与强化学习算法结合而产生的深度强化学习算法在很多领域取得突破性进展。最早引起大家注意的是deepmind团队利用DQN算法挑战雅达利游戏，其得分竟然超过了专业人类玩家。这也直接导致了google不惜以重金收购deepmind团队，该成果于2015年还发表在了《Nature》上面，从此学者们纷纷入坑深度强化学习。回过头来，我们看看，DQN到底是什么东西。

我们从以下几个角度对DQN算法进行理解：

**第一个角度：DQN是一个深度神经网络.**

![img](https://pic2.zhimg.com/80/v2-98cd6f21013e39c27e0b130408ef0385_hd.png)

图2.1 DQN网络

如图2.1为DQN网络，从直观上来看，DQN是一个深度神经网络，更确切地说，DQN是一个3个卷积层+2个全连接层的深度神经网络。它的输入是图像，即游戏当前的画面；输出是18个动作的概率分布（游戏手柄18个动作的概率值）。

**第二个角度：DQN的训练方法是强化学习**

DQN是一个深度神经网络，我们需要训练这个网络，以达到游戏通关的目的。何为训练？就是调整神经网络的权值。如何调整神经网络的权值呢？方法是强化学习方法。但是，强化学习方法并非是调整网络权值的唯一方法。如果，我们有足够的数据，我们完全可以利用监督学习或者模仿学习等方法对该网络权值进行学习。所以，从训练神经网络的角度来理解强化学习，强化学习算法不过是调整神经网络权值的一种方法。如何利用强化学习方法调整DQN网络的权值我们在这里不再赘述，详细请看本专栏知乎帖子

[深度强化学习系列 第一讲 DQN - 知乎专栏](https://zhuanlan.zhihu.com/p/26052182)

说了那么多，我要表达的是什么呢？

其实，我要表达的是：**深度强化学习算法中很重要的一个因素是深度神经网络，深度强化学习算法是利用强化学习的方法调优这个深度神经网络。**

如果这个深度神经网络设计地很差，就算你的强化学习算法很强大，也无法实现一个很好的效果。打个做饭的比方，中国有句话叫巧妇难为无米之炊。在深度强化学习算法中，深度神经网络就是米、就是食材，而强化学习算法则是巧妇、是厨艺。只有在一个足够好的深度神经网络的基础上，强化学习算法才能将这个网络调成一个效果很好的网络。

那么问题来了，什么是一个好的神经网络？DQN是一个好的神经网络吗？

按理说，DQN网络取得了很好的效果，应该是一个好的神经网络。但是，作者发现其实它并不是一个好的网络。因为，已经调优的深度神经网络很难泛化到其他的游戏中。也就是说，该网络并没有学到什么本质的东西。

为什么呢？

这得从网络结构来看。从DQN的网络结构我们发现，DQN其实是一个前向的多层神经网络。这种网络结构最常用的场合是识别。在现有的深度强化学习领域，大部分的研究是把这种网络结构直接拿过来表示策略。该类网络结构的特点是：输入是状态，输出是动作，也就是策略。作者称，这种策略为reactive policy（反应式策略）。其实，从字面意思来理解也很容易理解，即给一个状态，给出一个反应动作。例如DQN就是一个反应式策略。 **但是从强化学习要解决的任务来看，这种反应式策略并不是一个好的策略。**为什么？因为，强化学习通常要解决的问题是马尔科夫决策问题，而解决马尔科夫决策问题本质上是解决一个序列决策问题。也就是说当前的决策需要考虑后续的决策使得整个决策总体最优。很显然，反应式策略并不能表达后续策略对当前策略的影响。从这个意义上来说，反应式策略并不是一个好的网络结构。

那么什么样的策略网络是好的策略网络呢？作者给出的答案是：**具有规划能力的策略网络是好的策略网络。**

所谓规划就是考虑后续的回报。遗憾的是，目前大部分强化学习所用的深度网络都是反应式网络，缺少显示地规划计算。然而，即便如此，仍然有很多反应式网络很成功，比如玩雅达利游戏的DQN等。细细思考，我们会发现这些网络的成功其实要归功于训练该网络的方法——强化学习训练方法。强化学习的训练方法在训练网络时（调整网络参数时）考虑到了规划问题。然而，由于网络本身没有规划模块，因此当运用到新的环境时，大部分需要重新训练，也就是说泛化能力很差。

假如，训练策略本身就有规划模块，一旦规划模块被训练好，即使换了一个新的环境，类似的任务，具有规划模块的策略网络则可以利用已经训练好的规划模块对新的任务进行规划。由于该策略网络已经学到了该类任务的本质，因此泛化能力很强。对于具有规划模块的策略网络，训练方法变得更灵活了，不必像以前那样依赖强化学习算法。此时，我们可以利用成熟的监督学习方法，模仿学习方法。当然了，在没有数据标签时，还是需要利用强化学习的训练方法。

![img](https://pic4.zhimg.com/80/v2-49cf64dada6440e34a780aaab29315f3_hd.png)

图2.2 网格世界导航问题，目标是从起始点无障碍地到达目标位置

为了理解为什么规划是策略中重要的组成成分，作者举了一个网格世界中导航的例子。在该例子中，智能体能够看到整个网格地图，知道目标点的位置，其任务是从起始点无障碍地达到目标位置。人们所希望的是，在训练完一个场景（如图2.2左边的场景）后，在另外一个场景（如图2.2后边的场景）该策略依然有效。遗憾的是，当采用基于CNN的神经网络策略时，无法实现泛化。作者分析，该网络并没有理解行为的目标指引本质。

前面说了那么多，其实还是那句话，具有规划计算的网络策略是好的策略。也就是说将规划模块嵌入到策略网络中。接下来的问题是：如何嵌入呢？

**第2.2节 值迭代网络**

最常用的规划算法是值迭代规划算法，在本专栏的第二讲 基于模型的动态规划方法

[强化学习入门 第二讲 基于模型的动态规划方法 - 知乎专栏](https://zhuanlan.zhihu.com/p/25580624) 讲解了动态规划的思想。所谓规划，其实就是蕴含着一个优化的问题。我们在这里所用的规划基于的是贝尔曼优化原理，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%0Av%5E%2A%3D%5Cmax_aR_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%5Cin+S%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%7D%5Cupsilon%5E%2A%5Cleft%28s%27%5Cright%29%0A%5C%5D) （2.1）

基于贝尔曼优化原理，具体的实现算法用的是迭代更新，也就是值迭代算法。第二讲中已给出，在这里为了表述的方便，我们再次给出值迭代的过程：

![img](https://pic1.zhimg.com/80/v2-165d1458f90d34652dbd3d6fd7134e48_hd.png)

图2.3 值迭代算法

作者提出，将这个迭代算法嵌入到神经网络中。如何将该迭代算法嵌入到一个网络中呢？这是该论文最关键的部分。

作者发现，值迭代的计算过程跟CNN网络的传播过程很像，可以利用CNN网络来表示值迭代过程。

值迭代过程为什么跟CNN很像？

我们看如图2.3的值迭代算法。

在值迭代算法中，最关键的公式是迭代公式：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%0A%5Cupsilon_%7Bl%2B1%7D%5Cleft%28s%5Cright%29%3D%5Cunderset%7Ba%7D%7B%5Cmax%7D%5C+R_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%5Cin+S%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%5Cupsilon_l%5Cleft%28s%27%5Cright%29%7D%0A%5C%5D) （2.2）

我们可以将这个迭代公式分成两步：

第一步遍历所有的动作a得到不同动作a所对应的一个值函数更新，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%0A%5Cupsilon_%7Bl%2B1%7D%5Cleft%28s%2Ca%5Cright%29%3DR_%7Bs%7D%5E%7Ba%7D%2B%5Cgamma%5Csum_%7Bs%27%5Cin+S%7D%7BP_%7Bss%27%7D%5E%7Ba%7D%5Cupsilon_l%5Cleft%28s%27%5Cright%29%7D%0A%5C%5D) （2.3）

第二步，对动作a进行遍历，找到最大的![[公式]](https://www.zhihu.com/equation?tex=%5Cupsilon_%7Bl%2B1%7D%5Cleft%28s%2Ca%5Cright%29)，即：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%0A%5Cupsilon_%7Bl%2B1%7D%5Cleft%28s%5Cright%29%3D%5Cunderset%7Ba%7D%7B%5Cmax%7D%5Cupsilon_%7Bl%2B1%7D%5Cleft%28s%2Ca%5Cright%29%0A%5C%5D) （2.4）

熟悉卷积神经网络CNN的同学应该很清楚，值函数迭代过程中的公式（2.3）相当于CNN中的卷积操作，（2.4）式相当于池化操作。从这个意义上讲，可以将值迭代的过程用CNN网络嵌入到策略网络，使得策略网络具有规划计算的功能。跟卷积神经网络稍稍不同的是，在对值函数进行卷积操作的时候，偏移量![[公式]](https://www.zhihu.com/equation?tex=R_%7Bs%7D%5E%7Ba%7D)对应着每个像素的偏移量，从CNN的角度来看，状态转移概率![[公式]](https://www.zhihu.com/equation?tex=P_%7Bss%27%7D%5E%7Ba%7D)可以看成是卷积核，回报![[公式]](https://www.zhihu.com/equation?tex=R_%7Bs%7D%5E%7Ba%7D)可以看成是每个点对应的偏移。卷积核的个数等于动作空间的维数。值迭代网络如图2.3所示。

![img](https://pic4.zhimg.com/80/v2-adc0c27aed717cc292bdb0ee979130f7_hd.png)

图2.3 值迭代网络

为了完成值迭代网络，还需要处理两个部分，如图2.3右图中的![[公式]](https://www.zhihu.com/equation?tex=%5Cbar%7BR%7D)和![[公式]](https://www.zhihu.com/equation?tex=%5Cbar%7BV%7D)，即值迭代模块的输入和输出。值迭代模块的输入是解决如何从观测值经过加工变成值迭代模块的输入这个问题；值迭代模块的输出是解决如何将迭代得到的值函数嵌入到策略网络中。

**（1）值迭代模块的输入**

![img](https://pic1.zhimg.com/80/v2-d6e3ccf4a8c721fcd1939e8ab416b57c_hd.png)

图2.4 值迭代模块的输入

对于网格世界导航问题，输入是对系统的观测，如当前地图，目标点的位置，智能体当前的位置。作者在论文中指出，观测可以通过简单的映射将图像映射成回报图，比如目标位置对应着高回报，障碍物的区域对应着负回报。在这里，可以利用一层卷积操作来实现。

**（2）值迭代模块输出的嵌入**

![img](https://pic4.zhimg.com/80/v2-121025894d6b13a3ed81a63f221c435f_hd.png)

图2.5 值迭代模块的输出

经过值迭代网络（这里用CNN网络来实现）后，我们便得到了最优值函数。如何利用最优值函数呢？

还是从本系列第二讲出发得到答案。有了最优值函数，最优策略为：

![[公式]](https://www.zhihu.com/equation?tex=%5C%5B%0A%5Cpi%5E%2A%5Cleft%28s%5Cright%29%3Darg%5C%5Cmax_aR%5Cleft%28s%2Ca%5Cright%29%2B%5Cgamma%5Csum_%7Bs%27%7D%7BP%5Cleft%28s%27%7Cs%2Ca%5Cright%29V%5E%2A%5Cleft%28s%27%5Cright%29%7D%0A%5C%5D)

在深度学习领域，当给定的标签只与输入特征的一个自己相关时，我们称之为注意力（attention）机制，因此在值迭代模块后跟一个attention网络，最简单的attention机制就是取当前状态的邻域。

至此，我们的值迭代网络差不多就讲完了。

**2.3** **值迭代网络的训练方法**

值迭代网络既可以采用模仿学习（IL）也可以采用强化学习（RL）的方法对其训练。所谓模仿学习就是利用专家数据对网络参数进行训练。对于网格世界的导航任务，专家数据可以来自传统的规划算法，比如Dijkstra算法或A*算法。RL算法如我们之前介绍的算法。