#+TITLE:     Note: Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation
#+AUTHOR:    MoozIiSP
#+DATE:      <2018-12-25 Tue>
#+TAGS:      R-CNN {Object Detection}
#+OPTIONS:   toc:2
#+STARTUP:   indent latexpreview
#+SETUPFILE: https://fniessen.github.io/org-html-themes/setup/theme-readtheorg.setup

* About Authors
- Ross Girshick
- Jeff Donahue
- Trevor Darrell
- Jitendra Malik
- UC Berkeley and ICSI

* Content

** Introduction
在04-14年期间，视觉识别任务大多数采用SIFT和方向梯度直方图（Histogram of oriented
gradient，HOG）。SIFT和HOG方法都是块状（blockwise）方向直方图，是一种我们可以大
致与V1[fn:1]中的复合细胞相关联的特征。V1是灵长类视觉通路的第一个皮层区域。视觉中
识别过程，是一个多层次、多阶段的过程，从这些过程中得到一些用于识别的重要信息。

Fukushima的 *神经认知机* （neocognitron），是一种受到生物学启发的层次和 *平移不变性
（shift-invariant）* 的模型，用于模式识别，是在这一过程中最早的尝试。然而，神经感
知机缺少一种监督训练算法。LeCun等人，通过展示经由后向传播算法的随机梯度下降
（Stochastic gradient descent, SGD）可以训练卷积神经网络（Convolutional
neural networks, CNNs），为此来补上这个缺口。而卷积神经网络是一类可以扩展神经认知
机的模型。

虽然CNNs在 *1990s* 年代使用广泛，但是随着 *支持向量机（Support Vector Machine, SVM）*
的出现，该方法显得过时，尤其是在计算机视觉领域。在 *2012* 年，Krizhevsky等人，在
LuCun CNN的基础上利用 *RLU（Recitifying non-linearites Unit）* 和“ *dropout* ”正则化，
来展示在ILSVRC上更高的图像分类准确率，这重新激起了对CNNs的兴趣。

你说图像分类结果的性能这么好，那在多大程度上我们可以将CNN的成果普及到目标检测呢？
本文是第一个展示CNN在戏剧上引导更好的目标检测性能，并与基于简单的HOG-like特征的
系统进行比较。当然实现这个需要解决两个问题：

1. 如何使用深度网络定位目标；
2. 如何使用一小部分经过标注的数据来训练高容量模型（high-capacity mode）[fn:2][fn:3]。

*** localizing one or many objects within an image

- 定位框架化 :: 一种方法是将定位框架化为一个回归问题，不过与本文同一时期开工的
     Szegedy等人的工作表明这个策略在实践中进展不顺。

- 滑动窗口检测 :: 另一种方法是构建一个滑动窗口检测器，不过CNNs已经使用了这个方法
     至少20年，一般是在有限目标类别上，如人脸和行人。为了维持 *高空间分辨率
     （high spatial resolution）*这些CNNs一般只有两个卷积和池化层，但是，在考虑
     采用滑动窗口方法的情形下，本文的网络结构变得复杂了，有五个卷积层，并且用非
     常大的感受域（Receptive fields）（195x195像素）和步长（32x32像素）来处理输
     入图像，这使在滑动窗口范式中精确定位成为了一个开放性的技术挑战。

     相反地，如Gu等人所讨论的，我们通过在“recognition using regions”范式中操作
     来解决CNN的定位问题。测试时，本文方法对输入图像生成了大约2000类别无关的区域
     候选，使用CNN从每个候选中提取固定长度的特征向量，并使用 *类别特定线性SVMs*
     对每一个区域进行分类。我们使用 *仿射图像变形（affine image warping）* 技术，
     计算来自每个区域候选的固定大小的CNN输入，忽略区域的形状。图1展示了本文方法
     的概要，并强调了本文的一些结果。因此，本文系统将区域候选与CNNs相结合，并取
     名为R-CNN（Regions with CNN features）。

*** training a high-capacity model with only a small quantity of annotated detection data
就第二问题，所面临的挑战有：
1. 标记的数据稀缺；
2. 当前，没有足够的数据来训练一个大规模CNN。

解决这个问题的传统方案使用无监督的预训练，其次进行有监督的微调。本文的第二个主要
贡献是展示对大型辅助数据集进行有监督的预训练，其次对小数据集进行特定领域的微调。
当数据稀缺的时候，对学习高容量CNNs，这是一个有效的范式。在本文实验中，对检测进行
微调可以将平均准确率提高到8个百分点。经过微调之后，在居于VOC2010数据集上，本文系
统相较于高度调整的基于HOG的DPM（Deformable part model）的33%平均准确率，实现了
54%的平均准确率。

本文系统也相当高效。只有特定类的计算是一个合理程度上小的矩阵向量积和贪心的非极大
抑制(non-maximum suppression)。这种计算属性来自于所有类别共享的特征，并且与先前
使用的区域特征相比也是低两个数量级维度（cf. [32]）。

*HOG-like* 特征的一项优点就是它们的简单：很容易理解它们所携带的信息（虽然[34]说
我们的直觉可能并不可靠。我们可以洞察CNN所习得的表征吗？或许具有超过0.54亿的稠密
连接层是关键所在？并不是。我们发现CNN中94%的参数都可以被移除，与之相对应的代价也
只是识别准确率有平均程度的下降。相反，通过探查网络中的单元，我们发现卷积层学习到
了各式各样丰富的特征（如图3）。

理解本文方法中的失败模式也是改进这个方法的关键，所以我们也对使用Hoiem等人的检测
分析工具得到的结果进行汇报。作为该分析的直接结果，我们证明了简单的边界框回归方法
显着减少了错误定位，这是主要的错误模式。

在开发技术细节之前，我们注意到因为R-CNN对区域的操作，所以很自然地将它扩展到语义
分割任务。通过微小的调整，我们也在PASCAL VOC分割任务上实现了最前沿的结果，基于
VOC2011测试集取得了47.9%的平均分割准确率。

** R-CNN目标检测
我们的目标检测系统由三个模块组成：
1. 第一个生成类别无关区域候选，这些候选（proposals）定义了对检测器可用的一组候选
   （candidate）检测。
2. 第二个模块是大规模卷积神经网络，从每个区域提取固定长度的特征向量。
3. 第三个模块是一组特定类别的线性SVMs。

*** 模型设计

- 候选区域 :: 最近很多论文都提供了生成类别无关区域候选的方法。例如，objectness
     [1]，selectivesearch [32]，category-independent object proposals [11]，
     constrained parametricmin-cuts (CPMC) [5]，multi-scale combinatorial
     grouping [3]，以及Cireşan等人通过将CNN应用到规则间隔的方形作物来检测有丝分
     裂细胞，这是候选区域的一种特殊案例。然而对于特殊候选区域方法，R-CNN是不可知
     的，我们使用选择搜索算法（selective search）来与先前的检测工作[32, 35]做一
     个对照比较（controlled comparison）。

- 特征提取 :: 我们使用由Keizhevsky等人描述的基于Caffe实现的CNN，从每个候选区域中
     提取4096-维度的特征向量。通过五卷积层和两全连接层，向前传播一个减去均差
     （mean-subracted）的227x227彩色图像来计算。更多的网络结构细节参见[21,22]。

     为了对每个候选区域计算特征，我们必须先将该区域的图像数据转换成一种与CNN相互
     兼容的形式（Krizhevsky等人的CNN结构要求固定227x227像素大小的输入）。在任意
     形状区域的多种可能的转变中，我们选择最简单的。不管候选区域的大小和纵横比如
     何，我们将环绕它的紧凑边界盒中的所有像素变形到所需要的大小。

     在变形之前，我们扩展紧凑边界盒，以至于在变形尺寸上，围绕原始盒的变形图像的
     上下文，正好有p个像素（我们使用\(p=16\)）。 图2展示了变形训练区域的随机抽样。
     补充材料讨论了变形的其他方法。

*** 测试时间检测
我们在测试图像上使用选择搜索算法，来提取大约2000候选区域（本文所有实验使用选择搜
索算法的“fast mode”）。我们对每一个区域进行变形，并通过CNN前向传播它，以便读取
所需层的特征。那么，对于每一个类别，我们使用为该类训练的SVM，对提取的特征向量进
行计分。给出图像中所有已计分的区域，我们分别对每一个类别运用贪心非极大抑制
（greedy non-maximum suppression），如果具有交叉结合（IoU）重叠且选定区域的计分
相比学习阈值更高，则拒绝该区域。

**** 运行时分析
两个属性可以使检测高效：
1. 所有CNN参数在所有类别中都是共享的；
2. 当与其他常见方法，诸如视觉字包（ *bag-of-visual-word* ）编码的空间金字塔，相
   比较时，CNN计算的特征向量都是低维的。例如，在UVA检测系统中所使用的特征比我们
   的要大两个数量级（360k vs. 4k-dimensional）。

共享的结果就是计算候选区域和特征的时间消耗（在GPU上的速度为13s/image，CPU为
53s/image）都被分摊到所有类别上。只有特定类的计算是在特征、SVM权重和非极大抑制之
间的点积。实践中，图像的所有点积都被分批到单个矩阵-矩阵积（matrix-matrix product）
中。这个特征矩阵一般是\(2000 \times 4096\)大小，并且SVM的权重矩阵是\(4096 \times
N\)大小，这里的N
代表类别的数量。

本分析表明R-CNN在不求助如hashing等的近似技术（approximate techniques）的帮助下，
可以扩展到数千个目标类别。即时有100k类别，在现代多核CPU上，矩阵乘积的结果也只是
花费10s。这个高效的方法不仅仅是使用候选区域和特征共享的结果。由于高纬度特征，UVA
系统会慢两个数量级，同时需要134GB内存才能存储1000k线性预测器，而我们的低纬度特征
仅需要1.5GB。

将R-CNN与Dean等人最近基于使用DPM和hashing的可扩展检测[8]的工作进行对比也很有趣。
他们报告说，当引入10k牵引器（distractor）类别时，每张图像的运行时间为5分钟，VOC
2007的平均准确率约为16%。使用我们的方法，10k检测器可以在CPU上运行大约一分钟，因
为没有近似值，mAP将保持在59％（第3.2节）。

*** 训练

- 有监督预训练 :: 我们在具有图像级标注（即，没有边界框标签）的大型辅助数据集
     （ILSVRC 2012）上有条理地预训练CNN。使用开源Caffe CNN库[21]进行预训练。简而
     言之，我们的CNN几乎与Krizhevsky等人[22]的表现相符，并在ILSVRC 2012验证集上获
     得高出2.2个百分点的top-1错误率。这种差异是因为简化了训练过程。

- 特定领域微调 :: 为了使我们的CNN适应新任务（检测）和新领域（变形的VOC窗口），我
     们继续使用来自VOC的变形的候选区域，对CNN参数进行随机梯度下降（SGD）训练。除
     了用随机初始化的21路分类层（对于20个VOC类别加背景）替换CNN的ImageNet特定的
     1000路分类层，CNN架构保持不变。我们将所有大于等于0.5 IoU并带有基本事实
     （ground-truth）盒的候选区域，看作盒子类别的正例，剩余的则作为反例。我们以
     0.001的学习率（初始预训练率的1/10）开始SGD，这允许微调进行而不破坏初始化。
     在每次SGD迭代中，我们统一采样32个正案例窗口（在所有类别上）和96个背景窗口，
     构建一个128大小的小批量（mini-batch）。我们将采样偏向正窗口，因为它们与背景
     相比非常罕见。

- 目标类别分类器 :: 考虑训练二元分类器来检测汽车。很明显，紧紧包围汽车的图像区域
     应该是一个正例。同样，与汽车无关的背景区域应该是一个反例。而不太清楚的是如
     何去标记与汽车部分重叠的区域。我们使用IoU重叠阈值解决此问题，低于该阈值则将
     区域定义为负例。通过在验证集上对\(\{0,0.1,\cdots,0.5\}\)的网格搜索，选择重
     叠阈值。我们发现仔细选择此阈值 *非常重要* 。将其设置为0.5，如[32]所示，平均
     准确率降低了5个点。同样，将其设置为0会使平均准确率降低4个点。正例被简单地定
     义为每个类的基本事实（ground-truth）边界盒。

一旦提取了特征并应用了训练标签，我们就会优化每个类的线性SVM。由于训练数据太大以
此无法存入内存，我们采用标准的hard negative mining方法[14,30]。Hard negative
mining收敛很快，实践中mAP在一次性传递所有图像后停止增加。

在补充材料中，我们讨论为什么在微调与SVM训练中对正例和反例的定义不同。 _我们还讨论了为什么训练检测分类器是有必要的，而不是简单地使用来自微调CNN的最终层（fc8）的输出。_

*** PASCAL VOC 2010-12 结果
遵循PASCAL VOC最佳实践[12]，我们检验了VOC 2007数据集的所有设计决策和超参数（第
3.2节）。对于VOC 2010-12数据集的最终结果，我们对基于VOC 2012训练的CNN进行了微调，
并优化了我们在VOC2012 trainval上的检测SVM。我们仅针对两种主要算法变体（使用和不
使用边界盒回归），提交一次测试结果给评估服务器。

表1展示了VOC 2010的完整结果。我们将我们的方法与四个强基准的方法进行了对比，包括
SegDPM [15]，它将DPM探测器与语义分割系统的输出相结合[4]，并使用额外的检测器间上
下文（inter-detection）和图像分类器重新校正。最密切相关的对比是Uijlings等人的UVA
系统[32]，因为我们的系统使用一样的候选区域算法。为了对区域进行分类，他们的方法构
建了一个四级空间金字塔，并用密集采样的SIFT，Extended OpponentSIFT和RGB-SIFT描述
符填充它，每个向量用4000字的码本量化。使用直方图交叉核（histogram intersection
kernel）SVM执行分类。与其多特征，非线性核SVM方法相比，我们实现了mAP的大幅提升，
从35.1％提高到53.7％，同时也更快（第2.2节）。我们的方法在VOC 2011/12测试中实现了
类似的性能（53.3% mAP）。

** 可视化，解剖和错误模式

*** 特征可视化
第一层滤波器可以直接可视化，并且易于理解[22]。它们捕捉定向边缘和相反颜色
（opponent colors）。了解后续层更具挑战性。 Zeiler和Fergus在[36]中提出了一种视觉
上具有吸引力的反卷积方法。我们提出了一种简单（和互补）的非参数方法，可直接显示网
络学到的内容。

我们的想法是在网络中挑出一个特定的单元（特征）并使用它就好像它本身就是一个物体检
测器。也就是说，我们在大量保留（held-out）候选区域（约1000万）上计算单元的激活函
数，从最高到最低激活函数（activation?）对候选进行排序，执行非极大抑制，然后显示
得分最高的区域。我们的方法通过准确显示它所触发的输入，让所选单元“自己说话”。我
们避免为了查看不同的视觉模式求平均值，并深入了解由单元计算的不变性。

我们从pool5池化层可视化单元，这是网络的第五和最后的卷积层的最大输出（maxpooled
output）。pool5特征图是\(6 \times 6 \times 256 = 9216-dimensional\)。忽略边界效
应，每个pool5单元在原始\(227 \times 227\)像素输入中都具有\(195 \times 195\)像素
的感受域。中心pool5池化层几乎具有全局视野，而靠近边缘的单元具有较小的，被剪裁的
支持（视野）。

图3中的每一行展示了我们在VOC 2007 trainval上进行微调的CNN中的pool5单元的前16次激
活函数。 256个功能独特的单元中的六个被可视化（更多见补充材料）。选择这些单元以显
示网络所学习的代表性样本。在第二行中，我们看到一个单元，触发了狗脸和点阵列。对应
于第三行的单位是红色斑点检测器。 还有用于人脸的检测器和更抽象的图案的检测器，抽
象的图像诸如，文本和带有窗户的三角形结构。该网络似乎学习了一种表示，该表示将少量
的类调整特征（class-tuned features）与形状，纹理，颜色和材料属性的分布式表示结合
在一起。随后的完全连接层fc6具有对这些丰富特征的大量组合进行建模的能力。

*** 剖析

- Performance layer-by-layer, without fine-tuning :: 为了理解哪些层对于检测性能
     至关重要，我们分析了基于VOC 2007数据集的CNN最后三层中每个层的结果。pool5池
     化层在3.1节中简要描述。最后两层总结如下。

     fc6全连接层完全连接到pool5池化层。为了计算特征，它将\(4096 \times 9216\)权
     重矩阵乘以pool5特征图（重塑为9216维向量），然后加上偏置向量。该中间向量是分
     量半波整流（\(x \leftarrow max(0,x)\)）。fc7全连接层是网络的最后一层。它通
     过将由fc6计算的特征乘以\(4096 \times 4096\)权重矩阵，并也是类似加上偏置向量，
     应用半波整流来实现。

     我们先来看来自未基于PASCAL微调的CNN的结果，即所有CNN参数仅在ILSVRC 2012上进
     行了预训练。逐层地分析性能（表2第1-3行）发现fc7的特征比fc6的特征更糟糕。这
     意味着可以在不降低mAP的情况下移除29％或约1680万个CNN参数。更令人惊讶的是，
     同时移除fc7和fc6也会产生相当好的结果，即使仅使用6％的CNN参数计算pool5特征。
     大部分CNN的代表性力量来自其卷积层，而不是来自更大的稠密连接层。该发现认为，
     在HOG的意义上，通过只使用CNN的卷积层计算任意大小的图像，具有潜在效用。这种
     表示可以在pool5特征之上实现滑动窗口检测器，包括DPM。
- Performance layer-by-layer, with fine-tuning :: 我们现在看看来自基于VOC 2017
     trainval微调参数的CNN的结果。改进正在进行（表2第4-6行）：微调将mAP提高8.0个
     百分点至54.2%。fc6和fc7的微调提升比pool5大得多，这表明从ImageNet学到的pool5
     特性是通用的，并且大多数改进都是从位于顶层，学习特定领域的非线性分类器获得
     的。
- Comparison to recent feature learning methods :: 在PASCAL VOC检测中已经尝试了
     相对较少的特征学习方法。我们看看最近基于DPM的两种方法。作为参考，我们还包括
     标准的基于HOG的DPM的结果[17]。

     1. 第一个DPM特征学习方法——DPM ST[25]——利用“ *草图标记（sketch token）* ”概率
        的直方图增强HOG特征。直观上，草图标记是通过图像区域（patch）中心的轮廓的
        紧密分布。通过 *随机森林* 在每个像素上计算草图标记的概率，该随机森
        林被训练为将\(35 \times 35\)像素区域分类为150个草图标记或背景之一。

     2. 第二种方法——DPM HSC [27]——用 *稀疏编码直方图* （HSC）替换HOG。为了计算
        HSC，使用\(100 \times 7 \times 7\)像素（灰度）原子的学习字典在每个像素上
        求解稀疏代码激活函数。由此产生的激活以三种方式（全波和两半波（both
        half-waves））进行修正，空间池化，单位\(l_2\)归一化，然后进行能级（power）
        转换（\(x \leftarrow sign(x) {\vert x \vert}^\alpha\)）。

所有R-CNN变体都强烈优于三个DPM基准（表2的第8-10行），包括使用特征学习的两个变体。
与仅使用HOG特征的DPM的最新版本相比，我们的mAP高出20多个百分点：54.2%对比33.7%——
相对提升61％。HOG和草图标记的组合比单独的HOG提高2.5 mAP点，而HSC比HOG提高4
mAP点（内部与他们私有的DPM基准进行比较——都使用非公开实现的DPM，其表现不如开源版
本[17]）。这些方法分别实现29.1％和34.3％的mAP。

*** 误差分析检测
我们使用了来自Hoiem等人[20]出色的检测分析工具，为揭示我们方法的误差模式（error
mode），了解微调是如何改变它们，以及了解与DPM相比我们的误差类型如何。分析工具的
完整摘要超出了本文的范围，我们鼓励读者参考[20]以了解一些更精细的细节（例如“标准
化AP”）。由于分析最好并入相关图的上下文中，因此我们在图4和图5的标题内进行讨论。

*** 边界盒回归
基于误差分析，我们实现了一种简单的方法来减少定位错误。受DPM[14]中使用的边界盒回
归的启发，我们训练线性回归模型，以根据选择性搜索区域候选的pool5特征预测新的检测
窗口。补充材料中提供了完整的详细信息。表1，表2和图4中的结果表明，这种简单的方法
修复了大量错误定位的检测，将mAP提高了3到4个点。

** 语义分割
区域分类是语义分割中的标准技术，允许我们轻松地将R-CNN应用于PASCAL VOC分割挑战。
为了便于与当前领先的语义分割系统（称为“ *二阶池化（second-order pooling）* ”的
O_{2}P）进行直接比较[4]，我们在他们的开源框架上工作。O2P使用 *CPMC* 为每个图像生
成150个区域候选，然后使用 *支持向量回归（SVR）* ，预测每一类别的每个区域的质量。他
们的方法的高性能是由于CPMC区域的质量和多种特征类型的强大二阶池化（SIFT和 *LBP* 的丰
富变体）。我们还注意到Farabet等人[13]最近使用CNN作为多尺度每像素分类器
（multi-scale per-pixel classifier），在几个密集场景标记数据集（不包括PASCAL）上
展示了不错的效果。

我们按照[2,4]并扩展PASCAL分割训练集，以包括Hariharan等人[19]提供的额外注释。基于
VOC 2011验证集，对设计决策和超参数进行了交叉验证。最终测试结果仅评估一次。

- CNN features for segmentation :: 我们评估了计算CPMC区域特征的三种策略，所有这
     些策略都是通过将区域周围的矩形窗口变形到227×227来进行的。第一种策略（full）
     忽略了区域的形状，并直接对变形窗口计算CNN特征，就像我们检测的那样。然而，这
     些功能忽略了该区域的非矩形形状。两个区域可能具有非常相似的边界盒，而重叠非
     常少。因此，第二个策略（fg）仅在区域的前景掩模上计算CNN特征。我们用 *均值输
     入（mean input）* 替换背景，以便在减去均值（mean subtraction）后背景区域为零。
     第三个策略（full+fg）简单地整合full和fg功能; 我们的实验验证了它们的互补性
- Results on VOC 2011 ::  表3显示了我们在VOC 2011验证集上与O_{2}P相比的结果摘要
     （有关完整的每类别结果，请参阅补充材料）。在每个特征计算策略中，全连接层fc6
     总是优于fc7，以下讨论涉及fc6特征。fg策略稍微优于full，表明掩膜（masked）区
     域形状提供更强的信号，与我们的直觉相匹配。然而，full+fg实现了47.9％的平均准
     确率，我们的最佳结果是4.2％（也略微优于O_{2}P），表明即使给出fg特征，full特
     征提供的上下文也是非常有用的。值得注意的是，在我们的full+fg特征上训练20个
     SVR在单个核心上要花费一个小时，而在O_{2}P特征上训练需要10+小时。

     在表4中，我们提供了VOC 2011测试集的结果，将我们表现最佳的方法fc6（full+fg）
     与两个强基准进行了比较。我们的方法在21个类别中的11个中实现了最高的分割准确
     率，并且最高的总分割准确率为47.9％，在各类别之间取平均值（但在任何合理的误
     差范围内可能与O_{2}P结果有联系）。通过微调可能可以实现更好的性能。

** 结论
近年来，目标检测性能停滞不前。性能最佳的系统是复杂的集合，将多个低级图像特征与来
自物体检测器和场景分类器的高级上下文相结合。本文介绍了一种简单且可扩展的物体检测
算法，与PASCAL VOC 2012上的最佳结果相比，可提供30％的相对改进。

我们通过两个见解实现了这一表现。第一种是将高容量卷积神经网络应用于自下而上的区域
候选，以便对目标进行定位和分割。第二个是在标记的训练数据稀缺时训练大型CNN的范
例。我们表明，对于具有丰富数据（图像分类）的辅助任务，通过监督来预训练网络是非常
有效的，然后针对数据稀缺（检测）的目标任务微调网络。我们推测，“监督的预训练/领
域特定的微调”范例对于各种数据稀缺的视觉问题将非常有效。

最后，我们指出通过结合使用计算机视觉和深度学习（自下而上区域候选和卷积神经网络）
的经典工具来实现这些结果是很重要的。这两者不是反对科学探究的对象，而是自然而不可
避免的合作伙伴。

* FIXME Annotations

** 滑动窗口检测
"sliding-window detector"，滑动窗口检测，所以针对于之前的那个织物图像的瑕疵检测
可以提出更进一步的改进。首先，从直觉上，感受器（现在还不知道，先借用这个名词）的
视野大小（滑窗大小）以及视野中有效信息都会影响检测效果，例如说ResNet56网络是接受
227×227像素大小的图片作为输入，那么如果所要学习的特征在图像上的占比很小是不是会
影响图像的识别呢？因为主要是在输入和标签上做个映射，那么无关的内容或者上下文会影
响到检测效果吧（大误）。

** 可视化分析工具
"detection analysis tool of Hoiem et al."，来自Hoiem的检测分析工具。

** Supervised pre-training
#+begin_quote
due to simplifications in the training process.
#+end_quote

怎么做到简化训练过程呢？

** Domain-specific fine-tuning
#+begin_quote
We bias the sampling towards positive windows because they are extremely rare
compared to background.
#+end_quote

为什么它们相对于背景是极其少见的？

** Object category classifiers
#+begin_quote
standard hard negative mining method [14, 30]. Hard negative mining converges
quickly and in practice mAP stops increasing *after only a single pass over all
images*.
#+end_quote

原文是采用这种方法来降低对内存的要求。

#+begin_quote
it's necessary to train detection classifiers rather than simply use outputs
from the final layer (fc8) of the fine-tuned CNN.
#+end_quote

参见补充材料，并且也讨论了相比微调vsSVM训练是很能定义正反的例子。

** high-level context
#+begin_quote
"low-level image features with high-level context" (Girshick et al 2014:586)
#+end_quote

低级图像特征诸如边缘balabala，那么高级上下文特征是？ (note on p.586)

* Keywords

** neocognitron
** 尺度不变特征变换（Scale-Invariant Feature Transform）
[[https://docs.opencv.org/trunk/da/df5/tutorial_py_sift_intro.html][原文来源]]，在任何尺度下拍摄到的图像都能检测到一致的关键点，而且每个被检测的特征点
都对应一个尺度因子。这是对方向不变性的一种补充。

相应的变体有：densely sampled SIFT、Extended OpponentSIFT、RGBSIFT。

** HOG
** SVM
** IoU (Intersection-over-Union)
** Spatial pyramid with bag-of-visual-word encodings
** UVA detection system
** DPM (Deformable part model)
** SegDPM
** inter-detector context
** histogram intersection kernel SVM
** class-tnned features
** half-wave rectified
** HSC (histograms of sparse codes)
** sketch token
** CPMC
** SVR
** cross-validated

* Vocabulary
- in one's own right
- speak for itself
- fire on
- in the sense of
- enable experimentation with

* Footnotes

[fn:1] V1，亦称纹状皮层（Striate cortex）。[[https://zh.wikipedia.org/wiki/%E8%A7%86%E8%A7%89%E7%9A%AE%E5%B1%82][更多信息]]见维基百科。

[fn:2] [[https://stats.stackexchange.com/questions/312424/what-is-the-capacity-of-a-machine-learning-model][What is the “capacity” of a machine learning model?]]

[fn:3] [[https://stackoverflow.com/questions/40337510/what-is-the-definition-of-high-capacity-cnn-or-high-capacity-architecture][What is the definition of “high-capacity cnn” or “high-capacity architecture”?]]
