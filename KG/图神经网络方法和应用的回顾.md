# 图神经网络方法和应用的回顾

Jie Zhou∗, Ganqu Cui∗, Zhengyan Zhang∗, Cheng Yang, Zhiyuan Liu, Maosong Sun

**摘要**：很多学习任务都需要处理包含元素间丰富关系信息的图形数据。物理系统的建模、分子指纹的学习、蛋白质界面的预测、疾病的分类等都需要模型从图形输入中学习。在文本和图像等非结构数据的学习等其他领域，对提取的结构进行推理，如句子的依赖树和图像的场景图，是一个重要的研究课题，也需要图形推理模型。图神经网络(GNNs)是一种连接主义模型，它通过在图的节点之间传递消息来获取图的依赖性。与标准神经网络不同的是，图神经网络保留了一种状态，这种状态可以用任意深度表示邻居的信息。虽然原始图神经网络很难训练成定点，但是最近在网络结构、优化技术和并行计算方面的进展使得利用它们进行成功的学习成为可能。近年来，基于图卷积网络(GCN)和门控图神经网络(GGNN)的系统在上述许多任务上都表现出了突破性的性能。在这次调查中，我们对现有的图形神经网络模型进行了详细的回顾，系统地对其应用进行了分类，并提出了四个有待进一步研究的问题。

## 1 介绍

图形是对一组对象(节点)及其关系(边)建模的一种数据结构。最近,研究分析图表和机器学习已经收到越来越多的关注,因为图形的表达能力,即图可以作为外延大量系统在各个领域包括社会科学(社交网络)[33],[46],自然科学(物理系统[5],[77]和蛋白质相互作用网络[27]),知识图表(32)和许多其他研究领域[23]。图分析作为机器学习中一种独特的非欧几里得数据结构，其研究重点是节点分类、链路预测和聚类。图神经网络是一种基于图域的深度学习方法。由于其令人信服的性能和较高的可解释性，GNN近年来已成为一种广泛应用的图形分析方法。在下面的段落中，我们将阐述图形神经网络的基本动机。

GNNs的第一个动机来源于卷积神经网络(CNNs)[50]。CNNs具有提取多尺度局部空间特征并将其组合成高表达性表征的能力，这使得几乎所有的机器学习领域都出现了突破，开启了深度学习[49]的新时代。然而，CNNs只能对图像(2D网格)、文本(1D序列)等常规欧几里德数据进行操作，而这些数据结构可视为图形的实例。随着我们对CNNs和图形的深入研究，我们发现了CNNs的关键:本地连接、共享权重和多层[49]的使用。这些对于解决图域问题也很重要，因为图是最典型的局部连通结构。2)与传统的谱图理论[20]相比，共享权降低了计算成本。3)多层结构是处理分层模式的关键，它捕捉不同大小的特征。因此，很容易想到将cnn推广到图形。但是，如1所示，局部卷积滤波器和池运算很难定义，这阻碍了CNN从欧几里得域向非欧几里得域的转换。

> ![1545704679065](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545704679065.png)
>
> 图1所示。左:欧几里得空间中的图像。右:非欧几里德空间中的图

另一个动机来自于图的嵌入，它学习在低维向量中表示图的节点、边或子图。在图论分析领域，传统的机器学习方法通常依赖于人工工程的特征，但由于其不灵活性和高成本而受到限制。DeepWalk[70]是第一个基于表示学习的图形嵌入方法，它遵循表示学习的思想和单词嵌入的成功之处，将节点作为单词，在图形上生成的随机游动作为句子，然后对它们应用SkipGram模型[62]。node2vec[31]、LINE[89]、SDNE[96]等类似方法也取得了突破。然而，这些方法可能在计算上很昂贵，而且对于大型图来说也不是最优的。gnn就是为了解决这些问题而设计的。

基于网络神经网络和图形嵌入技术，提出了基于网络神经网络的图形神经网络(GNNs)，用于从图形结构中综合信息。因此，它们可以对由元素及其依赖性组成的输入和/或输出建模。此外，图神经网络可以同时用RNN核对图上的扩散过程进行建模。

在接下来的部分中，我们解释了为什么图形神经网络值得研究的基本原因。首先，标准的神经网络如CNNs和RNNs不能很好地处理图形输入，因为它们按照特定的顺序将节点的特征进行叠加。然而，在图中并没有节点的自然顺序。为了完整的呈现一个图，我们应该遍历所有可能的顺序作为模型的输入，比如CNNs和RNNs，这在计算时是非常冗余的。为了解决这个问题，gnn分别在每个节点上传播，忽略节点的输入顺序。也就是说，GNNs的输出对于节点的输入顺序是不变的。其次，图中的一条边表示两个节点之间依赖关系的信息。在标准神经网络中，依赖信息仅仅被视为节点的特征。但是，GNNs可以根据图结构进行传播，而不是将其作为特性的一部分。一般来说，gnn通过对节点邻域状态的加权和来更新节点的隐藏状态。第三，推理是高水平人工智能的一个非常重要的研究课题，人类大脑的推理过程几乎是基于从日常经验中提取出来的图形。标准的神经网络已经显示出通过学习数据的分布来生成合成图像和文档的能力，但仍然不能从大型实验数据中学习推理图。然而，GNNs探索从非结构化数据(如场景图片和故事文档)生成图形，这可以为进一步的高级AI提供强大的神经模型。近年来，gnn在文本分类[2]、[25]、神经机翻译[4]、[7]、关系提取[63]、[69]、图像分类[28]等方面得到了广泛的应用[99]。

目前对图神经网络的研究有较全面的综述。[80]给出了早期图神经网络方法的形式化定义。和[79]展示了图神经网络的近似性质和计算能力。[64]提出了一个统一的框架，MoNet，将CNN架构推广到非欧几里得域(图和流形)，该框架可以推广图[2]、[46]上的几种光谱方法，以及流形[10]上的一些模型，[61]。[11]对几何深度学习进行了全面的回顾，提出了几何深度学习存在的问题、难点和解决方法,用途及未来发展方向。[64]，[11]侧重于将卷积推广到图形或流形，但本文只关注图形上定义的问题，还研究了图形神经网络中使用的其他机制，如门机制、注意机制和跳跃连接。[30]提出的消息传递神经网络(MPNN)可以推广几种图神经网络和图卷积网络方法。给出了信息传递神经网络的定义，并演示了其在量子化学中的应用。[98]提出了非局部神经网络(non-local neural network, NLNN)，它统一了几种自我关注的方式。然而，该模型并没有在原始论文的图表上明确定义。针对具体的应用领域，[30]和[98]只给出了如何使用其框架泛化其他模型的例子，并没有对其他的图神经网络模型进行综述。[6]提出了图形网络(GN)框架。该框架具有较强的推广其他模型的能力，其关系归纳偏差促进了组合推广，这被认为是人工智能的重中之重。但是，[6]是部分意见书，部分综述，部分统一，仅对应用进行了粗略分类。本文对不同的图神经网络模型进行了全面的综述，并对其应用进行了系统的分类。

综上所述，本文对图神经网络进行了广泛的研究，贡献如下。

- 我们对现有的图神经网络模型进行了详细的回顾。我们介绍了原始的模型、它的变体和一些通用框架。我们研究了这一领域的各种模型，并提供了一个统一的表示来表示不同模型中的不同传播步骤。通过识别相应的聚合器和更新器，可以很容易地使用我们的表示来区分不同的模型。
- 我们系统地对应用程序进行分类，并将应用程序划分为结构化场景、非结构化场景和其他场景。我们介绍了几种主要的应用程序及其在不同场景中的相应方法。
- 我们提出了四个有待进一步研究的问题。图神经网络存在过度平滑和缩放问题。目前还没有有效的方法来处理动态图形和建模非结构化的感觉数据。我们对每个问题进行了深入的分析，并提出了未来的研究方向。

本调查的其余部分组织如下。在第2节中，我们介绍了图神经网络族中的各种模型。首先介绍了原始框架及其局限性。然后我们提出了它的变体，试图释放这些限制。最后介绍了近年来提出的几种通用框架。在第3节，我们将介绍图神经网络在结构场景、非结构场景和其他场景中的几个主要应用。在第4节中，我们提出了图神经网络的四个开放问题以及未来的几个研究方向。最后，我们在第五节中总结了调查结果。

## 2 模型

图神经网络是非欧几里得结构上有用的工具，文献中提出了各种方法来改进模型的性能。

在2.1节中，我们描述了[80]中提出的原始图神经网络。我们还列出了原始GNN在表示能力和训练效率方面的局限性。在2.2节中，我们介绍了几种不同的图神经网络，旨在释放这些限制。这些变体操作不同类型的图形，利用不同的传播函数和先进的训练方法。在2.3节中，我们提出了三个通用框架，它们可以概括和扩展几个工作线。其中，消息传递神经网络(MPNN)[30]融合了各种图神经网络和图卷积网络方法;非局部神经网络(NLNN)[98]结合了几种自我注意类型的方法。而图神经网络(GN)[6]可以泛化本文提到的几乎所有的图神经网络变体。

在进一步进入不同的部分之前，我们给出了将在整篇论文中使用的标记。表示法的详细描述见表1。

### 2.1图神经网络

图神经网络(GNN)的概念最早是在[80]中提出的，它扩展了现有的神经网络来处理图域中表示的数据。在图中，每个节点都是由其特征和相关节点自然定义的。GNN的目标是学习一种包含每个节点邻域信息的状态嵌入hv R s。状态嵌入hv是节点v的一个s维向量，可用于产生节点标签等输出ov。设f为参数函数，称为局部转移函数，在所有节点间共享，并根据输入邻域更新节点状态。设g为描述输出如何产生的本地输出函数。然后，hv和ov的定义如下

![1545705913918](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545705913918.png)

其中xv、xco[v]、hne[v]、xne[v]分别是v的特征、v的边缘特征、状态特征和v邻域节点的特征。

令H、O、X、XN为分别叠加所有状态、所有输出、所有特征和所有节点特征构成的向量。然后我们有一个紧凑的形式为:![1545706217578](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545706217578.png)

其中，全局转换函数F和全局输出函数G分别是图中所有节点的F和G的堆叠版本。H的值是Eq. 3的不动点，且唯一的定义是假设F是一个收缩映射。

在巴拿赫不动点定理[44]的建议下，GNN采用以下经典迭代方法计算状态。

![1545706460457](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545706460457.png)

式中，Ht为H的第t次迭代，对于任意初值H(0)，动力系统Eq. 5指数收敛速度快于Eq. 3的解。注意，f和g中描述的计算可以解释为前馈神经网络。

当我们有了GNN的框架后，下一个问题是如何学习f和g的参数。以目标信息(某一特定节点的tv)作为监督，损失可以写成![1545706516539](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545706516539.png)

其中p是被监督的节点数。该学习算法基于梯度下降策略，由以下步骤组成。

- 状态$h^t_v$由Eq. 1迭代更新直到时刻t，它们接近公式3的不动点解:$h (t) \approx h $
- 权值W的梯度是由损失计算出来的。
- 权值W根据上一步计算的梯度进行更新。

**局限**：虽然实验结果表明，GNN是一种功能强大的结构化数据建模体系结构，但是原有的GNN仍然存在一些局限性。首先，对于不动点迭代更新节点的隐藏状态是低效的。如果放松不动点的假设，可以设计一个多层的GNN来得到节点及其邻域的稳定表示。其次，GNN在迭代中使用相同的参数，而大多数神经网络在不同的层中使用不同的参数，这是一种分层特征提取方法。此外，节点隐藏状态的更新是一个连续的过程，可以从像GRU和LSTM这样的RNN内核中获益。第三，边缘也有一些原始GNN无法模拟的信息特征。例如，知识图中的边具有关系的类型，不同边之间的消息传播应该根据它们的类型而有所不同。此外，如何学习边缘的隐藏状态也是一个重要的问题。最后，如果我们把焦点放在节点的表示上而不是图形上，就不适合使用不动点，因为在不动点上的表示的分布在数值上很平滑，区分每个节点的信息量也比较小。

![1545707142147](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545707142147.png)

### 2.2 图神经网络的变体

在这个小节中，我们提出了图神经网络的几种变体。第2.2.1节关注在不同图形类型上操作的变体。这些变体扩展了原始模型的表示能力。第2.2.2节列出了传播步骤上的几个修改(卷积、门机制、注意机制和跳过连接)，这些模型可以更好地学习表示。第2.2.3节介绍了使用高级培训方法的变体，提高了培训效率。图2概括了图神经网络的不同变体。

#### 2.2.1图类型

有向图图的第一个变体是有向图。无向边可以看作是两个有向边，说明了两个节点之间的关系。然而，有向边比无向边能带来更多的信息。例如，在一个知识图中，边从head实体开始到tail实体结束，head实体是tail实体的父类，这就意味着我们应该区别对待父类和子类的信息传播过程。ADGPM[41]采用Wp和Wc两种权重矩阵，结合更精确的结构信息。传播规律如下:

![1545707768534](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545707768534.png)

其中d1pap、d1cac分别为父节点和子节点的归一化邻接矩阵。

异构图图的第二种变体是异构图，其中有几种节点。处理异构图的最简单方法是将每个节点的类型转换为与原始特征连接的一个热门特征向量。此外，GraphInception[111]在异构图的传播中引入了元路径的概念。使用metapath，我们可以根据邻居的节点类型和距离对其进行分组。对于每个相邻组，GraphInception将其作为同构图中的子图来进行传播，并将来自不同同构图的传播结果连接起来，以进行集合节点表示。

边缘信息图：在图形的最后一种变体中，每条边都有自己的信息，比如边的权值或者边的类型。有两种方法来处理这类图:首先,我们可以将图转换成两偶图原边也成为节点和一个原边分成两个新的边缘这意味着有两个边缘之间的边缘节点和开始/结束节点。编码器的g2[7]使用下面的邻居的聚合函数:![1545707995802](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545707995802.png)

其中Wr和br为不同类型边(关系)的传播参数。其次，我们可以采用不同的权矩阵在不同的边缘上进行传播。当关系数量很大时，r-GCN[81]引入两种正则化方法来减少关系数量建模的参数数量:基-和块-对角线-分解。根据基分解，每个Wr定义如下:

![1545708170446](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545708170446.png)

即，R - gcn是Vb R din dout与系数arb的基变换的线性组合，使得系数仅依赖于R。在块对角分解中，R - gcn通过对一组低维矩阵的直接和来定义每个Wr，这些低维矩阵比第一个低维矩阵需要更多的参数。

#### 2.2.2 传播类型

传播步骤和输出步骤对于获取节点(或边)的隐藏状态至关重要。如下表所示，在传播步骤中，原图神经网络模型有几个主要的修改，而研究人员通常在输出步骤中遵循一个简单的前馈神经网络设置。GNN的不同变体比较见表2。这些变体使用不同的聚合器从每个节点的邻居和特定的更新器收集信息，以更新节点的隐藏状态。

卷积:将卷积推广到图域的兴趣越来越大。这方面的进展通常分为光谱方法和非光谱方法。

谱方法是用图的谱表示来工作的。[12]提出了光谱网络。通过计算图像拉普拉斯变换的特征分解，在傅立叶域中定义了卷积运算。操作可以被定义为一个信号的乘法x R N为每个节点(标量)和一个过滤器gθ=诊断接头(θ)参数化θ ∈ RN:

![1545708380116](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545708380116.png)

U是矩阵的特征向量归一化图像的拉普拉斯算子L = D公元1 2 1 2 = UΛUT(D度矩阵和图的邻接矩阵),一个对角矩阵的特征值Λ。

这种操作会导致潜在的密集计算和非空间局部化过滤器。[38]试图通过引入具有平滑系数的参数化，使谱滤波器空间局域化。[34]表明gθ(Λ)可以用一个截断近似扩张的切比雪夫多项式Tk k阶(x)。因此，操作是：

![1545720042712](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545720042712.png)

L˜=2λmax L−in.λmax表示L的最大特征值，θ∈R，K现在是θ∈系数的向量。Chebyshev多项式定义为Tk(X)=2xTk−1(X)−tk−2(X)，其中T0(X) =1和T1(X)=x。在Laplacian中，由于它是一个KTH阶多项式，所以它是K-局部化的。[25]利用这个k-局部卷积定义了一个卷积神经网络。 可以消除计算拉普拉斯特征向量的需要的工作。

[46]将分层卷积运算限制为K = 1，以减轻对于节点度分布非常宽的图的局部邻域结构的过拟合问题。它进一步接近λmax 2和方程简化为:

![1545720491991](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545720491991.png)

有两个自由参数θ0 0和θ0 1。在用θ=θ0 0=−θ0 1约束参数之后，我们可以获得以下表达式：

![1545720536163](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545720536163.png)

注意，叠加这个算子可能会导致数值不稳定和梯度爆炸/消失，[46]引入了重整化技巧:在+ d12 ad12 d12 a12, A = A + IN和D ii = pja ij。最后，[46]将该定义推广到信号xrnc，特征映射采用C输入通道和F滤波器，如下所示:

![1545720970323](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545720970323.png)

ΘR C F是一个矩阵滤波器的参数和Z N R F是卷积信号矩阵。

然而，在上述所有光谱方法中，学习滤波器依赖于拉普拉斯特征基，而拉普拉斯特征基依赖于图的结构，即基于特定结构训练的模型不能直接应用于具有不同结构的图。

非谱方法直接在图上定义卷积，操作空间上的近邻。非谱方法的主要挑战是定义具有不同大小邻域的卷积运算和保持CNNs的局部方差。

对于不同程度的节点，[26]使用不同的权重矩阵:

![1545721085634](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545721085634.png)

其中WNv L为第L层Nv度节点的权值矩阵，该方法的主要缺点是不能应用于节点度较大的大规模图。

[2]提出了扩散卷积神经网络(DCNNs)。转换矩阵用于定义DCNN中节点的邻域。对于节点分类，它有:

![1545721234306](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545721234306.png)

其中X是输入特征的N×F张量(N是节点数，F是特征数)。P∗是一个N×K×N张量，它包含矩阵P的幂级数{P，P2，…，PK}，P是 图邻接矩阵A中的度归一化转移矩阵A，将每个实体转化为扩散卷积表示，即由图di的K阶跃点定义的K×F矩阵。 F特征的模糊。然后用一个K×F权矩阵和一个非线性激活函数f来定义它。最后，H(N×K×F)表示每个节点的扩散表示。 在图中。

在图分类方面，DCNN只取节点表示的平均值

![1545721278741](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545721278741.png)

这里是一个由1组成的向量。DCNN也可以应用于边缘分类任务，这需要将边缘转换为节点，并增加邻接矩阵。

[66]提取每个节点恰好k个节点的邻域，并将其标准化。然后归一化邻域作为卷积运算的接受域。

[64]提出了一种基于非欧几里得域的空间域模型(MoNet)，该模型可以推广之前的几种技术。流形上的测地线CNN (GCNN)[61]和各向异性CNN (ACNN)[10]或图形上的GCN[46]和DCNN[2]可以表示为莫奈的特殊实例。

[33]提出了GraphSAGE，一个通用的归纳框架。该框架通过对节点的局部邻域特征进行采样和聚合来生成嵌入。![1545722148836](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545722148836.png)

然而，[33]并没有使用Eq.18中的完整的邻居集，而是通过均匀采样的固定大小的邻居集。[33]提出了三个聚合函数。

- 平均聚集器。平均聚合器简单地将邻居隐藏状态的元素平均化。它可以看作是从换能器GC的8卷积运算的近似。 n框架[46]，以便GCN变体的归纳版本可由![1545722229267](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545722229267.png)平均聚合器不同于其他聚合器，因为它不执行连接h t1 v和h tnv的连接操作。连接操作可以是视图，作为跳过连接[37]的一种形式，可以提高性能。
- LSTM聚合器。[33]还使用了一个基于lsm的聚合器，它具有更大的表达能力。然而，LSTMs以顺序的方式处理输入，因此它们不是排列不变的。[33]使LSTMs通过置换节点的邻居来对无序集进行操作。
- 池聚合器。在池聚集器中，每个邻居的隐藏状态通过一个完全连接的层进行馈送，然后将最大池操作应用于节点的邻居集。![1545722573445](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545722573445.png)请注意，这里可以使用任何对称函数来代替最大池操作。

门。有几项工作尝试在传播步骤中使用gate机制，如GRU[19]或LSTM[39]，以减少前GNN模型的限制，改善信息在图形结构中的长期传播。

[53]提出了门控图神经网络(GGNN)，该神经网络在传播步骤中使用了门控递归单元(GRU)，对一定数量的步骤T展开递归，并通过时间反向传播来计算梯度。

具体来说，传播模型的基本递归是![1545725673010](F:\Machine-learning-and-data-science-notebook\images\图神经网络：应用和技术回顾\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1545725673010.png)

