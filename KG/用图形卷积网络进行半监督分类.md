# 用图形卷积网络进行半监督分类

**摘要**：我们提出了一种可扩展的方法，用于图形结构数据的半监督学习，该方法基于直接在图上操作的卷积神经网络的有效变体。 我们通过频谱图卷积的局部一阶近似来激励我们的卷积结构的选择。我们的模型在图形边缘的数量上线性缩放，并学习隐藏层表示，其编码本地图形结构和节点的特征。 在关于引文网络和知识图数据集的大量实验中，我们证明了我们的方法在很大程度上优于相关方法。

## 1.简介

我们考虑在图形（例如引文网络）中对节点（例如文档）进行分类的问题，其中标签仅可用于一小部分节点。
这个问题可以被构建为基于图的半监督学习，其中标签信息通过某种形式的显式基于图的正则化在图上平滑（Zhu等，2003; Zhou等，2004; Belkin等。，2006; Weston等，2012），例如 通过在损失函数中使用图拉普拉斯正则化项：

$\mathcal{L}=\mathcal{L}_{0}+\lambda \mathcal{L}_{\mathrm{reg}}, \quad​$ with $\quad \mathcal{L}_{\mathrm{reg}}=\sum_{i, j} A_{i j}\left\|f\left(X_{i}\right)-f\left(X_{j}\right)\right\|^{2}=f(X)^{\top} \Delta f(X) .     （1） ​$

这里，$\mathcal{L}_{0}$表示监督损失w.r.t.在图的标记部分，$f(\cdot)$可以是神经网络，如可微函数，$λ$是加权因子，$X$是节点特征向量$X_i$的矩阵。Δ= DDA表示无定向图的非标准化图拉普拉斯算子G =（V，E），其中N个节点vi∈V，边缘（vi，vj）∈E，邻接矩阵A∈NN×N（二进制或加权）和a 度矩阵Dii = P j Aij。方程式的公式 1依赖于图中的连接节点可能共享相同标签的假设。然而，这种假设可能会限制建模能力，因为图形边缘不一定需要编码节点相似性，但可能包含其他信息。

在这项工作中，我们直接使用神经网络模型f（X，A）对图结构进行编码，并在具有标签的所有节点上在受监督目标L0上进行训练，从而避免在损失函数中显式的基于图的正则化。在图的邻接矩阵上调节f（·）将允许模型从监督损失L0分配梯度信息，并且使其能够学习具有和不具有标签的节点的表示。

我们的贡献有两重。首先，我们为神经网络模型引入一个简单且表现良好的分层预测规则，该规则直接在图上运行，并展示如何从谱图卷积的一阶近似中激发它（Hammond等，2011） ）。其次，我们演示了这种形式的基于图形的神经网络模型如何用于图中节点的快速和可扩展的半监督分类。在许多数据集上的实验表明，我们的模型在分类准确性和效率（在挂钟时间内测量）与最先进的半监督学习方法相比都是有利的。

## 2.关于图的快速近似卷积

在本节中，我们为特定的基于图形的神经网络模型$f（X，A）$提供理论动机，我们将在本文的其余部分中使用它。我们考虑使用具有以下分层传播规则的多层图形卷积网络（GCN）：
	$H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)$    （2）

这里，$\tilde{A}=A+I_{N}$是无向图$G$的邻接矩阵，其具有附加的自连接。$I_N$是单位矩阵，$\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}$ and $W^{(l)}$是层特定的可训练权重矩阵。$σ（·）$表示激活函数，例如$\operatorname{ReLU}(\cdot)=\max (0, \cdot)$。$H^{(l)} \in \mathbb{R}^{N \times D}$是第$l^{th}$层中的活动矩阵; H（0）= X.在下文中，我们表明这种传播规则的形式可以通过图上的局部光谱滤波器的一阶近似来激发（Hammond等，2011; Defferrard等，2016）。

$g_{\theta} \star x=U g_{\theta} U^{\top} x​$    (3)

### 2.1 光谱图形卷积

我们将图上的频谱卷积视为信号$x \in \mathbb{R}^{N}$（每个节点的标量）与傅里叶域中$\theta \in \mathbb{R}^{N}$参数化的滤波器$g_{\theta}=\operatorname{diag}(\theta)$的乘法，即：

$g_{\theta} \star x=U g_{\theta} U^{\top} x$

