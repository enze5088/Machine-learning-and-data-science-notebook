# 用图形卷积网络进行半监督分类

**摘要**：我们提出了一种可扩展的方法，用于图形结构数据的半监督学习，该方法基于直接在图上操作的卷积神经网络的有效变体。 我们通过频谱图卷积的局部一阶近似来激励我们的卷积结构的选择。我们的模型在图形边缘的数量上线性缩放，并学习隐藏层表示，其编码本地图形结构和节点的特征。 在关于引文网络和知识图数据集的大量实验中，我们证明了我们的方法在很大程度上优于相关方法。

## 1.简介

我们考虑在图形（例如引文网络）中对节点（例如文档）进行分类的问题，其中标签仅可用于一小部分节点。
这个问题可以被构建为基于图的半监督学习，其中标签信息通过某种形式的显式基于图的正则化在图上平滑（Zhu等，2003; Zhou等，2004; Belkin等。，2006; Weston等，2012），例如 通过在损失函数中使用图拉普拉斯正则化项：

$\mathcal{L}=\mathcal{L}_{0}+\lambda \mathcal{L}_{\mathrm{reg}}, \quad​$ with $\quad \mathcal{L}_{\mathrm{reg}}=\sum_{i, j} A_{i j}\left\|f\left(X_{i}\right)-f\left(X_{j}\right)\right\|^{2}=f(X)^{\top} \Delta f(X) .     （1） ​$

这里，$\mathcal{L}_{0}​$表示监督损失w.r.t.在图的标记部分，$f(\cdot)​$可以是神经网络，如可微函数，$λ​$是加权因子，$X​$是节点特征向量$X_i​$的矩阵。Δ= DDA表示无定向图的非标准化图拉普拉斯算子G =（V，E），其中N个节点vi∈V，边缘（vi，vj）∈E，邻接矩阵A∈NN×N（二进制或加权）和a 度矩阵Dii = P j Aij。方程式的公式 1依赖于图中的连接节点可能共享相同标签的假设。然而，这种假设可能会限制建模能力，因为图形边缘不一定需要编码节点相似性，但可能包含其他信息。

在这项工作中，我们直接使用神经网络模型f（X，A）对图结构进行编码，并在具有标签的所有节点上在受监督目标L0上进行训练，从而避免在损失函数中显式的基于图的正则化。在图的邻接矩阵上调节f（·）将允许模型从监督损失L0分配梯度信息，并且使其能够学习具有和不具有标签的节点的表示。

我们的贡献有两重。首先，我们为神经网络模型引入一个简单且表现良好的分层预测规则，该规则直接在图上运行，并展示如何从谱图卷积的一阶近似中激发它（Hammond等，2011） ）。其次，我们演示了这种形式的基于图形的神经网络模型如何用于图中节点的快速和可扩展的半监督分类。在许多数据集上的实验表明，我们的模型在分类准确性和效率（在挂钟时间内测量）与最先进的半监督学习方法相比都是有利的。

## 2.关于图的快速近似卷积

在本节中，我们为特定的基于图形的神经网络模型$f（X，A）$提供理论动机，我们将在本文的其余部分中使用它。我们考虑使用具有以下分层传播规则的多层图形卷积网络（GCN）：
	$H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} H^{(l)} W^{(l)}\right)$    （2）

这里，$\tilde{A}=A+I_{N}$是无向图$G$的邻接矩阵，其具有附加的自连接。$I_N$是单位矩阵，$\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}$ and $W^{(l)}$是层特定的可训练权重矩阵。$σ（·）$表示激活函数，例如$\operatorname{ReLU}(\cdot)=\max (0, \cdot)$。$H^{(l)} \in \mathbb{R}^{N \times D}$是第$l^{th}$层中的活动矩阵; H（0）= X.在下文中，我们表明这种传播规则的形式可以通过图上的局部光谱滤波器的一阶近似来激发（Hammond等，2011; Defferrard等，2016）。

$g_{\theta} \star x=U g_{\theta} U^{\top} x​$    (3)

### 2.1 光谱图形卷积

我们将图上的频谱卷积视为信号$x \in \mathbb{R}^{N}$（每个节点的标量）与傅里叶域中$\theta \in \mathbb{R}^{N}$参数化的滤波器$g_{\theta}=\operatorname{diag}(\theta)$的乘法，即：

$g_{\theta} \star x=U g_{\theta} U^{\top} x$

其中U是归一化图的特征向量的矩阵，其中，特征值的对角矩阵$\Lambda$ 和 $U^{\top} x$是X的图傅立叶变换，其中，特征值的对称矩阵为$L=I_{N}-D^{-\frac{1}{2}} A D^{-\frac{1}{2}}=U \Lambda U^{\top}$。我们可以将$g_{\theta}$理解为L的特征值的函数，即$g_{\theta}(\Lambda)$。评估方程 图3的计算是昂贵的，因为与特征向量矩阵U的乘法是$O（N^2）$。此外，首先计算L的特征分解对于大图来说可能是非常昂贵的。为了解决这个问题，哈蒙德等人提出了这个问题。（2011）$g_{\theta}(\Lambda)​$可以通过Chebyshev多项式Tk（x）直到K阶的截断展开很好地近似：

$g_{\theta^{\prime}}(\Lambda) \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{\Lambda})$

重新调整的Λ=~2λmaxΛ？IN。λmax表示L的最大特征值.θ0∈RK现在是切比雪夫系数的向量。Chebyshev多项式递归地定义为Tk（x）= 2xTkk1（x））Tkk2（x），其中T0（x）= 1且T1（x）= x。读者可参考Hammond等人。（2011）深入讨论这种近似。

回到信号x与滤波器gθ0的卷积的定义，我们现在有：

$g_{\theta^{\prime}} \star x \approx \sum_{k=0}^{K} \theta_{k}^{\prime} T_{k}(\tilde{L}) x$

L~ =2λmaxLL IN; 通过注意到（UΛU>）k =UΛkU>可以容易地验证。注意，该表达式现在是K-局部化的，因为它是拉普拉斯算子中的K阶多项式，即它仅取决于距离中心节点（K阶邻域）最大K步的节点。评估方程的复杂性。5是O（| E |），即边缘数是线性的。Defferrard等人。（2016）使用这个K局部化卷积来定义图上的卷积神经网络。

### 2.2 分层线性模型

因此，可以通过堆叠等式1的形式的多个卷积层来构建基于图卷的神经网络模型。在图5中，每层后面是逐点非线性。现在，假设我们将逐层卷积运算限制为K = 1（参见方程5），即线性w.r.t. L因此是图拉普拉斯谱的线性函数。

以这种方式，我们仍然可以通过堆叠多个这样的层来恢复丰富类型的卷积滤波器函数，但是我们不限于由例如切比雪夫多项式给出的显式参数化。我们直观地期望这样的模型可以缓解具有非常宽的节点度分布的图的局部邻域结构过度拟合的问题，例如社交网络，引用网络，知识图和许多其他现实世界图数据集。另外，对于固定的计算预算，这种分层线性公式允许我们构建更深层次的模型，这种做法已知可以提高许多领域的建模能力（He et al。，2016）。

在GCN的这种线性公式中，我们进一步近似λmax≈2，因为我们可以预期神经网络参数将在训练期间适应这种规模变化。在这些近似下，Eq5简化为：

$g_{\theta^{\prime}} \star x \approx \theta_{0}^{\prime} x+\theta_{1}^{\prime}\left(L-I_{N}\right) x=\theta_{0}^{\prime} x-\theta_{1}^{\prime} D^{-\frac{1}{2}} A D^{-\frac{1}{2}} x$

具有两个自由参数θ00和θ01。过滤器参数可以在整个图形上共享。这种形式的滤波器的连续应用然后有效地卷积节点的k阶邻域，其中k是神经网络模型中的连续滤波操作或卷积层的数量。

在实践中，进一步限制参数的数量以解决过度拟合并最小化每层的操作（例如矩阵乘法）的数量可能是有益的。这给我们留下了以下表达式：

$g_{\theta} \star x \approx \theta\left(I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}\right) x$

具有单个参数$\theta=\theta_{0}^{\prime}=-\theta_{1}^{\prime}$。请注意，$I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}}$现在在[0，2]范围内有特征值。因此，重复应用这个算子会导致数值不稳定。以及在深度神经网络模型中使用的爆炸/消失梯度。为了缓解这个问题，我们介绍了以下重整化技巧：在$I_{N}+D^{-\frac{1}{2}} A D^{-\frac{1}{2}} \rightarrow \tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}}$，利用$\tilde{A}=A+I_{N}$ 和 $\tilde{D}_{i i}=\sum_{j} \tilde{A}_{i j}$

我们可以将该定义推广到具有$C$个输入通道的信号$X \in \mathbb{R}^{N \times C}$（即每个节点的C维特征向量）和$F$滤波器或特征映射如下：$Z=\tilde{D}^{-\frac{1}{2}} \tilde{A} \tilde{D}^{-\frac{1}{2}} X \Theta$

其中$\Theta \in \mathbb{R}^{C \times F}$现在是滤波器参数的矩阵，$Z \in \mathbb{R}^{N \times F}$是卷积信号矩阵。该滤波操作具有复杂度$\mathcal{O}(|\mathcal{E}| F C)$，因为$\tilde{A} X$可以有效地实现为具有密集矩阵的稀疏矩阵的乘积。

## 3 半监督节点分类

引入了一个简单但灵活的模型f（X，A）来有效地在图上传播信息，我们可以回到半监督节点分类的问题。
如在介绍中所概述的，我们可以通过在数据X和基础图结构的邻接矩阵A上调节我们的模型f（X，A）来放松通常在基于图的半监督学习中做出的某些假设。我们期望这种设置在邻接矩阵包含数据X中不存在的信息的情况下特别强大，例如引文网络中的文档之间的引用链接或知识图中的关系。整体模型，用于半监督学习的多层GCN，在图1中示意性地示出。

### 3.1 示例

在下文中，我们考虑在具有对称邻接矩阵A（二进制或加权）的图上用于半监督节点分类的两层GCN。
我们首先在预处理步骤中计算A = D〜〜1 A〜D〜〜1。然后我们的前向模型采用简单的形式：

$Z=f(X, A)=\operatorname{softmax}\left(\hat{A} \operatorname{ReLU}\left(\hat{A} X W^{(0)}\right) W^{(1)}\right)​$

> ![1554905131675](F:\Machine-learning-and-data-science-notebook\images\GCN\1554905131675.png)图1:左图:输出层使用C输入通道和F特征图进行半重叠学习的多层图卷积网络(GCN)示意图。图的结构(边缘以黑线表示)在层之间共享，标签用Yi表示。右:t-SNE (Maaten & Hinton, 2008)使用5%的标签，在Cora数据集上训练双层GCN的隐藏层激活的可视化(Sen et al.， 2008)。颜色表示文档类。

其中，W(0)∈R C×H是具有H特征映射的隐含层的输入-隐藏权矩阵。

W(1)∈R H×F是一个隐藏到输出权矩阵。按行应用softmax激活函数，定义为softmax(习)= 1zexp(习)和Z = pi exp(习)。对于半监督多类分类，我们对所有标记的例子进行交叉熵误差评估:

$\mathcal{L}=-\sum_{l \in \mathcal{Y}_{L}} \sum_{f=1}^{F} Y_{l f} \ln Z_{l f}$

其中YL是一组具有标签的节点索引。

利用梯度下降训练神经网络权值W(0)和W(1)。在本工作中,我们使用完整的数据集对每一次训练迭代执行批处理梯度下降,这是一个可行的选择。只要数据集在内存中。使用一个的稀疏表示,内存需求为O(可比E),即边数是线性的。通过博士引入训练过程中的随机性。Opout(斯里兰卡瓦斯塔瓦等人,2014年)。我们留下内存有效扩展小批随机梯度下降,供今后的工作。

### 3.2 成就

在实践中，我们利用TensorFlow (Abadi et al.， 2015)利用稀疏密集矩阵乘法实现了基于gpu的Eq. 9高效实现2。计算式9的计算复杂度为O(|E|CHF)，即图边数为线性。

## 4 相关工作

我们的模型既来自基于图的半监督学习领域，也来自最近在图上操作的神经网络方面的工作。在下面的内容中，我们简要地概述了 这两个领域的相关工作。

### 4.1 基于图的半监督学习

近年来，人们提出了大量的基于图表示的半监督学习方法，其中大部分分为两大类：使用某种形式e的方法。 图的拉普拉斯正则化和基于图嵌入的方法。图拉普拉斯正则化的突出例子包括标号传播(朱等人，2003)，流形正则化。 (Belkin等人，2006年)和深度半监督嵌入(Weston等人，2012年)。

最近，人们的注意力转移到了学习图形嵌入的模型上(Mikolov等人，2013年)。DeepWalk(Perozzi等人，2014年)通过PR学习嵌入 节点的局部邻域，从图上的随机游动中取样。line(Tang等人，2015)和node2vec(Grover&Leskovec，2016)用更复杂的随机游动扩展DeepWalk 或者广度优先搜索方案。然而，对于所有这些方法，都需要一个包含随机游走生成和半监督训练的多步流水线，其中每个步骤都必须分别进行优化。类行星(Yang等人， 通过在学习嵌入过程中注入标签信息来缓解这一问题。

### 4.2 图上的神经网络

以前在Gori等人中引入了对图进行操作的神经网络。斯卡塞利等人(2005年)。(2009)作为递归神经网络的一种形式。它们的框架需要重复的APPL。 压缩映射为传播函数，直到节点表示达到稳定的不动点。这一限制后来在Li等人身上得到了缓解。(2016)引进现代实践 r递归神经网络训练到原图神经网络框架。Duvenaud等人(2015)在图和图级分类方法上引入了类似卷积的传播规则。他们的方法需要学习节点度特定的权重矩阵。 不缩放到具有宽节点度分布的大图。我们的模型使用每一层的单一权重矩阵，并通过适当的归一化来处理不同的节点度。 邻接矩阵(见3.1节)。

最近在Atwood&Towsley(2016)中引入了一种基于图的神经网络节点分类的相关方法。他们报告了O(N2)的复杂性，限制了可能的应用范围。 国家统计局。在另一个与此相关的模型中，Niepert等人。(2016)将局部图转换为输入到常规一维卷积神经网络中的序列，这需要定义 在预处理步骤中的节点排序。

我们的方法是基于谱图卷积神经网络，介绍了Bruna等人。(2014年)，后来由Defferrard等人延长。(2016)具有快速局部化卷积。与.形成对比 这些工作，我们在这里考虑的任务，传感器节点分类在网络中的规模要大得多。我们表明，在这种情况下，一些简化(见2.2节)ca n被引入到Bruna等人的原始框架中。(2014年)和Defferrard等人。(2016)提高了大规模网络的可扩展性和分类性能。

## 5 实验

我们在一些实验中测试了我们的模型：引用网络中的半监督文档分类，从知识图中提取的二分图中的半监督实体分类。 ，对各种图的传播模型进行了评价，并对随机图进行了运行时分析。

### 5.1 数据集

我们密切关注Yang等人的实验设置。（2016）。数据集统计数据总结在表1中。在引文网络数据集-Citeseer，Cora和Pubmed（Sen等人，2008）中 - 节点是文档，边缘是引文链接。标签速率表示用于训练的标记节点数除以每个数据集中的节点总数。NELL（Carlson等人，2010; Yang等人，2016）是从具有53,564个关系节点和9,891个实体节点的知识图中提取的二分图数据集。

![1554905975663](F:\Machine-learning-and-data-science-notebook\images\GCN\1554905975663.png)

**Citation networks**   我们考虑了三个引文网络数据集：Citeseer、Cora和Pubmed(Sen等人，2008年)。这些数据集包含每个文档的稀疏字包特征向量和引用链接列表betw。 甚至文件。我们将引用链接视为(无向)边，并构造一个二元对称邻接矩阵A。每个文档都有一个类标签。对于训练，我们每班只使用20个标签，但所有的特征向量都使用。

**NELL**   NELL是从（Carlson等人，2010）中介绍的知识图中提取的数据集。知识图是与定向标记边（关系）连接的一组实体。我们遵循Yang等人所述的预处理方案。（2016）。我们为每个实体对（e1，r，e2）分配单独的关系节点r1和r2为（e1，r1）和（e2，r2）。实体节点由稀疏特征向量描述。我们通过为每个关系节点分配唯一的单热表示来扩展NELL中的特征数量，从而有效地为每个节点产生61,278-dim稀疏特征向量。这里的半监督任务考虑了训练集中每个类只有一个标记示例的极端情况。如果在节点i和j之间存在一个或多个边缘，则通过设置条目Aij = 1，从该图构造二元对称邻接矩阵。

**Random graphs**   我们模拟各种大小的随机图数据集进行实验，测量每个时代的训练时间。对于N个节点的数据集，我们创建了一个随机图，随机分配2N个边。我们将恒等矩阵IN作为输入特征矩阵X，从而隐式地采用一种无特征的方法，其中模型只被告知每个节点的身份，由一个唯一的节点指定- 热向量。我们为每个节点添加虚拟标签yi=1。

### 5.2 实验设置

除非另有说明，否则我们将按照第3.1节中的描述训练双层GCN，并在1,000个标记示例的测试集上评估预定精度。我们使用附录B中最多10层的深层模型提供了额外的实验。我们选择与Yang等人相同的数据集分割。
（2016）具有用于超参数优化的500个标记示例的附加验证集（所有层的丢失率，第一GCN层的L2正则化因子和隐藏单元的数量）。我们不使用验证集标签进行培训。

对于引文网络数据集，我们仅针对Cora优化超参数，并为Citeseer和Pubmed使用相同的参数集。我们使用Adam（Kingma＆Ba，2015）训练所有模型最多200个时期（训练迭代），学习率为0.01，并且窗口大小为10，提前停止，即如果验证损失没有减少，我们将停止训练 连续10个时代。我们使用Glorot＆Bengio（2010）中描述的初始化来初始化权重，并相应地（行）归一化输入特征向量。在随机图数据集上，我们使用32个单位的隐藏层大小并省略正则化（即既没有丢失也没有L2正则化）。

### 5.3 基线

我们将与Yang等人的基线方法进行比较。（2016），即标签传播（LP）（Zhu等，2003），半监督嵌入（SemiEmb）（Weston等，2012），流形正则化（ManiReg）（Belkin等，2006）和跳过 基于图的图嵌入（DeepWalk）（Perozzi等，2014）。我们省略了TSVM（Joachims，1999），因为它不能扩展到我们的一个数据集中的大量类。

我们进一步与Lu＆Getoor（2003）中提出的迭代分类算法（ICA）结合两个逻辑回归分类器进行比较，一个用于单独的局部节点特征，一个用于使用局部特征的关系分类和一个聚合算子，如Sen et所述。（2008年）。我们首先使用所有标记的训练集节点训练本地分类器，并使用它来引导未标记节点的类标签以进行关系分类器训练。我们在所有未标记的节点上使用随机节点排序运行迭代分类（关系分类器）10次迭代（使用本地分类器自举）。L2正则化参数和聚合运算符（计数与prop，参见Sen等人（2008））是基于每个数据集的验证集性能分别选择的。

最后，我们与Planetoid(Yang等人，2016)作了比较，我们总是选择他们表现最好的模型变体(换能型和归纳型)作为基线。

## 6 结果

### 6.1 半监督节点分类

结果摘要见表2。报告的数字表示分类准确率(百分比)。对于ICA，我们报告了具有随机节点排序的100次运行的平均精度。所有其他基线方法的结果取自Planetoid论文(Yang等人，2016年)。Pletoid*表示在他们的论文中提出的变体中各自数据集的最佳模型。

![1554951861227](F:\Machine-learning-and-data-science-notebook\images\GCN\1554951861227.png)

我们进一步报告壁钟培训时间，以秒为止，直到我们的方法（包括评估验证错误）和Planetoid的收敛（括号内）。对于后者，我们使用了由作者3提供的实现，并在与GCN模型相同的硬件（使用GPU）上进行了训练。
我们在与Yang等人相同的数据集分割中训练和测试了我们的模型。（2016）并报告随机权重初始化的100次运行的平均准确度。我们对Citeseer，Cora和Pubmed使用了以下几组超参数：0.5（辍学率），5·10-4（L2正则化）和16（隐藏单位数）; 对于NELL：0.1（丢失率），1·10-5（L2正则化）和64（隐藏单位数）。

此外，我们报告了我们模型在10个随机绘制的数据集分割中的性能，这些数据集分割的大小与Yang等人相同。（2016），由GCN（rand。分裂）表示。在这里，我们报告测试集上的预测准确度的平均值和标准误差百分比。

### 6.2 传播模型的评估

我们在引用网络数据集上比较了我们提出的每层传播模型的不同变体。我们遵循上一节中描述的实验设置。结果摘要见表3。我们原始GCN模型的传播模型用重整化技巧表示（粗体）。在所有其他情况下，将两个神经网络层的传播模型替换为在传播模型下指定的模型。报告的数字表示使用随机权重矩阵初始化的100次重复运行的平均分类准确度。在每层有多个变量Θi的情况下，我们对第一层的所有权重矩阵施加L2正则化。

![1554951942882](F:\Machine-learning-and-data-science-notebook\images\GCN\1554951942882.png)

### 6.3 每期训练时间

在这里，我们报告模拟随机图上100个时期的每个时期的平均训练时间（前向传递，交叉熵计算，后向传递）的结果，以秒钟时钟时间测量。有关这些实验中使用的随机图数据集的详细说明，请参见第5.1节。我们比较了TensorFlow中GPU和仅CPU实现4的结果（Abadi等，2015）。图2总结了结果。

## 7  讨论

### 7.1 半监督模型

在本文的实验中，我们的半监督节点分类方法比现有的相关方法有很大的优势。基于图拉普拉斯正则化(Z)的方法 Hu等人，2003年；Belkin等人，2006年；Weston等人，2012年)很可能受到限制，因为他们假设边缘只是编码节点的相似性。另一方面，基于跳格模型的方法由于其基于多步流水线而难以优化这一事实而受到限制。我们提出的模型可以克服这两个限制，而 与相关方法相比，在效率方面(以挂钟时间衡量)仍是比较有利的。从每一层的相邻节点传播特征信息可以提高分类性能。 与ICA等方法(Lu&Getoor，2003)相比，只聚合标签信息。

我们进一步证明了所提出的重整化传播模型(Eq)。8)既提高了效率(减少了参数和运算，如乘法或加法)，又提高了效率。 对许多数据集的预测性能与NA_(ı)Ve-1阶模型(Eq)相比较.(6)或使用Chebyshev多项式(Eq)的高阶图卷积模型。5)。

### 7.2 限制和今后的工作

在这里，我们描述了当前模型的一些局限性，并概述了在未来的工作中如何克服这些局限性。

内存要求：在具有完全批量梯度下降的当前设置中，内存要求随数据集的大小线性增长。我们已经证明，对于不适合GPU内存的大型图形，CPU上的培训仍然是一个可行的选择。小批量随机梯度下降可以缓解这个问题。然而，生成小批量的过程应该考虑GCN模型中的层数，因为具有K层的GCN的K阶邻域必须存储在存储器中以用于精确的过程。对于非常大且密集连接的图形数据集，可能需要进一步的近似。
	定向边和边缘特征：我们的框架目前不自然地支持边缘特征，并且仅限于无向图（加权或未加权）。然而，NELL上的结果表明，通过将原始有向图表示为无向二分图，可以处理有向边和边缘特征，其中附加节点表示原始图中的边（详见第5.1节）。
	限制假设：通过第2节中介绍的近似，我们隐含地假设局部性（依赖于具有K层的GCN的K阶邻域）和自连接与相邻节点的边缘的相等重要性。但是，对于某些数据集，在A~的定义中引入权衡参数λ可能是有益的：

$\tilde{A}=A+\lambda I_{N}$

在典型的半监督环境中，这个参数现在与有监督损失和无监督损失之间的权衡参数具有相似的作用(参见等式)。1)。然而，在这里，它可以通过梯度下降来学习。

## 8 结论

提出了一种对图结构数据进行半监督分类的新方法.我们的gcn模型使用了一种基于一阶近似的有效分层传播规则。 关于图上的谱卷积。在多个网络数据集上的实验表明，所提出的gcn模型能够以一种有用的方式对图形结构和节点特征进行编码。 半监督分类在这种情况下，我们的模型在计算效率上明显优于最近提出的几种方法。

