# Max-Margin DeepWalk：网络表示的判别学习

摘要：DeepWalk是一种典型的表示学习方法，可以学习社交网络中顶点的低维表示。与其他网络表示学习（NRL）模型类似，它将网络结构编码为顶点表示，并以无监督的形式学习。然而，当应用于机器学习任务（例如顶点分类）时，所学习的表示通常缺乏辨别能力。在本文中，我们通过提出一种新的半监督模型，最大边缘DeepWalk（MMDW）来克服这一挑战。MMDW是一个统一的NRL框架，它共同优化了最大边际分类器和目标社交表示学习模型。受最大边际分类器的影响，学习的表示不仅包含网络结构，而且具有区分特征。学习表示的可视化表明我们的模型比无监督表示更具辨别力，并且顶点分类的实验结果表明我们的方法比其他状态方法实现了显着的改进。源代码可以从https://github.com/thunlp/MMDW.中获取。

## 1.简介

网络表示在网络分析领域发挥着关键作用。有效的网络表示有助于许多网络分析任务，例如顶点分类，聚类和链接预测。作为网络中的基本组件，每个顶点通常表示为离散符号，称为独热表示。由于其简单性，这种表示方法已被广泛用于网络分析。然而，独热表示通常会受到稀疏性问题的影响，并且没有充分考虑顶点之间的相关性。

在近年来分布式表示学习的意识形态的推动下，提出了网络表示学习（NRL）来解决这些问题。NRL旨在为每个顶点学习实值向量以反映其网络信息。学习的向量能够进行各种网络分析任务，例如顶点分类和链接预测。顶点之间的相关性也可以基于实值向量来实现。

近年来已经提出了许多NRL模型，例如DeepWalk [Perozzi et al。，2014]和LINE [Tang et al。，2015]。
[Perozzi et al。，2014]提出了一个在线NRL模型DeepWalk，它根据本地网络信息学习顶点表示。它进行随机游走以获得顶点序列。通过大量序列，DeepWalk采用Skip-Gram [Mikolov等，2013a]，一种有效的单词表示学习模型，通过将顶点序列视为单词句来学习顶点表示。在多标签网络分类任务中已经验证了单词和顶点之间的直接类比。

以前的大多数NRL模型都是在无监督模式中学习的。虽然所学习的表征可以应用于各种任务，但在特定的预测任务中表现出较弱的能力。值得指出的是，在现实世界中，还有许多额外的关于社交网络顶点的标记信息。例如，维基百科的页面可以有分类标签，比如“艺术”、“历史”、“科学”;Cora和Citeseer中的论文也用字段标签存储，便于检索。这种标记信息通常包含有用的顶点摘要或特征，但在原始网络表示学习模型中没有直接使用。

如何将标记信息集成到网络表示学习中，学习网络顶点的鉴别表示是非常重要的。通过MaxEdge原理的启发，我们提出了最大限度的Deepwalk(mmDW)，这是一个判别NRL模型，旨在为社交网络中的顶点寻找预测表示。

如图1所示，MMDW首先将DeepWalk学习为矩阵因式分解。然后，它训练了一个基于最大边缘的分类器(例如，支持向量机[Hearst等人，1998])，并扩大了t。 支持向量与分类边界之间的距离。换句话说，MMDW联合优化了基于最大裕度的分类器(例如支持向量机)和NRL模型。受最大边缘分类器的影响，顶点的表示 更有判断力，更适合于预测任务，我们将在下面的章节中说明这一点。

最后，我们作出了以下几项值得注意的贡献：

(1)提出了一种判别的NRL模型-最大边缘深度行走，将标号信息融合到顶点表示中。作为一种半监督模型，它将最大裕度原理应用到网络中。 表象学习是第一次。

(2)提出了NRL中有偏梯度的概念。向量的有偏梯度指示了矢量应向的方向。这一运动能够扩大两者之间的差距。 在梯度下降算法中，以梯度形式进行两类。

(3)在多个真实数据集上进行了顶点分类实验，验证了MMDW方法的有效性。实验结果表明，MMDW的性能明显优于传统方法。 AlNRL模型与典型的NRL方法相比，它得到了5%~10%的改进。此外，我们还比较了使用t-SNE可视化来说明MMDW的区别.

## 2.相关工作

网络表示学习(NRL)的目的是将网络结构编码到一个低维空间中.学习到的表示可以用于许多网络分析任务，如顶点分类。 聚类和链接预测。

许多NRL模型被提出来学习有效的顶点表示。受Skip-Gram的启发[Mikolov等人，2013年a]，NLP中广泛采用的词汇表示学习模型，[Perozzi等人] .，2014年]由相应的字顶点提出的“深度行走”(DeepWalk)。DDeepWalk执行随机游动来生成顶点序列，并对Skip-Gram模型进行训练以获得顶点表示。从SkipGram派生出来的DeepWalk已经在各种网络分析任务上得到了广泛的验证。另一个典型的NRL模型是line[Tang等人，2015]，它被提议用于处理具有数百万个顶点和数十亿个边的大规模网络。

以下是其他领域的一些基于最大边际的学习方法。[Rolle，2004]首次将最大裕度原理引入马尔可夫网络.[朱等人，2012年]提议的最大熵描述 LDA(MedLDA)学习一个区分主题模型(例如，潜在Dirichlet分配[Blee等人，2003年])。此外，max-space还有利于许多nlp任务，例如语义分析[taskar e]。 t al.，2004]和分词[Pei等人，2014年]。

然而，据我们所知，很少有考虑标记信息的工作学习区分网络表示。上述的nrl方法大多是在unsup中学习的。 简易时装。为了填补这一空白，我们提出了最大边际深度行走(Mmdw)来学习社交网络中顶点的鉴别表示。

## 3.框架

在这一部分中，我们提出了一种新的半监督社会表示模型(MMDW)，它在学习顶点表示时利用标记信息。MMDW是一所大学 基于矩阵分解的分层学习框架。在该模型中，我们对基于最大裕度的分类器(SVM)和目标矩阵因式分解模型进行了优化。相比之下，传统的方法通常学习社会表征，而不利用标记信息并将学习到的表示应用到分类任务中。顶点‘ 标签无法影响学习表象的方式。因此，学习的表征往往不那么有区别。

### 3.1形式化

让我们从正式界定网络表示学习的问题开始。假设有一个社会网络$G=(V，E)$，其中$V$是所有顶点的集合，$E$是这些顶点之间的联系，即$E \subset V \times V$，社会表征学习的目的是建立一个低模糊度。 每个顶点$v \in V$的内射表示$\mathbf{x}_{v} \in \mathbb{R}^{k}$，其中$k$是表示空间的维数，它的期望$|V|$要小得多。学习到的表示编码了社交网络中顶点的语义角色，可以用来度量顶点之间的关联性，也可以作为分类任务的特征。 使用相应的标记$l \in\{1, \cdots, m\}$，可以训练出逻辑回归和支持向量机等分类器。

在下面的部分中，我们介绍了一个典型的社会表示模型，DeepWalk及其矩阵分解形式。之后，我们详细介绍了最大利润率深度行走.

### 3.2 deepwalk作为矩阵分解

DeepWalk[Perozzi等人，2014年]在网络上执行随机游走以建立顶点序列。以顶点序列为词序列，采用Skip-Gram[Mikolov等人，2013 b.] 使用字表示算法，学习网络表示。DeepWalk的性能已经在多个任务和数据集上得到了广泛的验证。

在Skip-Gram的启示下，DeepWalk的目标是最大限度地提高目标顶点与其文本顶点在随机行走窗口内的共现概率。正式地说，假设我们有一个随机漫步。 序列$\mathbf{s}=\left\{v_{1}, \dots, v_{M}\right\}$，每一个$v_{i} \in V$。我们设置了一个窗口大小$K$，对于每个目标顶点$v_i$，我们定义了它的上下文顶点$c_i=\left(v_{i-K}, \dots, v_{i+K}\right) \backslash v_{i}$。因此，DeepWalk的目标可以是 如下所示:

$\mathcal{L}(S)=\sum_{\mathbf{s} \in S}\left[\frac{1}{M} \sum_{i=K}^{M-K} \sum_{v_{j} \in \mathbf{c}_{i}} \log \operatorname{Pr}\left(v_{j} | v_{i}\right)\right]$

这里，S是由随机游动产生的随机游动序列的集合。用Softmax函数计算概率$\operatorname{Pr}\left(v_{j} | v_{i}\right)$，

$\operatorname{Pr}\left(v_{j} | v_{i}\right)=\frac{\exp \left(\mathbf{x}_{j} \cdot \mathbf{x}_{i}\right)}{\sum_{t \in V} \exp \left(\mathbf{x}_{t} \cdot \mathbf{x}_{i}\right)}$

其中$x_j$和$x_i$是顶点$v_J$和$v_i$的表示向量，$(·)$是向量之间的内积。

[Yang等人，2015]证明了DeepWalk实际上分解了一个矩阵M:$M_{i j}=\log \frac{\left[e_{i}\left(A+A^{2}+\cdots+A^{t}\right)\right]_{j}}{t}$

这里，A是转换矩阵，可以看作是行归一化邻接矩阵。EI表示一个指示向量，其中第一个条目是1，其他条目都是0.条目vij是log。 顶点I步走到顶点j的平均概率的算术。

从公式3我们发现计算精确的M是耗费时间的。。因此，我们利用[Yang等人，2015]中的设置，将矩阵分解为M=(AA2)/2。在实践中，我们分解了M而不是o f log M，因为log M比M有更多的非零项，矩阵因式分解的复杂性与非零项的数目成正比[Yu等人，2014]。

现在，我们用矩阵因式分解$M=X^{T} Y$来形式化DeepWalk。我们的目的是找出矩阵$X \in \mathbb{R}^{k \times|V|}$和$Y \in \mathbb{R}^{k \times|V|}$以尽量减少：

$\min _{X, Y} \mathcal{L}_{D W}=\min _{X, Y}\left\|M-\left(X^{T} Y\right)\right\|_{2}^{2}+\frac{\lambda}{2}\left(\|X\|_{2}^{2}+\|Y\|_{2}^{2}\right)$

### 3.3 最高限额-深度行走

最大裕度方法，如支持向量机(Svms)[herst等人，1998]，通常用于各种鉴别问题，包括文档分类和手写字符识别。

本文以学习表示X为特征，训练支持向量机进行顶点分类。假设训练集T={(x1，L1)，···，(xt，Lt)}，则多类支持向量机的目标是FIN。 通过求解下列约束优化问题，得到最优线性函数：

$\min _{W, \xi} \mathcal{L}_{S V M}=\min _{W, \xi} \frac{1}{2}\|W\|_{2}^{2}+C \sum_{i=1}^{T} \xi_{i}$
s.t. $\quad \mathbf{w}_{l_{i}}^{T} \mathbf{x}_{i}-\mathbf{w}_{j}^{T} \mathbf{x}_{i} \geq e_{i}^{j}-\xi_{i}, \quad \forall i, j​$

此处

$e_{i}^{j}=\left\{\begin{array}{ll}{1,} & {\text { if } \quad l_{i} \neq j} \\ {0,} & {\text { if } \quad l_{i}=j}\end{array}\right.$

这里，W=[W1，··，Wm]T是支持向量机的权矩阵，⇠=[⇠1，··，⇠T]是训练集中容许误差的松弛变量。

正如前面提到的那样，这种流水线方法不会影响顶点表示的学习方式。通过学习表示，支持向量机只能帮助找到最优的分类边界。一个 这是一个结果，这些陈述本身并不是歧视性的。

在主题模型最大边缘学习的启发下[朱等人，2012]，我们提出了最大边缘深度行走(MMDW)来学习社交网络中顶点的判别表示。MMDW旨在优化 支持向量机的最大裕度分类器以及基于矩阵分解的深度行走。目标的定义如下：$\begin{aligned} \min _{X, Y, W, \xi} \mathcal{L}=& \min _{X, Y, W, \xi} \mathcal{L}_{D W}+\frac{1}{2}\|W\|_{2}^{2}+C \sum_{i=1}^{T} \xi_{i} \\ & \text { s.t. } \quad \mathbf{w}_{l_{i}}^{T} \mathbf{x}_{i}-\mathbf{w}_{j}^{T} \mathbf{x}_{i} \geq e_{i}^{j}-\xi_{i}, \quad \forall i, j \end{aligned}$

## 5.实验

在这一部分中，我们在社交网络中进行顶点分类，以评估我们提出的模型。此外，我们还对学习的表征进行可视化，以验证MMDW能够学习区分性表征。

### 5.1 数据集和实验设置

我们使用以下三个典型的数据集进行顶点分类。

**Cora：**Cora 是[McCallum等人，2000]撰写的一篇研究论文集。它包含2，708篇机器学习论文，分为7类。他们之间的引文关系 一个典型的社交网络。

**Citeseer：**Citeseer是[McCallum等人，2000]撰写的另一篇研究论文集。它包含3，312份出版物和它们之间的4，732个连接。这些论文来自6个班。

**Wiki：**Wiki[Sen等人，2008年]包含19个类别的2，405个网页以及它们之间的17，981个链接。它比Cora和Citeseer密度大得多。

为了进行评估，我们随机抽取了一部分有标记的顶点，并将其表示作为训练的特征，其余的用于测试。我们将培训率从10%提高到 90%，并使用多类支持向量机[Crammer and Singer，2002]构建分类器.

### 5.2 基线方法

![1554992537007](F:\Machine-learning-and-data-science-notebook\images\MMDW\1554992537007.png)

DeepWalk。DeepWalk[Perozzi等人，2014]是一个典型的NRL模型，它学习基于网络结构的顶点表示。我们在DeepWalk中设置参数如下，窗口大小K=5，每次行走 顶点&=80与表示维数k=200。对于顶点v，我们以表示v作为网络特征向量。

DeepWalk as Matrix Factorization：在前面的部分中，我们介绍了DeepWalk可以被训练成矩阵因式分解的形式。因此，我们将矩阵M=(AA2)/2分解，并取分解后的矩阵M=(A2)/2 矩阵X作为顶点的表示。

2nd-LINE.line[Tang等人，2015]是另一个在大规模网络中学习网络表示的NRL模型。我们使用二阶接近线(第二线)来学习有向n的表示。 蚀刻。与DeepWalk一样，我们还将表示长度设置为200。

### 5.3 实验结果和分析

表1、表2和表3显示了不同数据集上不同训练比率的分类准确性。在这些表中，我们将DeepWalk表示为dw，DeepWalk的矩阵分解形式表示为 MFDW和二线的简称。我们还展示了在⌘范围从10！4到10！2之间的MMDW的性能。从这些表格中，我们可以看到以下几点：

(1)在不同的数据集和不同的训练比率上，所提出的最大边际深度行走一致且显着地优于所有基线。请注意，mmdw实现了近10%的改进。 当训练率在50%左右时，对Citerseer和Wiki进行5%的改进。DeepWalk不能很好地表示Citeseer和Wiki中的顶点，而MMDW能够很好地处理这种情况。泰斯 E改进表明，MMDW具有更强的鲁棒性，尤其是当网络表示质量较差时性能更好。

(2)需要特别注意的是，MMDW在Citeseer和Wiki上的训练数据比原来的DeepWalk少了一半，但它的表现要好得多。结果表明，将MMDW应用于预测任务是有效的。

(3)与原始的社会表征学习方法相比，本文提出的MMDW学习方法有了很大的改进。相反，原DeepWalk和DeepWalk作为矩阵因式分解的性能为u。 在各种数据集上都是稳定的。这表明，引入有监督的信息是非常重要的，MMDW对于多样化的网络是灵活的。

上面的观察表明，MMDW能够结合标签信息来生成高质量的表示。MMDW不是特定于应用程序的。Verti的学习表征 CES可以应用于多个任务中，包括顶点相似度、链接预测、分类等。有偏梯度的思想可以很容易地推广到其他矩阵分解方法中。

### 5.4 收敛性和参数敏感性

MMDW交替优化最大裕度分类器和矩阵因式分解。在图2的上半部分，我们展示了在不同训练比率下训练模型时精度的收敛趋势。我们观察到，我们提出的MMDW总是在2或3次迭代后获得最佳性能。MMDW的性能从一开始就迅速提高，然后趋于稳定。快速收敛速度 表示MMDW培训的效率。

由于原始梯度和偏置是通过不同的方法计算出来的，它们最初处于不同的数量级。因此，我们引入了一个超参数⌘来平衡偏置梯度。 T和原始梯度。我们确定训练率为50%，并测试了不同⌘的MMDW的性能。

从图2的底部，我们观察到，当⌘在10！5到10！2之间时，mmdw具有稳定的性能，当⌘=10！2时，性能最好。一个固定参数的⌘适合于衍射。 NT数据集。

### 5.5 可视化

在本文中，我们提出了学习社交网络的区分表示的最大限度深度行走。为为了验证学习到的表示是否是区分性的，我们使用t-SNE可视化工具在图3中给出了顶点的二维表示。 在这个图中，每个点代表一个顶点，每个颜色代表一个类别。我们随机选择了4个类别，以更清楚地显示趋势。

### 5.6 案例研究

为了证明MMDW的有效性，我们以Cora数据集为例提供了一个实例。这个实例的标题是“FastOnlineQ(！)”，它属于“强化学习”的范畴。 如表4所示，我们列出了前5位近邻，以及DeepWalk和MMDW学到的表示。在这里，我们使用余弦相似度来度量顶点之间的距离。

从表4中，我们观察到DeepWalk找到的两个邻居与实例属于同一类别，而MMDW则发现5。从实例的标题中，我们可以得到它是相关的 o“在线学习”和“Q问题”。DeepWalk发现的大多数邻居与这些主题没有关联，而MMDW发现的大多数邻居都与这些主题密切相关。它表明 通过考虑标记信息，MMDW提高了表示的质量。

## 6 结论和今后的工作

本文提出了一种用于社交网络的判别表示法学习模型-最大裕度深度行走(MMDW).在介绍标记信息和最大裕度原理的基础上，提出了mmdw。 学习反映其网络结构和标注信息的顶点表示。在真实数据集上的实验结果表明，MMDW对预测任务是有效的.莫雷 学习表征的可视化证实了MMDW的识别性。

我们将在今后的工作中进行更多的探索：

- 我们已经证明了最大边际深度行走的有效性。在未来的研究中，我们将探讨如何在其他社会表征学习模式(如LINE)上进行最大限度学习。
- 通过将原深度行走转化为矩阵因式分解形式，我们学习了一种判别的深度行走。这样，就更容易平衡偏置矢量和梯度。不过，这是一种离线方法。今后，我们将努力探索在线鉴别学习的方法。