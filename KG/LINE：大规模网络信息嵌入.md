# LINE：大规模网络信息嵌入

**LINE︰ Large-Information NetworkEmbedding**
Jian Tang1,Meng Qu2, MingzheWang∗, Ming Zhang2, Jun Yan1,Qiaozhu Mei3

1MicrosoftResearch Asia, {jiatang,junyan}@microsoft.com

2School ofEECS, Peking University, {mnqu, wangmingzhe, mzhang_cs}@pku.edu.cn

3School ofInformation, University of Michigan, qmei@umich.edu



摘要：本文研究了将非常大的信息网络嵌入到低维向量空间中的问题，该问题在可视化、节点分类和链路预测等任务中具有重要的应用价值。现有的图形嵌入方法大多不适用于通常包含数百万节点的真实信息网络。本文提出了一种新的网络嵌入方法，称为LINE，它适用于任意类型的信息网络:无向、有向和/或加权。该方法优化了一个精心设计的目标函数，同时保留了本地和全局网络结构。针对经典随机梯度下降法的局限性，提出了一种边缘采样算法，提高了推理的有效性和效率。实证实验证明了这条线在各种真实信息网络上的有效性，包括语言网络、社交网络和引文网络。该算法是非常有效的，它能够学习在一个典型的单机上在几个小时内嵌入一个具有数百万个顶点和数十亿条边的网络。这一行的源代码可以在网上找到。

一、 介绍

信息网络在现实世界中无处不在，例如航空公司网络、出版物网络、社交和通信网络以及万维网。这些信息网络的规模从数百个节点到数百万个节点不等。对大型信息网络的分析越来越受到学术界和工业界的重视。摘要研究了将信息网络嵌入低维空间的问题，在低维空间中，每个顶点都表示为一个低维向量。这种低维嵌入在可视化[21]、节点分类[3]、链接预测[10]、推荐[23]等多种应用中非常有用。

机器学习文献(e.g.，[4,20,2])提出了各种各样的图形嵌入方法。它们通常在较小的网络上表现良好。当涉及到包含数百万节点和数十亿边缘的真实世界信息网络时，问题就变得更具挑战性。例如，Twitter的follow-follower网络在2012年拥有1.75亿活跃用户和大约200亿边缘用户。大多数现有的图嵌入算法并不适用于这种规模的网络。例如，经典的图嵌入算法，如MDS[4]、异构映射[20]、拉普拉斯特征映射[2]，其时间复杂度至少是顶点数的二次函数，这对于具有数百万节点的网络来说代价太大。虽然最近有一些关于大规模网络嵌入的研究，但是这些方法要么使用的是一种非为网络设计的间接方法(如[1])，要么缺乏针对网络嵌入的明确的目标函数(如[16])。我们期望一个具有精心设计的目标函数来保持图的性质的新模型，以及一种有效的优化技术，能够有效地找到数百万节点的嵌入。

本文提出了一种网络嵌入模型 LINE。它可以扩展到非常大的，任意类型的网络:无向，有向和/或加权。该模型优化了保持局部网络结构和全局网络结构的目标。自然地，局部结构由网络中观察到的链路表示，这些链路捕获顶点之间的一阶近似。现有的大多数图嵌入算法都是为了保持这种一阶邻近性而设计的，例如异构映射[20]和拉普拉斯特征映射[2]，即使它们没有缩放。我们观察到，在真实的网络中，许多(如果不是大多数)合法链接实际上没有被观察到。换句话说，在真实世界的数据中观测到的一阶近似并不足以保持全局网络结构。作为补充，我们研究了顶点之间的二阶邻近性，它不是通过观察到的连接强度来确定的，而是通过顶点的共享邻域结构来确定的。二阶邻近性的一般概念可以解释为共享邻居可能相似的节点。这种直觉可以在社会学和语言学的理论中找到。例如，在社交网络[6]中，“两个人的友谊网络的重叠程度与他们之间的联系强度相关”;在文本语料库[5]中，“你应该从它的朋友那里知道一个词”(Firth, J. R. 1957:11)。事实上，拥有许多共同的朋友的人很可能会有相同的兴趣，成为朋友，和许多相似的词一起使用的词很可能有相似的含义。

> ![1543411128948](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1543411128948.png)
>
> 图1:一个玩具信息网络的例子。边缘可以是非有向的、有向的和/或加权的。顶点6和顶点7应该被紧密地放置在低维空间中，因为它们是通过一个强有力的纽带连接在一起的。顶点5和6也应该被放置得很近，因为它们共享相似的邻居。

图1给出了一个说明性的例子。由于顶点6与7之间的边权值较大，即， 6和7的一阶邻近性较高，在嵌入空间中应紧密表示。另一方面，虽然顶点5和6之间没有联系，但它们有许多共同的邻域，即，它们具有较高的二阶邻近性，因此也应该彼此密切表示。我们期望二阶近似的考虑能有效地补充一阶近似的稀疏性，更好地保持网络的全局结构。在本文中，我们将提出精心设计的目标，以保持一级和二级接近性。

即使找到了一个合理的目标，为一个非常大的网络优化它也是一个挑战。利用随机梯度下降法进行优化是近年来备受关注的一种方法。然而，我们证明了在真实的信息网络中直接采用随机梯度下降法是有问题的。这是因为在许多网络中，边是加权的，而且权值通常具有很大的方差。考虑一个单词共现网络，其中单词对的权重(共现)可能从1到数十万。这些边的权值会被乘以梯度，导致梯度的爆炸，从而影响性能。为了解决这一问题，我们提出了一种新的边缘采样方法，提高了推理的有效性和效率。我们对概率与权重成比例的边缘进行采样，然后将采样的边缘作为二值边缘进行模型更新。在这个采样过程中，目标函数保持不变，边缘的权重不再影响梯度。

LINE非常通用，即使找到了一个合理的目标，它也可以很好地用于定向，为非常大的网络优化它是一个挑战。利用随机梯度下降法进行优化是近年来备受关注的一种方法。然而，我们证明了在真实的信息网络中直接采用随机梯度下降法是有问题的。这是因为在许多网络中，边是加权的，而且权值通常具有很大的方差。考虑一个单词共现网络，其中单词对的权重(共现)可能从1到数十万。这些边的权值会被乘以梯度，导致梯度的爆炸，从而影响性能。为了解决这一问题，我们提出了一种新的边缘采样方法，提高了推理的有效性和效率。我们对概率与权重成比例的边缘进行采样，然后将采样的边缘作为二值边缘进行模型更新。在这个采样过程中，目标函数保持不变，边缘的权重不再影响梯度。

总之，我们做出了以下贡献

- 我们提出了一种新的网络嵌入模型，称为线，它适用于任意类型的信息网络，并且很容易扩展到数百万个节点。它有一个精心设计的目标函数，既保留了一阶相似度，也保留了二阶相似度。
- 我们提出了一种边缘采样算法来优化目标。该算法克服了经典随机梯度算法的局限性，提高了推理的有效性和效率。
- 我们在真实世界的信息网络上进行了广泛的实验。实验结果验证了该模型的有效性和有效性。

**组织：**本文的其余部分组织如下。第二部分对相关工作进行了总结。第三节正式定义了大规模信息网络嵌入问题。第四部分详细介绍了直线模型。第五部分给出了实验结果。最后，我们在第6节中总结。

## 2 相关工作

我们的工作涉及到一般的经典的图像嵌入或降维方法，如多维标度(MDS)[4]，异构化[20]，LLE[18]和拉普拉斯特征映射[2]。这些方法通常首先利用数据点的特征向量构造亲和图，例如数据的k近邻图，然后将亲和图[22]嵌入低维空间。然而，这些算法通常依赖于求解亲和矩阵的主导特征向量，其复杂度至少是节点数的二次型，使得它们在处理大规模网络时效率低下。

最近的文献中有一种技术叫做图分解[1]。利用随机梯度下降法，通过矩阵分解求出大图的低维嵌入。这是可能的，因为图可以表示为亲和矩阵。然而，矩阵分解的目标并不是为网络设计的，因此并不一定要保留全局网络结构。直观地说，图分解期望节点具有更高的一阶邻近性。相反，直线模型使用的目标是专门为网络设计的，它既保留了一级代理，也保留了二级代理。在实际应用中，图分解方法只适用于无向图，而所提出的模型适用于无向图和有向图。

最近与我们相关的工作是DeepWalk[16]，它为嵌入社交网络部署了一个截断的随机游走。虽然从经验上讲是有效的，但是深度行走并没有提供一个清晰的目标来阐明哪些网络属性被保留下来。从直觉上讲，DeepWalk期望二阶邻近度更高的节点产生类似的低维表示，而这条线同时保留了一阶和二阶代理。DeepWalk使用随机游走来扩展顶点的邻域，这类似于深度优先搜索。我们采用广度优先的搜索策略，这是一种更合理的二阶逼近方法。在实际应用中，深度遍历只适用于未加权网络，而我们的模型适用于具有加权和未加权边的网络。

在第5节中，我们使用各种真实世界网络对所提出的模型与这些方法进行了实证比较。

## 3 问题定义

我们正式定义了利用一阶和二阶代理嵌入大规模信息网络的问题。我们首先定义一个信息网络如下

定义1。(信息网络)信息网络定义为G = (V, E)，其中V是顶点的集合，每个顶点代表一个数据对象，E是顶点之间的边的集合，每个边代表两个数据对象之间的关系。每条边e e是一个有序对e = (u, v)，与权值wuv > 0相关联，表示关系的强度。如果G是无向的，我们有(u, v) (v, u)和wuv wvu;如果G是定向的，我们有(u, v) 6 (v, u)和wuv 6 wvu。

在实践中，信息网络可以是定向的(如引用网络)，也可以是非定向的(如Facebook用户的社交网络)。边的权值可以是二进制的，也可以取任何实数。注意，虽然负边权值是可能的，但在本研究中我们只考虑非负权值。例如，在引文网络和社交网络中，wuv取二进制值;在不同对象间的共现网络中，wuv可以取任何非负值。在某些网络中，边的权值可能会出现偏差，因为一些对象可能同时出现多次，而另一些对象可能只出现几次。

将信息网络嵌入低维空间在各种应用程序中都很有用。要进行嵌入，必须保留网络结构。第一直觉是局部网络结构，即。，则必须保持顶点之间的局部成对邻近性。我们将局部网络结构定义为顶点之间的第一存储距离。

定义2。(一阶邻近性)网络中的一阶邻近性是两个顶点之间的局部成对邻近性。对于每一对由边(u, v)连接的顶点，该边的权值wuv表示u与v之间的第一阶近似。如果u与v之间没有观察到边，则它们的一阶近似为0。

一阶近似通常意味着现实网络中两个节点的相似性。例如，在社交网络中彼此是朋友的人往往有相似的兴趣;在万维网中相互链接的页面往往会讨论类似的主题。由于这种重要性，许多现有的图嵌入算法，如异构映射、LLE、拉普拉斯特征映射和图分解，都以保持一阶邻近性为目标。	

然而，在真实世界的信息网络中，所观察到的链接只占很小的比例，很多链接缺失了[10]。缺失链路上的一对节点具有零一级邻近性，尽管它们在本质上非常相似。因此，仅靠一阶邻近性是不足以维持网络结构的，重要的是寻找另一种邻近性概念来解决稀疏性问题。一个自然的直觉是，拥有相似邻居的顶点往往彼此相似。例如，在社交网络中，拥有相似朋友的人往往有相似的兴趣，从而成为朋友;在词共现网络中，总是与同一组词共现的词往往具有相似的意义。因此，我们定义了二阶邻近性，它补充了一阶邻近性，并保留了网络结构。

定义3。(二阶邻近性)网络中一对顶点(u, v)之间的二阶邻近性是它们的邻域网络结构之间的相似性。数学上，令pu = (wu,1，…)， wu，|V |)表示u与所有其他顶点的一阶近似，则u与V的二阶近似由pu与pv的相似性决定。如果u和v之间没有连接顶点，那么u和v之间的二阶近似为0。

我们研究了网络嵌入的一阶和二阶邻近性，其定义如上。

定义4。(大规模信息网络嵌入)给定一个大型网络G = (V, E),大规模信息网络嵌入的问题旨在代表每个顶点V∈采访到一个低维空间,即,学习一个函数fG: V→Rd, d V | |。在空间Rd中，保留了顶点之间的一阶近似和二阶近似。

接下来，我们介绍了一个同时保留一阶和二阶代理的大规模网络嵌入模型。

## 4 LINE:大规模信息网络嵌入

一种适用于现实信息网络的理想的嵌入模型必须满足以下几个要求:首先，它必须能够同时保持顶点之间的一阶邻近性和二阶邻近性;其次，它必须适用于非常大的网络，比如数百万个顶点和数十亿条边;第三，它可以处理任意类型的边缘网络:有向、无向和/或加权。在这一部分，我们提出了一种新的网络嵌入模型，称为LINE，它满足所有这三个要求。

### 4.1 模型描述

我们分别描述了保持一阶接近性和二阶接近性的直线模型，然后介绍了一种将这两种接近性结合起来的简单方法。

#### 4.1.1 LINE的一阶相似性

一阶邻近性是指网络中顶点之间的局部成对邻近性。为了对一阶邻近度建模，对于每条无向边(i, j)，我们定义顶点vi与vj的联合概率如下:

![1544712894504](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544712894504.png)

其中~ui R d是顶点vi. Eqn的低维向量表示。(1)定义空间V V上的分布p(·，·)，其经验概率可以定义为p1(i, j) = wij W，其中W = p(i,j) E wij。为了保持一阶近似，一种直接的方法是最小化以下目标函数

![1544712846971](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544712846971.png)

其中d(·，·)为两个分布之间的距离。我们选择最小化两个概率分布的kl散度。用kl散度代替d(·，·)，省略一些常数，我们有:

![1544712993178](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544712993178.png)

注意，一阶近似仅适用于无向图，不适用于有向图。通过查找{~ui}i=1..|V |使得Eqn中的目标最小化。(3)，我们可以表示d维空间中的每个顶点。

#### 4.1.2 LINE的二阶相似性

二阶邻近性适用于有向图和无向图。给定一个网络，在不丧失通用性的前提下，我们假设它是有向的(一个无向边可以看作是两个方向相反、权值相等的有向边)。二阶邻近性假设共享多个顶点到其他顶点的连接的顶点彼此相似。在这种情况下，每个顶点也被视为一个特定的上下文，并且在上下文中具有相似分布的顶点被认为是相似的。因此，每个顶点都扮演两个角色:顶点本身和其他顶点的特定上下文。我们引入了两个向量~ui和~u0 i，其中~ui表示作为顶点的vi，而~u0 i表示作为特定上下文的vi。对于每条有向边(i, j)，我们首先定义顶点vi生成上下文vj的概率为:

![1544713221242](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544713221242.png)

其中|V |是顶点或上下文的数量。对于每个顶点vi, Eqn。(4)在上下文中实际定义了条件分布p2(·|vi)，即，即网络中所有顶点的集合。如上所述，二阶邻近性假定在上下文中具有相似分布的顶点彼此相似。为了保持二阶邻近性，我们应该使低维表示的上下文p2(·|vi)的条件分布接近经验分布p2(·|vi)。因此，我们将以下目标函数最小化：

![1544713342728](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544713342728.png)

其中d(·，·)为两个分布之间的距离。作为网络中顶点的重要性可能不同,我们在目标函数中引入λi代表网络中顶点我的声望,可以测量通过学位或估计算法如PageRank [15]。经验分布p2 (·| vi)被定义为p2 (vj | vi) =维琪di,维琪在哪边的重量(i, j)和迪是顶点的出度我,即di = P k N(我)伟嘉,其中N(我)是一组out-neighbors vi。本文为简单起见我们λi顶点的度我,即λi = di,这里我们也采用KL-divergence距离函数。用KL-divergence代替d(··),设置λi = di和省略一些常数,

![1544713473754](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544713473754.png)

通过学习{ ~ ui } i = 1 . .|V |和{~u0 i}i=1..为了使这个目标最小化，我们可以用一个d维向量~ui表示每个顶点vi。

#### 4.1.3 结合一阶和二阶近似性

为了同时保留一阶和二阶近似性来嵌入网络，我们在实践中发现一种简单有效的方法是对分别保留一阶和二阶邻近性的线模型进行训练，然后将两种方法训练的嵌入对每个顶点进行连接。将这两种接近性结合起来的一种更有原则的方法是共同训练目标函数(3)和(6)，我们将这两种函数留作今后的工作。

### 4.2 模型优化

优化目标(6)的计算代价很大，在计算条件概率p2(·|vi)时，需要对整个顶点集求和。为了解决这个问题，我们采用了[13]中提出的负采样的方法，即对每条边(i, j)按照一定的噪声分布对多条负边进行采样。

![1544713905501](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544713905501.png)

在σ(x) = 1 / (1 + exp (x))是sigmoid函数。第一项对观测到的边缘进行建模，第二项对从噪声分布中提取的负边缘进行建模，K为负边缘的个数。我们设Pn(v) dv /4为[13]，其中dv为顶点v的出度。

对于目标函数(3)，存在一个平凡解:uik =，对于i=1，…， |V |， k = 1，…为了避免繁琐的求解，我们仍然可以利用负采样方法(7)，将~u0T j改为~uT j。

我们采用异步随机梯度算法(ASGD)[17]对Eqn进行优化。(7)在每个步骤中，ASGD算法对一小批边缘进行采样，然后更新模型参数。如果 对n个边(i，j)进行采样，得到梯度w.r.t。顶点I的嵌入向量~UI将计算为：

![1544714278383](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1544714278383.png)

注意，梯度将乘以边缘的权值。当边的权值存在较大的方差时，这将成为一个问题。例如，在一个词共现网络中，有些词共现多次(如上万次)，而有些词只共现几次。在这样的网络中，梯度的尺度是不同的，很难找到一个好的学习率。如果我们根据权重较小的边选择较大的学习率，那么权重较大的边的梯度就会爆炸，而根据权重较大的边选择学习率，梯度就会太小。

#### 4.2.1 边缘采样优化

解决上述问题的直觉是，如果所有边的权值相等(例如，具有二进制边的网络)，那么就没有选择合适学习速率的问题。因此，一种简单的处理方法是将加权边展开为多个二进制边，例如，将权值为w的边展开为w个二进制边。这将解决问题，但会大大增加内存需求，特别是当边缘的权重非常大时。为了解决这个问题，我们可以对原始边缘进行采样，将采样的边缘作为二值边缘，采样概率与原始边缘的权值成比例。采用这种边缘采样处理，总体目标函数保持不变。问题归结为如何根据边的权值对边进行采样。

令W = (w1, w2，…)， w|E|)表示边的权值序列。我们可以简单地计算权值wsum = P|E| i= 1wi的和，然后在[0,wsum]的范围内采样一个随机值，看看该随机值落在哪个区间[1 j=0 wj, Pi j=0 wj]。这种方法需要O(|E|)的时间来绘制样本，当边数|E|较大时，这种方法开销较大。我们使用别名表方法[9]根据边的权值绘制样本，重复从相同的离散分布绘制样本只需要O(1)时间。

从别名表中对边进行采样需要常数时间O(1)，负采样优化需要O(d(K+ 1))时间，其中K为负采样个数。因此，总的来说，每一步都需要O(dK)时间。在实践中，我们发现优化步骤的数量通常与边的数量成正比(|E|)。因此，直线的总时间复杂度为O(dK|E|)，它与边数|E|是线性的，不依赖于顶点数|V |。边缘采样处理在不影响随机梯度下降效率的前提下，提高了随机梯度下降的效率。

### 4.3 讨论

讨论了线模型的几个实际问题。低程度的顶点。一个实际问题是如何准确嵌入小角度的顶点。由于该节点的邻节点数量非常小，很难准确地推断出它的表示形式，尤其是对于高度依赖上下文数量的基于二阶邻近性的方法。一个直观的解决方案是通过增加更高阶的邻居来扩展这些顶点的邻居，比如邻居的邻居。在本文中，我们只考虑添加二阶邻域，即，到每个顶点。顶点i和它的二阶邻居j之间的权值被测量为：

![1544714759863](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544714759863.png)

在实际操作中，只能添加顶点{j}的一个子集，其中最大的接近wij与低次顶点i.新顶点。另一个实际问题是如何找到新到达顶点的表示。对于一个新的顶点i，如果已知它与现有顶点的连接，我们可以得到关于现有顶点的经验分布p1(·，vi)和p2(·|vi)。根据目标函数Eqn，得到新的顶点的嵌入。(3)或Eqn。(6)，一种直接的方法是最小化下列目标函数之一

![1544714859191](F:\Machine-learning-and-data-science-notebook\images\LINE\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544714859191.png)

通过更新新顶点的嵌入，保持现有顶点的嵌入。如果新顶点和现有顶点之间没有连接，我们必须求助于其他信息，例如顶点的文本信息，并将其作为以后的工作。

## 5 实验

数据集。

(1)网络语言。我们从维基百科的整个英文页面构建了一个词共现网络。每5个单词滑动窗口中的单词被认为是同时出现的。频率小于5的单词被过滤掉。(2)社交网络。我们使用两个社交网络:Flickr和Youtube2。Flickr网络比Youtube网络密度大(与DeepWalk[16]中使用的网络相同)。(3)引文网络。引文网络有两种类型:作者引文网络和论文引文网络。利用DBLP数据集[19]3构建了作者间和论文间的引文网络。作者引文网络记录了一个作者所写和另一个作者所引用的论文数量。这些网络的详细统计数据汇总在表1中。它们代表了各种各样的信息网络:有向和无向、二进制和加权。每个网络至少包含50万个节点和数百万条边，最大的网络包含大约200万个节点和10亿条边。

比较算法。

图分解(GF)[1]。并与矩阵分解技术进行了比较。信息网络可以表示为亲和矩阵，并且能够通过矩阵分解用低维向量表示每个顶点。图因式分解通过随机梯度下降优化，能够处理大型网络。它只适用于无向网络。

DeepWalk[16]。DeepWalk是最近提出的一种用于社交网络嵌入的方法，它只适用于具有二元边缘的网络。对于每个顶点，从该顶点开始的截尾随机游走用于获取上下文信息，因此只使用二阶邻近性。

LINE-SGD。这是4.1节中介绍的优化目标Eqn的线模型。(3)或Eqn。(6)直接用随机梯度下降法。该方法在对边缘进行采样进行模型更新时，将边缘的权值直接乘以梯度。这种方法有两种变体:LINE-SGD(1)和LINE-SGD(2)，它们分别使用一阶和二阶邻近。