



# 网络表示学习：综述

Daokun Zhang, Jie Yin, Xingquan Zhu Senior Member, IEEE, Chengqi Zhang Senior Member, IEEE

#### 摘要

随着信息技术的广泛使用，信息网络越来越受欢迎，以捕捉各种学科之间的复杂关系，例如社交网络，引文网络，电信网络和生物网络。分析这些网络揭示了社会生活的不同方面，例如社会结构，信息传播和沟通模式。然而，实际上，大规模的信息网络经常使网络分析任务在计算上昂贵或难以处理。最近已经提出网络表示学习作为通过保留网络拓扑结构，顶点内容和其他辅助信息将网络顶点嵌入到低维向量空间中的新学习范例。这有助于在新的向量空间中容易地处理原始网络以进行进一步分析。在本次调查中，我们对数据挖掘和机器学习领域的网络表示学习的当前文献进行了全面的回顾。我们提出新的分类法，根据潜在的学习机制，旨在保留的网络信息以及算法设计和方法，对最先进的网络表示学习技术进行分类和总结。我们总结了用于验证网络表示学习的评估协议，包括已发布的基准数据集，评估方法和开源算法。我们还进行了实证研究，以比较代表性算法在常见数据集上的性能，并分析其计算复杂性。最后，我们建议有前途的研究方向，以促进未来的研究。

**索引术语** - 信息网络，图挖掘，网络表示学习，网络嵌入。



## 1 简介

信息网络在社交网络，引文网络，电信网络和生物网络等形式的各种实际应用中变得无处不在。这些网络的规模范围从数百到数百万甚至数十亿个顶点[1]。 分析信息网络在许多学科的各种新兴应用中起着至关重要的作用。例如，在社交网络中，将用户分类为有意义的社交群组对于许多重要任务是有用的，例如用户搜索，有针对性的广告和推荐; 在通信网络中，检测社区结构有助于更好地理解谣言传播过程; 在生物网络中，推断蛋白质之间的相互作用可以促进疾病的新治疗。 然而，对这些网络的有效分析在很大程度上依赖于网络的表示方式。通常，离散邻接矩阵用于表示网络，其仅捕获顶点之间的相邻关系。 实际上，这种简单的表示不能体现更复杂，更高阶的结构关系，例如路径，频繁的子结构等。结果，这样的传统例程经常使得许多网络分析任务在大规模网络上计算上昂贵且难以处理。 以社区检测为例，大多数现有算法涉及计算矩阵[2]的谱分解，其相对于顶点数至少具有二次时间复杂度。 这种计算开销使得算法很难扩展到具有数百万个顶点的大规模网络。

近年来，网络表示学习(NRL)引起了人们广泛的研究兴趣。NRL的目标是学习潜在的、低维的网络顶点表示，同时保持网络拓扑。Y结构、顶点内容和其他侧信息。在学习了新的顶点表示之后，通过应用常规向量，网络分析任务可以轻松高效地执行基于机器学习算法的新的表示空间。这就省去了直接应用于原始网络的复杂算法的必要性。

早期与网络表示学习相关的工作可以追溯到21世纪初，当时研究人员提出图形嵌入算法作为降维技术的一部分。给定一组i.i.d. （独立且相同分布的）数据点作为输入，图形嵌入算法首先计算成对数据点之间的相似性以构建亲和度图，例如k-最近邻图，然后将亲和度图嵌入到具有低得多的新空间中维。我们的想法是找到一个隐藏的低维流形结构在由构造的图形反射的高维数据几何中，使得连接的顶点在新的嵌入空间中保持彼此更接近。Isomap，局部线性嵌入（LLE）[4]和拉普拉斯算子图[5]是基于这个基本原理的算法的例子。但是，图形嵌入算法是在i.i.d上设计的。数据主要用于减少维数的目的。这些算法中的大多数通常至少具有相对于顶点数量的二次时间复杂度，因此当它们应用于大规模网络时，可伸缩性是主要问题。

自2008年以来，重要的研究工作转向开发直接为复杂信息网络设计的有效和可伸缩的表示法学习技术。许多 已经提出了NRL算法，例如[6]、[7]、[8]、[9]来嵌入现有网络，在各种应用中显示出了良好的性能。这些算法将网络嵌入到一个潜在的、低端的网络中。 保留结构邻近性和属性亲和力的空间，使得网络的原始顶点可以表示为低维向量。结果是小巧的，低矮的 然后，向量表示可以作为任何基于向量的机器学习算法的特征。这为范围广泛的网络分析任务的方便和高效铺平了道路。 在新的向量空间中处理，例如节点分类[10]、[11]、链接预测[12]、[13]、聚类[2]、建议[14]、[15]、相似搜索[16]和可视化[17]。使用VE 表示复杂网络的ctor表示已逐渐发展到许多其他领域，如城市计算中的兴趣点推荐[15]和知识图搜索[1]。 [8]知识工程和数据库系统。

### 1.1 挑战

尽管具有巨大的潜力，但网络表示学习本质上是困难的，并且面临着我们总结如下的几个关键挑战。

**结构保持**：为了学习信息顶点表示，网络表示学习应该保持网络结构，这样，在原始结构空间中彼此相似/接近的顶点就应该保持在原来的结构空间中。 也可以在学习的向量空间中类似地表示。然而，正如[19]，[20]所述，顶点之间的结构层次相似性不仅反映在局部邻域结构上，而且反映在更全局的社区结构上。因此， 在网络表示学习中，应同时保持局部结构和全局结构。

**内容保存**：除了结构信息外，许多网络的顶点都在属性上附加了丰富的内容。顶点属性不仅对网络的形成有着巨大的影响，而且为度量顶点之间的属性级相似性提供了直接的依据。因此，如果导入得当，属性内容可以补偿网络结构，以呈现更多的信息顶点表示。然而，由于这两个信息源的异质性， 如何有效地利用顶点属性，使其补偿而不是恶化网络结构是一个开放的研究问题。

**数据稀疏性**：对于许多现实世界的信息网络来说，由于隐私或法律的限制，网络结构和顶点内容都存在数据稀疏的问题。在结构层，有时只观察到非常有限的链接，因此很难发现没有显式连接的顶点之间的结构级相关性。在顶点内容层次上，往往缺少许多顶点属性值，这增加了度量内容级顶点相似度的难度。因此，网络重组是一项具有挑战性的工作。 学习演示以克服数据稀疏问题。

**可伸缩性**：现实世界的网络，特别是社交网络，由数以百万或数十亿计的顶点组成。大规模的网络不仅挑战传统的网络分析任务，而且还挑战新生儿网络表示学习任务。特别关注的是，学习具有有限计算资源的大规模网络的顶点表示可能需要数月的时间，这实际上是不可行的， 特别是对于涉及调整参数的大量路径的情况。 因此，有必要设计能够有效学习顶点表示的NRL算法，同时保证大规模网络的有效性。

### 1.2我们的贡献

这项调查提供了一个全面的，最新的网络表示法学习技术，重点是学习顶点表示。它不仅涵盖了早期维护网络结构的工作，而且还涵盖了最近将顶点内容和/或顶点标签作为辅助信息纳入学习的最新研究热潮,网络嵌入过程。希望能为研究界更好地理解(1)网络表征学习方法的新分类提供有益的指导，(2)网络表征学习方法的特点和独特性， 以及不同类型的网络嵌入方法的生态位，以及(3)资源和未来的挑战，以刺激该领域的研究。特别是，这项调查有四大贡献：

- 我们提出了新的分类法，根据潜在的学习机制、打算保存的网络信息以及算法设计和方法，对现有的网络表示学习技术进行分类。因此，这项调查为更好地了解现有工作提供了新的角度。
- 我们对最先进的网络表示学习算法进行了详细而深入的研究.与现有的图形嵌入调查相比，我们不仅审查了一个更全面的问题。 一组关于网络表示学习的研究工作，同时也为理解不同算法的优缺点提供了多方面的算法视角。
- 我们总结了用于验证网络表示学习技术的评估协议，包括已发布的基准数据集、评估方法和开源算法。我们还进行了实证研究，比较了代表性算法的性能，并详细分析了计算复杂性。
- 为了促进未来的研究，我们提出了未来网络表示学习的六个有前途的研究方向，总结了当前研究工作的局限性，并提出了新的研究思路对每个方向。

### 1.3 相关调查和差异

在最近的文献中，有一些与图嵌入和表征学习相关的调查。第一个是[21]，它回顾了一些有代表性的网络表示学习方法，并围绕表征学习的概念及其与其他相关领域的联系（如降维，深度学习和网络科学）访问了一些关键概念。 [22]从方法论角度对代表性网络嵌入算法进行了分类。 [23]回顾了一些表示学习方法，用于在编码器 - 解码器框架内嵌入单个顶点和子图，特别是那些受深度学习启发的子图。 然而，这些调查所审查的大多数嵌入式算法主要保留了净工作结构。 最近，[24]，[25]扩展到涵盖利用其他辅助信息（如顶点属性和/或顶点标签）来利用表示学习的工作。

总之，现有的调查有以下局限性。首先，他们通常只关注一个分类来对现有的工作进行分类。它们都没有提供一个多方面的视角来分析最先进的网络表示学习技术，并比较它们的优缺点。第二，现有的调查没有 没有深入分析算法的复杂性和优化方法，或者它们没有提供实证结果来比较不同算法的性能。第三，缺乏对现有资源的总结，如公开可用的数据集和开源算法，以便利今后的研究。在这项工作中，我们提供了最全面的调查，以弥补差距。我们相信这项调查对研究人员和从业者都有好处，使他们对不同的研究方法有更深入的了解。 并提供丰富的资源，以促进未来在该领域的研究。

### 1.4 综述的结构

本调查的其余部分组织如下。在第二节中，我们提供了理解问题和接下来讨论的模型所需的预备和定义。第三节提出新的分类单元。 对现有的网络表示学习技术进行分类。第4节和第5节分别回顾了两类有代表性的算法。第六节讨论了网络表示学习的成功应用。在第七节中，我们总结了用于验证网络表示学习的评估协议， 并对算法的性能和复杂度进行了比较。我们在第8节讨论了潜在的研究方向，并在第9节总结了调查结果。

## 2 符号和定义

在本节中，作为预备，我们首先定义用于讨论模型的重要术语，然后是网络表示学习问题的正式定义。为了便于表示，我们首先定义了将在整个调查过程中使用的通用符号列表，如表1所示。

|           G            |      给定信息网络      |
| :--------------------: | :--------------------: |
|         **V**          | 给定信息网络中的顶点集 |
|         **E**          |  给定信息网络中的边集  |
|       **\|V\|**        |         顶点数         |
|       **\|E\|**        |          边数          |
|           m            |       顶点属性数       |
|           d            |   学习顶点表示的维数   |
|  $\ X\in R^{\|V\|* m}$    |      顶点属性矩阵      |
|           Y            |       顶点标签集       |
|         \|Y\|          |       顶点标签数       |
| $\ Y\in R^{ \|V\| * \|Y\|}$ |      顶点标号矩阵      |

定义1(信息网络)：一个信息网络被定义为G=(V，E，X，Y)，其中V表示一组顶点，|V|=G中的顶点数。E⊆(V×V)表示连接Ver的一组边。 $\ X∈R^{| V |×m}$ 是顶点属性矩阵，其中m是属性的数量，元素X<sub>ij</sub>是第j个属性的第i个顶点的值。$\  Y∈R^{| V |×| Y |}$ 是顶点标签矩阵，Y是一组标签。 如果第i个顶点具有第k个标签，元素$\ Y_{ik} = 1$; 否则，$\ Y_{ik} $= -1。 由于隐私问题或信息访问困难，顶点属性矩阵X通常是稀疏的，并且顶点标签矩阵Y通常是未观察到的或部分观察到的。 对于每个  $\ \(v_i，v_j）∈E$，如果信息网络G是无向的，我们有$\ \(v_j，v_i）∈E$; 如果G被定向，则$\ \(v_j，v_i)$ 不必要地属于E.如果信息网络是二进制的（未加权的），则每个边缘$\ \(v_i，v_j）∈E$也与权重$\ W_{ij}$相关联，其等于1。

从直觉上看，信息网络的产生不是毫无根据的，而是由某些潜在机制引导或支配的。其潜在机制虽然鲜为人知，但可以从信息网络中广泛存在的一些网络属性中反映出来。因此，公共网络属性对于学习顶点表示是必不可少的，这些顶点表示可以提供信息，以准确地解释信息网络。下面，我们介绍了几种常见的网络属性。

定义2(一阶邻近)。一阶邻近是两个连通顶点之间的局部两两邻近[1]。对于每个顶点对 $\ \(v_i，v_j)$，如果 $\ \(v_i，v_j)∈E$，则$\ v_i和v_j$之间的一阶邻近度为$\ w_{ij}$；否则，$\ v_i$ 和 $\ v_j$ 之间的一阶邻近度为0。一阶邻近捕获顶点之间的直接邻居关系。

定义3(二阶邻近和高阶接近)：二阶邻近捕获了每对顶点之间的两步关系[1]。对于每个顶点对(vi，vj)，二阶邻近度取决于公共邻居的数目。 由两个顶点共享，也可以通过从vi到vj的两步跃迁概率进行等效度量。与二阶邻近相比，高阶邻近[26]捕捉到了更多的全局结构，揭示了每一对顶点之间的k阶(k≥3)关系。对于每个顶点对(Vi) ，高阶邻近度由k阶(k≥3)从顶点vi到顶点vj的跃迁概率来度量，这也可以由从vi到vj的k步(k≥3)路径数来反映。 二阶和高阶近邻捕获了具有相似结构上下文的一对间接连接的顶点之间的相似性。

定义4(结构作用邻近)。结构角色近邻描述了顶点之间的相似性，如链的边缘、恒星的中心和两个社区之间的桥梁等，在它们的邻域中充当着相似的角色。在通信和交通网络中，顶点的结构角色是表征其性质的重要因素。不同的是，一阶、二阶和高阶接近捕获了网络中彼此接近的顶点之间的相似性，而结构角色接近则试图去发现它们之间的相似性。 r在共享等价结构角色的同时，远距离顶点之间的相似性。如图1所示，顶点4和顶点12彼此相距很远，而它们作为恒星的中心扮演着同样的结构角色。因此，它们在结构上具有高度的接近性。

定义5(社区内邻近性)。社区内邻近是同一社区中顶点之间的一对一接近。许多网络都有社区结构，其中同一社区内的顶点-顶点连接是密集的，但与社区外顶点的连接是稀疏的[27]。作为集群结构，社区保留了其中某些顶点的公共性质。例如，在社会网络中，社区可以通过兴趣或背景来代表社会群体；在引文网络中，社区可能代表关于同一主题的相关论文。社区内的邻近性通过保留同一社区中的顶点共享的公共属性来捕获这种集群结构[28]。

顶点属性：除了网络结构外，顶点属性还可以为度量顶点之间的内容级相似性提供直接证据。如[7]、[20]、[29]所示，顶点属性和网络结构可以互相帮助滤除噪声信息，并相互补偿，共同学习信息顶点表示。

顶点标签：顶点标签向某些类或组提供关于每个网络顶点的语义分类的直接信息。顶点标签受到网络结构和顶点属性的强烈影响和内在关联[30]。虽然顶点标签通常是部分观察到的，但是当与网络结构和顶点属性相结合时，它们会鼓励网络结构和顶点属性一致的标记。并帮助学习信息丰富和区分性的顶点表示。



> ![1542019744893](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542019744893.png)
>
> 图1.结构角色邻近性的一个说明性的例子。顶点4和顶点12具有相似的结构作用，但彼此相距很远。

定义6(网络表示学习(NRL)。给定一个信息网络G =（V，E，X，Y），通过整合E中的网络结构，X中的顶点属性和Y中的顶点标签（如果可用），网络表示学习的任务是学习映射函数 $\ f ：v →r_v∈R^d$其中$r_v$是顶点v的学习矢量表示，d是学习表示的维度。 变换f预先提供原始网络信息，使得在原始网络中类似的两个顶点也应该在学习矢量空间中类似地表示。

学习的顶点表示应满足以下条件：(1)低维，即d_x~(-v)，换句话说，学习顶点表示的维数要小得多。 在原有的邻接矩阵表示的维数上，计算了内存效率和后续网络分析任务的可扩展性；

(2)信息表示，即学习的顶点表示应保持由网络结构、顶点属性和顶点标签(如果有的话)所反映的顶点邻近性；

(3)连续的，即学习的顶点表示应该具有连续的实数，以支持后续的网络分析任务，如顶点分类、顶点聚类或异常检测。 并具有平滑的决策边界，以确保这些任务的鲁棒性。



> ![1542019955651](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542019955651.png)
>
> 图2.网络表示学习的概念观。(A)中的顶点使用基于其社区信息的ID和颜色进行索引。网络表示学习 (B)将所有顶点转化为一个二维向量空间，从而使结构接近的顶点在新的嵌入空间中彼此接近。

图2示出了使用玩具网络的网络表示学习的概念视图。在这种情况下，只考虑网络结构来学习顶点表示。提供信息 网络如图所示。2(A)，NRL的目标是将所有网络顶点嵌入到低维空间中，如图所示。2(B)。在嵌入空间中，具有结构邻近点的顶点彼此间有着紧密的表示。例如，由于顶点7和顶点8是直接连接的，一阶邻近使得它们在嵌入空间中彼此紧密相连。虽然顶点2和顶点5不是直接连接的，但由于它们具有很高的二阶邻近性，所以它们之间也是紧密地嵌入在一起的。 这两个顶点。顶点20和顶点25不直接相连，也不共享共同的直接邻居。然而，它们是由许多k阶路径(k≥3)连接起来的，这证明了它们具有高阶邻近性.因此，顶点20和顶点25也具有紧密的嵌入。与其他顶点不同，顶点10-16显然属于原始网络中的同一个社区。这种社区内的邻近性保证了这些顶点的图像在嵌入空间中也显示出清晰的簇结构。

> ![1542020140572](https://github.com/mp5088643/Machine-learning-and-data-science-notebook/blob/master/images/网络表示学习/%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542020140572.png)
>
> 图3.我们将网络表示学习分为两类：无监督网络表示学习和半监督网络表示学习，这取决于顶点标签是否是。 可供学习。对于每一组，我们进一步将方法划分为两个子组，这取决于表示学习是基于网络拓扑结构，还是基于节点的信息， 内容.

## 3 范畴化

在本节中，我们提出了一个新的分类法来对现有的网络表示学习技术进行分类，如图3所示。分类的第一层是基于 为学习提供了顶点标签。分类的第一层是基于是否为学习提供顶点标签。根据这一点，我们将网络表示学习分为两类：无监督网络表示学习和半监督网络表示学习。

无监督网络表示学习：在此设置中，没有为学习顶点表示提供标记顶点。因此，网络表示学习被认为是一项独立于后续学习的通用任务，顶点表示是以无监督的方式学习的。

大多数现有的NRL算法都属于这一类。在新的嵌入空间中学习顶点表示后，将它们作为基于向量的各种学习任务算法的特征。根据可供学习的网络信息类型，非监督NRL算法还可进一步分为两个子组：只保留网的无监督结构保持方法。 工作结构和无监督内容增强方法，这些方法结合顶点属性和网络结构来学习联合顶点嵌入。

半监督网络表示学习。在这种情况下，存在一些标记点用于表示学习。因为顶点标签在确定每个顶点的分类中对于网络结构和顶点属性具有强相关性起着至关重要的作用，所以提出了半监督网络表示学习以利用网络中可用的顶点标签来寻求更有效的联合矢量表示。

在这种情况下，网络表示学习与监督学习任务(如顶点分类)相结合。为了同时优化顶点表示的学习和网络顶点的分类，常提出一个统一的目标函数。因此，对于不同的类别，学习的顶点表示可以是信息性的，也可以是区分性的。半监督NRL算法也可分为两大类：半监督结构保持法和半监督内容增广法。

> ![1542022260814](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542020140572.png)
> 表2.按用于学习的信息源分列的NRL算法摘要

表2根据用于表示学习的信息源总结了所有NRL算法。一般来说，信息源主要有三种类型：网络结构、顶点属性和顶点标签。大多数无监督的NRL算法专注于保留用于学习顶点表示的网络结构，并且仅少数算法（例如，TADW [7]，HSCA [8]）尝试利用顶点属性。相反，在半监督学习设置下，有一半的算法打算将顶点属性与网络结构和顶点标签相结合来学习顶点表示。在这两种设置中，大多数算法都集中于保留微观结构，而极少数算法（例如，MNMF [28]，DP [41]，HARP [42]）试图利用介观和宏观结构。

从算法的角度，可以将上述两种不同环境下的网络表示学习方法归纳为五类。

1)基于矩阵分解的方法。基于矩阵分解的方法以矩阵的形式表示网络顶点之间的联系，并利用矩阵因式分解来获得嵌入。为了保持网络结构，构造了不同类型的矩阵，如k阶转移概率矩阵、模块化矩阵或顶点上下文矩阵[7]。通过假设这种高维顶点表示只受少量潜在因素的影响，利用矩阵因式分解来嵌入高维顶点表示。 变成一个潜在的、低维的结构保持空间。因子化策略根据其目标在不同算法之间变化。例如，在模块化最大化方法[31]中，特征分解是在模块矩阵上进行的，以学习社区指示顶点表示[53]。在TADW算法[7]中，归纳矩阵分解[54]是在顶点表示学习中对顶点上下文矩阵同时保持顶点文本特征和网络结构的一种方法。尽管基于矩阵分解的方法在学习信息顶点表示方面已被证明是有效的，可伸缩性是一个主要的瓶颈，因为在一个包含数百万行和列的矩阵上进行因式分解是内存密集型的，计算成本很高，有时甚至是不可行的。 

2)基于随机游走的方法。对于可伸缩的顶点表示学习，利用随机游走来捕获顶点之间的结构关系。通过执行截断的随机游走，信息网络被转换为顶点序列的集合，其中顶点 - 上下文对的出现频率测量它们之间的结构距离。借鉴词表示学习的思想[55]，[56]，通过使用每个顶点来预测其上下文来学习顶点表示。DeepWalk[6]是利用随机游动学习顶点表示的先驱工作。node2vec[34]进一步利用有偏随机游走策略来捕获更灵活的上下文结构。

作为仅保留版本结构的扩展，DDRW[45]、gene[48]和SemNE[49]等算法将顶点标签与网络结构结合起来，以利用表示学习，P PNE[44]导入顶点属性，Tri-DNR[50]使用顶点标签和属性强制执行模型。由于这些模型可以通过在线方式进行培训，它们具有很大的扩展潜力。

> ![1542022302610](https://github.com/mp5088643/Machine-learning-and-data-science-notebook/blob/master/images/网络表示学习/1542022302610.png)
>
> NRL算法的方法论分类

3)基于边缘建模的方法。与利用矩阵或随机游走捕捉网络结构的方法不同，基于边缘建模的方法直接从顶点-顶点连接中学习顶点表示。为了捕获一阶和二阶邻近，LINE [1]分别对连接的顶点建模联合概率分布和条件概率分布。为了学习链接文档的表示，LDE[51]通过最大化连接文档之间的条件概率来建模文档-文档关系。pRBM[29]通过使连通顶点的隐式RBM表示类似于彼此，从而使RBM[57]模型适应于链接数据。图GAN[37]采用生成对抗性网(GaN)[58] 精确建模顶点连通概率。与基于矩阵分解和随机游走的方法相比，基于边缘建模的方法效率更高。这些方法不能捕获全局网络结构，因为它们只考虑可观测的顶点连通性信息。

4)基于深度学习的方法。为了提取复杂的结构特征和学习深度、高度非线性的顶点表示，还将深度学习技术[59]、[60]应用于网络表示学习。例如，DNGR[9]将叠加去噪自动编码器(SDAE)[60]应用于高维矩阵表示，以学习深度低维顶点表示。SDNE[19]使用半实物 -监督深度自编码模型[59]在网络结构中建立非线性模型。基于深度学习的方法具有捕捉网络非线性的能力，但计算时间往往较长。传统的深度学习架构是为一维，二维， 或者三维欧氏结构化数据，但是需要在非欧几里德结构化数据(如图)上开发有效的解决方案。

5)杂交方法。其他一些方法利用上述方法的混合来学习顶点表示。例如，DP[41]用度惩罚原理增强了谱嵌入[5]和DeepWalk[6]以保持宏观无尺度性质。HARP [42]利用基于随机游走的方法（DeepWalk [6]和node2vec [34]）和基于边缘建模的方法（LINE [1]）来学习从小型采样网络到原始网络的顶点表示。

我们总结了五种网络表示学习技术，并在表3中比较了它们的优缺点。
