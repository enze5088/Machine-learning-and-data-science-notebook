



# 网络表示学习：综述

Daokun Zhang, Jie Yin, Xingquan Zhu Senior Member, IEEE, Chengqi Zhang Senior Member, IEEE

#### 摘要

随着信息技术的广泛使用，信息网络越来越受欢迎，以捕捉各种学科之间的复杂关系，例如社交网络，引文网络，电信网络和生物网络。分析这些网络揭示了社会生活的不同方面，例如社会结构，信息传播和沟通模式。然而，实际上，大规模的信息网络经常使网络分析任务在计算上昂贵或难以处理。最近已经提出网络表示学习作为通过保留网络拓扑结构，顶点内容和其他辅助信息将网络顶点嵌入到低维向量空间中的新学习范例。这有助于在新的向量空间中容易地处理原始网络以进行进一步分析。在本次调查中，我们对数据挖掘和机器学习领域的网络表示学习的当前文献进行了全面的回顾。我们提出新的分类法，根据潜在的学习机制，旨在保留的网络信息以及算法设计和方法，对最先进的网络表示学习技术进行分类和总结。我们总结了用于验证网络表示学习的评估协议，包括已发布的基准数据集，评估方法和开源算法。我们还进行了实证研究，以比较代表性算法在常见数据集上的性能，并分析其计算复杂性。最后，我们建议有前途的研究方向，以促进未来的研究。

**索引术语** - 信息网络，图挖掘，网络表示学习，网络嵌入。



## 1 简介

信息网络在社交网络，引文网络，电信网络和生物网络等形式的各种实际应用中变得无处不在。这些网络的规模范围从数百到数百万甚至数十亿个顶点[1]。 分析信息网络在许多学科的各种新兴应用中起着至关重要的作用。例如，在社交网络中，对于许多重要任务而言，将用户分类为有意义的社交群组是非常有帮助的，例如用户搜索，有针对性的广告和推荐; 在通信网络中，检测社区结构有助于更好地理解谣言传播过程; 在生物网络中，推断蛋白质之间的相互作用可以促进疾病的新治疗。 然而，对这些网络的有效分析在很大程度上依赖于网络的表示方式。通常，离散邻接矩阵用于表示网络，其仅捕获顶点之间的相邻关系。 实际上，这种简单的表示不能体现更复杂，更高阶的结构关系，例如路径，频繁的子结构等。结果，这样的传统例程经常使得许多网络分析任务在大规模网络上计算上昂贵且难以处理。 以社区检测为例，大多数现有算法涉及计算矩阵[2]的谱分解，其相对于顶点数至少具有二次时间复杂度。 这种计算开销使得算法很难扩展到具有数百万个顶点的大规模网络。

近年来，网络表示学习(NRL)引起了人们广泛的研究兴趣。NRL旨在学习潜在的、低维的网络顶点表示，同时保持网络拓扑结构、顶点内容和其他侧信息。在学习了新的顶点表示之后，通将传统的基于向量的机器学习算法应用到新的表示空间中，可以方便有效地完成网络分析任务。这就省去了直接应用于原始网络的复杂算法的必要性。

早期与网络表示学习有关的工作可以追溯到本世纪初，当时研究人员提出了作为降维技术的一部分的图嵌入算法。在给定一组独立的、同分布的数据点作为输入的情况下，图嵌入算法首先计算两两数据点之间的相似度，构造一个亲和图，例如k-最近邻图，然后将亲和图嵌入到低维空间中。其思想是在构造的图形所反映的高维数据几何中找到一个隐藏在高维数据几何中的低维流形结构，从而使连通点在新的嵌入空间中保持更接近。Isomap，局部线性嵌入（LLE）[4]和拉普拉斯算子图[5]是基于这个基本原理的算法实例。然而，图嵌入算法是在i.i.d.数据上设计的主要用于降维。这些算法中的大多数通常至少具有相对于顶点数量的二次时间复杂度，因此当它们应用于大规模网络时，可伸缩性是主要问题。

自2008年以来，重要的研究工作转向开发直接为复杂信息网络设计的有效和可伸缩的表示法学习技术。许多 已经提出了NRL算法，例如[6]、[7]、[8]、[9]来嵌入现有网络，在各种应用中显示出了良好的性能。这些算法将网络嵌入到一个潜在的、低维的网络中。 保留结构的邻近性和属性亲和力的空间，使得网络的原始顶点可以表示为低维向量。由此产生的紧凑的、低维的向量表示可以作为任何基于向量的机器学习算法的特征。这为在新的向量空间中轻松有效地处理各种网络分析任务铺平了道路，如节点分类[10]，[11]，链路预测[12]，[13]，聚类[2]，推荐[14]，[15]，相似搜索[16]，可视化[17]。利用向量表示来表示复杂的网络，现在已经逐渐发展到许多其他领域，如城市计算中的兴趣点推荐[15]和知识工程和数据库系统中的知识图谱搜索[18]。

### 1.1 挑战

尽管具有巨大的潜力，但网络表示学习本质上是困难的，并且面临着我们总结如下的几个关键挑战。

**结构保持**：为了学习信息丰富的顶点表示，网络表示学习应该保持网络结构，这样，在原始结构空间中彼此相似/接近的顶点就应该保持在原来的结构空间中。 在学习的向量空间中也可以类似地表示。然而，正如[19]，[20]所述，顶点之间的结构层次相似性不仅反映在局部邻域结构上，而且反映在更全局的社区结构上。因此， 在网络表示学习中，应同时保持局部结构和全局结构。

**内容保存**：除了结构信息外，许多网络的顶点都在属性上附加了丰富的内容。顶点属性不仅对网络的形成有着巨大的影响，而且为度量顶点之间的属性级相似性提供了直接的依据。因此，如果导入得当，属性内容可以补偿网络结构，以呈现更多的信息顶点表示。然而，由于这两个信息源的异质性， 如何有效地利用顶点属性使其补偿而不是恶化网络结构是一个开放的研究问题。

**数据稀疏性**：对于许多现实世界的信息网络来说，由于隐私或法律的限制，网络结构和顶点内容都存在数据稀疏的问题。在结构层，有时只观察到非常有限的链接，因此很难发现没有显式连接的顶点之间的结构级相关性。在顶点内容层次上，往往缺少许多顶点属性值，这增加了度量内容级顶点相似度的难度。因此，网络重组是一项具有挑战性的工作。 学习演示以克服数据稀疏问题。

**可伸缩性**：现实世界的网络，特别是社交网络，由数以百万或数十亿计的顶点组成。大规模的网络不仅挑战传统的网络分析任务，而且还挑战新生的网络表示学习任务。如果没有特别考虑，对于计算资源有限的大型网络，学习顶点表示可能会花费数月的时间，这实际上是不可行的，特别是对于涉及大量跟踪调优参数的情况。 因此，有必要设计能够有效学习顶点表示的NRL算法，同时保证大规模网络的有效性。

### 1.2我们的贡献

这项调查提供了一个全面的回顾了最新的网络表示法学习技术，重点是学习顶点表示。它不仅涵盖了早期维护网络结构的工作，而且还涵盖了最近将顶点内容和/或顶点标签作为辅助信息纳入学习的最新研究热潮,网络嵌入过程。希望能为研究界更好地理解(1)网络表征学习方法的新分类提供有益的指导，(2)网络表征学习方法的特点和独特性， 以及不同类型的网络嵌入方法的生态位，以及(3)资源和未来的挑战，以刺激该领域的研究。特别是，这项调查有四大贡献：

- 我们提出了新的分类法，根据潜在的学习机制、打算保存的网络信息以及算法设计和方法，对现有的网络表示学习技术进行分类。因此，这项调查为更好地了解现有工作提供了新的角度。
- 我们对最先进的网络表示学习算法进行了详细而深入的研究.与现有的图形嵌入调查相比，我们不仅审查了一个更全面的问题。 一组关于网络表示学习的研究工作，同时也为理解不同算法的优缺点提供了多方面的算法视角。
- 我们总结了用于验证网络表示学习技术的评估协议，包括已发布的基准数据集、评估方法和开源算法。我们还进行了实证研究，比较了代表性算法的性能，并详细分析了计算复杂性。
- 为了促进未来的研究，我们提出了未来网络表示学习的六个有前途的研究方向，总结了当前研究工作的局限性，并提出了新的研究思路对每个方向。

### 1.3 相关调查和差异

在最近的文献中，有一些与图嵌入和表示学习相关的调查。第一个是[21]，它回顾了一些有代表性的网络表示学习方法，并围绕表征学习的概念及其与其他相关领域的联系（如降维，深度学习和网络科学）访问了一些关键概念。 [22]从方法论角度对代表性网络嵌入算法进行了分类。 [23]回顾了一些表示学习方法，用于在编码器 - 解码器框架内嵌入单个顶点和子图，特别是那些受深度学习启发的子图。 然而，这些调查所审查的大多数嵌入式算法主要保留了净工作结构。 最近，[24]，[25]扩展到涵盖利用其他辅助信息（如顶点属性和/或顶点标签）来利用表示学习的工作。

总之，现有的调查有以下局限性。首先，他们通常只关注一个分类来对现有的工作进行分类。它们都没有提供一个多方面的视角来分析最先进的网络表示学习技术，并比较它们的优缺点。第二，现有的调查没有 没有深入分析算法的复杂性和优化方法，或者它们没有提供实证结果来比较不同算法的性能。第三，缺乏对现有资源的总结，如公开可用的数据集和开源算法，以便利今后的研究。在这项工作中，我们提供了最全面的调查，以弥补差距。我们相信这项调查对研究人员和从业者都有好处，使他们对不同的研究方法有更深入的了解。 并提供丰富的资源，以促进未来在该领域的研究。

### 1.4 综述的结构

本调查的其余部分组织如下。在第二节中，我们提供了理解问题和接下来讨论的模型所需的预备和定义。第三节提出新的分类单元。 对现有的网络表示学习技术进行分类。第4节和第5节分别回顾了两类有代表性的算法。第六节讨论了网络表示学习的成功应用。在第七节中，我们总结了用于验证网络表示学习的评估协议， 并对算法的性能和复杂度进行了比较。我们在第8节讨论了潜在的研究方向，并在第9节总结了调查结果。

## 2 符号和定义

在本节中，作为预备，我们首先定义用于讨论模型的重要术语，然后是网络表示学习问题的正式定义。为了便于表示，我们首先定义了将在整个调查过程中使用的通用符号列表，如表1所示。

|           G            |      给定信息网络      |
| :--------------------: | :--------------------: |
|         **V**          | 给定信息网络中的顶点集 |
|         **E**          |  给定信息网络中的边集  |
|       **\|V\|**        |         顶点数         |
|       **\|E\|**        |          边数          |
|           m            |       顶点属性数       |
|           d            |   学习顶点表示的维数   |
|  $\ X\in R^{\|V\|* m}$    |      顶点属性矩阵      |
|           Y            |       顶点标签集       |
|         \|Y\|          |       顶点标签数       |
| $\ Y\in R^{ \|V\| * \|Y\|}$ |      顶点标号矩阵      |

**定义1(信息网络)**：一个信息网络被定义为G=(V，E，X，Y)，其中V表示一组顶点，|V|=G中的顶点数。E⊆(V×V)表示连接顶点的一组边。 $\ X∈R^{| V |×m}$ 是顶点属性矩阵，其中m是属性的数量，元素X<sub>ij</sub>是第j个属性的第i个顶点的值。$\  Y∈R^{| V |×| Y |}$ 是顶点标签矩阵，Y是一组标签。 如果第i个顶点具有第k个标签，元素$\ Y_{ik} = 1$; 否则，$\ Y_{ik} $= -1。 由于隐私问题或信息访问困难，顶点属性矩阵X通常是稀疏的，并且顶点标签矩阵Y通常是未观察到的或部分观察到的。 对于每个  $\ \(v_i，v_j）∈E$，如果信息网络G是无向的，我们有$\ \(v_j，v_i）∈E$; 如果G是有向的，则$\ \(v_j，v_i)$ 不一定属于E.如果信息网络是二进制的（未加权的），则每个边缘$\ \(v_i，v_j）∈E$也与权重$\ W_{ij}$相关联，其等于1。

从直觉上看，信息网络的产生不是毫无根据的，而是由某些潜在机制引导或支配的。其潜在机制虽然鲜为人知，但可以从信息网络中广泛存在的一些网络属性中反映出来。因此，公共网络属性对于学习顶点表示是必不可少的，这些顶点表示可以提供信息，以准确地解释信息网络。下面，我们介绍了几种常见的网络属性。

**定义2(一阶邻近)**：一阶邻近是两个连通顶点之间的局部两两邻近[1]。对于每个顶点对 $ \(v_i，v_j \)$，如果 $\ \(v_i，v_j)∈E$，则$\ v_i和v_j$之间的一阶邻近度为$\ w_{ij}$；否则，$\ v_i$ 和 $\ v_j$ 之间的一阶邻近度为0。一阶邻近捕获顶点之间的直接邻居关系。

**定义3(二阶邻近和高阶接近)**：二阶邻近捕获了每对顶点之间的两步关系[1]。对于每个顶点对($v_i$，$v_j$)，二阶邻近度取决于公共邻居的数目。 由两个顶点共享，也可以通过从$ v_i$到$v_j$的两步跃迁概率进行等效度量。与二阶邻近相比，高阶邻近[26]捕捉到了更多的全局结构，揭示了每一对顶点之间的k阶(k≥3)关系。对于每个顶点对($V_i$) ，高阶邻近度由k阶(k≥3)从顶点$v_i$到顶点$v_j$的跃迁概率来度量，这也可以由从$v_i$到$ v_j$的k步(k≥3)路径数来反映。 二阶和高阶近邻捕获了具有相似结构上下文的一对间接连接的顶点之间的相似性。

**定义4(结构作用邻近性)**：结构角色近邻描述了顶点之间的相似性，如链的边缘、恒星的中心和两个社区之间的桥梁等，在它们的邻域中充当着相似的角色。在通信和交通网络中，顶点的结构作用是表征其性质的重要因素。不同于一阶,二阶和高阶近邻,捕捉的相似性网络中顶点相互接近,结构作用接近试图发现遥远的顶点之间的相似性而共享等效结构的角色。如图1所示，顶点4和顶点12彼此相距很远，而它们作为恒星的中心扮演着同样的结构角色。因此，它们在结构上具有高度的接近性。

**定义5(社区内邻近性)**：内聚邻近度是同一社区内顶点之间的两两接近度。许多网络具有社区结构，其中同一社区内的顶点-顶点连接是密集的，但是社区外的顶点连接是稀疏的[27]。作为集群结构，一个社区保留了它内部顶点的某些公共属性。例如，在社会网络中，社区可能代表社会群体的利益或背景;在引文网络中，社区可能代表同一主题的相关论文。社区内的邻近性通过保持同一个社区[28]中顶点共享的公共属性来获取这种集群结构。

**顶点属性：**除了网络结构外，顶点属性还可以为度量顶点之间的内容级相似性提供直接证据。如[7]、[20]、[29]所示，顶点属性和网络结构可以互相帮助滤除噪声信息，并相互补偿，共同学习信息顶点表示。

**顶点标签：**顶点标签向某些类或组提供关于每个网络顶点的语义分类的直接信息。顶点标签受到网络结构和顶点属性的强烈影响和内在关联[30]。虽然顶点标签通常是部分观察到的，但是当与网络结构和顶点属性相结合时，它们会鼓励网络结构和顶点属性一致的标记。并帮助学习信息丰富和区分性的顶点表示。



> ![1542019744893](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542019744893.png)
>
> 图1.结构角色邻近性的一个说明性的例子。顶点4和顶点12具有相似的结构作用，但彼此相距很远。

**定义6(网络表示学习(NRL)**：给定一个信息网络G =（V，E，X，Y），通过整合E中的网络结构，X中的顶点属性和Y中的顶点标签（如果可用），网络表示学习的任务是学习映射函数 $\ f ：v →r_v∈R^d$其中$r_v$是顶点v的学习矢量表示，d是学习表示的维度。 变换f预先提供原始网络信息，使得在原始网络中类似的两个顶点也应该在学习矢量空间中类似地表示。

学习的顶点表示应满足以下条件：

(1)低维，即$d<<|V|$，换句话说，为了提高内存效率和后续网络分析任务的可扩展性，学习顶点表示的维数要比原始邻接矩阵表示的维数小得多;

(2)信息表示，即学习的顶点表示应保持由网络结构、顶点属性和顶点标签(如果有的话)所反映的顶点邻近性；

(3)连续的，即学习的顶点表示应该具有连续的实数，以支持后续的网络分析任务，如顶点分类、顶点聚类或异常检测, 并具有平滑的决策边界，以确保这些任务的鲁棒性。



> ![1542019955651](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542019955651.png)
>
> 图2.网络表示学习的概念观。(A)中的顶点使用基于其社区信息的ID和颜色进行索引。网络表示学习 (B)将所有顶点转化为一个二维向量空间，从而使结构接近的顶点在新的嵌入空间中彼此接近。

图2示出了使用玩具网络的网络表示学习的概念视图。在这种情况下，只考虑网络结构来学习顶点表示。提供信息 网络如图所示。2(A)，NRL的目标是将所有网络顶点嵌入到低维空间中，如图所示。2(B)。在嵌入空间中，具有结构邻近点的顶点彼此间有着紧密的表示。例如，由于顶点7和顶点8是直接连接的，一阶邻近使得它们在嵌入空间中彼此紧密相连。虽然顶点2和顶点5不是直接连接的，但由于它们具有很高的二阶邻近性，所以它们之间也是紧密地嵌入在一起的。 这两个顶点。顶点20和顶点25不直接相连，也不共享共同的直接邻居。然而，它们是由许多k阶路径(k≥3)连接起来的，这证明了它们具有高阶邻近性.因此，顶点20和顶点25也具有紧密的嵌入。与其他顶点不同，顶点10-16显然属于原始网络中的同一个社区。这种社区内的邻近性保证了这些顶点的图像在嵌入空间中也显示出清晰的簇结构。

> ![1542020140572](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542020140572.png)
>
> 图3.我们将网络表示学习分为两类：无监督网络表示学习和半监督网络表示学习，这取决于顶点标签是否是可供学习。对于每一组，我们进一步将方法划分为两个子组，这取决于表示学习是基于网络拓扑结构，还是基于来自节点内容的信息。

## 3 范畴化

在本节中，我们提出了一个新的分类法来对现有的网络表示学习技术进行分类，如图3所示。分类的第一层是基于是否为学习提供顶点标签。根据这一点，我们将网络表示学习分为两类：无监督网络表示学习和半监督网络表示学习。

**无监督网络表示学习**：在此设置中，没有为学习顶点表示提供标记顶点。因此，网络表示学习被认为是一项独立于后续学习的通用任务，顶点表示是以无监督的方式学习的。

大多数现有的NRL算法都属于这一类。在新的嵌入空间中学习顶点表示后，它们被视为用于各种学习任务的任何基于向量的算法的特征。无监督的NRL算法可以根据可供学习的网络信息类型进一步分为两个子组：只保留网的无监督结构保持方法， 非监督内容增强方法，结合顶点属性和网络结构来学习联合顶点嵌入。

**半监督网络表示学习**：在这种情况下，存在一些用于表示学习的标记顶点。因为顶点标签在确定每个顶点的分类中起着至关重要的作用，并且与网络结构和顶点属性有很强的相关性，提出了利用网络中的顶点标签来寻找更有效的联合向量表示的网络表示学习方法。

在这种情况下，网络表示学习与监督学习任务(如顶点分类)相结合。为了同时优化顶点表示的学习和网络顶点的分类，常提出一个统一的目标函数。因此，对于不同的类别，学习的顶点表示可以是信息性的，也可以是区分性的。半监督NRL算法也可分为两大类：半监督结构保持法和半监督内容增广法。

> ![1542022260814](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/1542022260814.png)
> 表2.按用于学习的信息源分列的NRL算法摘要

表2根据用于表示学习的信息源总结了所有NRL算法。一般来说，信息源主要有三种类型：网络结构、顶点属性和顶点标签。大多数无监督的NRL算法专注于保留用于学习顶点表示的网络结构，并且仅少数算法（例如，TADW [7]，HSCA [8]）尝试利用顶点属性。相反，在半监督学习设置下，有一半的算法打算将顶点属性与网络结构和顶点标签相结合来学习顶点表示。在这两种设置中，大多数算法都集中于保留微观结构，而极少数算法（例如，MNMF [28]，DP [41]，HARP [42]）试图利用介观和宏观结构。

从算法的角度，可以将上述两种不同环境下的网络表示学习方法归纳为五类。

1)基于矩阵分解的方法。基于矩阵分解的方法以矩阵的形式表示网络顶点之间的联系，并利用矩阵因式分解来获得嵌入。为了保持网络结构，构造了不同类型的矩阵，如k阶转移概率矩阵、模块化矩阵或顶点上下文矩阵[7]。通过假设这种高维顶点表示只受少量潜在因素的影响，利用矩阵因式分解来嵌入高维顶点表示。 变成一个潜在的、低维的结构保持空间。因子化策略根据其目标在不同算法之间变化。例如，在模块化最大化方法[31]中，特征分解是在模块矩阵上进行的，以学习社区指示顶点表示[53]。在TADW算法[7]中，归纳矩阵分解[54]是在顶点表示学习中对顶点上下文矩阵同时保持顶点文本特征和网络结构的一种方法。尽管基于矩阵分解的方法在学习信息顶点表示方面已被证明是有效的，可伸缩性是一个主要的瓶颈，因为在一个包含数百万行和列的矩阵上进行因式分解是内存密集型的，计算成本很高，有时甚至是不可行的。 

2)基于随机游走的方法。对于可伸缩的顶点表示学习，利用随机游走来捕获顶点之间的结构关系。通过执行截断的随机游走，信息网络被转换为顶点序列的集合，其中顶点 - 上下文对的出现频率测量它们之间的结构距离。借鉴词表示学习的思想[55]，[56]，通过使用每个顶点来预测其上下文来学习顶点表示。DeepWalk[6]是利用随机游动学习顶点表示的先驱工作。node2vec[34]进一步利用有偏随机游走策略来捕获更灵活的上下文结构。

作为仅保留版本结构的扩展，DDRW[45]、gene[48]和SemNE[49]等算法将顶点标签与网络结构结合起来，以利用表示学习，PPNE[44]导入顶点属性，Tri-DNR[50]使用顶点标签和属性强制执行模型。由于这些模型可以通过在线方式进行培训，它们具有很大的扩展潜力。

> ![1542022302610](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/1542022302610.png)
>
> NRL算法的方法论分类

3)基于边建模的方法。与利用矩阵或随机游走捕捉网络结构的方法不同，基于边建模的方法直接从顶点-顶点连接中学习顶点表示。为了捕获一阶和二阶邻近，LINE [1]分别对连接的顶点建模联合概率分布和条件概率分布。为了学习链接文档的表示，LDE[51]通过最大化连接文档之间的条件概率来建模文档-文档关系。pRBM[29]通过使连通顶点的隐式RBM表示类似于彼此，从而使RBM[57]模型适应于链接数据。图GAN[37]采用生成对抗性网(GAN)[58] 精确建模顶点连通概率。与基于矩阵分解和随机游走的方法相比，基于边建模的方法效率更高。这些方法不能捕获全局网络结构，因为它们只考虑可观测的顶点连通性信息。

4)基于深度学习的方法。为了提取复杂的结构特征和学习深度、高度非线性的顶点表示，还将深度学习技术[59]、[60]应用于网络表示学习。例如，DNGR[9]将叠加去噪自动编码器(SDAE)[60]应用于高维矩阵表示，以学习深度低维顶点表示。SDNE[19]使用半实物 -监督深度自编码模型[59]在网络结构中建立非线性模型。基于深度学习的方法具有捕捉网络非线性的能力，但计算时间往往较长。传统的深度学习架构是为一维，二维， 或者三维欧氏结构化数据，但是需要在非欧几里德结构化数据(如图)上开发有效的解决方案。

5)杂交方法。其他一些方法利用上述方法的混合来学习顶点表示。例如，DP[41]用度惩罚原理增强了谱嵌入[5]和DeepWalk[6]以保持宏观无尺度性质。HARP [42]利用基于随机游走的方法（DeepWalk [6]和node2vec [34]）和基于边缘建模的方法（LINE [1]）来学习从小型采样网络到原始网络的顶点表示。

我们总结了五种网络表示学习技术，并在表3中比较了它们的优缺点。

##4 无监督网络表示学习

在本节中，我们回顾了无监督的网络表示学习方法，将它们分成两个子部分，如图3所示。在此基础上，我们总结了这两种方法的主要特点，并比较了这两种方法的不同之处。

### 4.1 无监督结构保持网络表示学习

结构保持网络表示学习是指打算保留网络结构的方法，在新的嵌入空间中，对于原始网络空间中的顶点彼此相近的概念，应该以相似的方式来表示。在这一类别中，研究工作一直集中在设计各种模型，以尽可能捕获原始网络传递的结构信息。

> ![1542075579686](F:\Machine-learning-and-data-science-notebook\images\网络表示学习\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542075579686.png)
>
> 图4.网络结构分类。

我们将学习顶点表示的网络结构归纳为三种类型：(I)微观结构，包括局部近邻，即一阶、二阶和高阶邻近，(Ii)介观结构，捕捉结构角色邻近性及群落内邻近性。以及(Iii)宏观结构，它捕捉全球网络属性，如无标度属性或小世界属性。如图4所示，下面的小节是根据我们对网络结构的分类来组织的。

#### 4.1.1 保存微观结构的NRL

这类NRL算法旨在保持其邻域中直接或间接连接的顶点之间的局部结构信息，包括一阶，二阶和高阶邻近。一阶接近捕获同质性，即直接连通的顶点趋向于彼此相似，而二阶和高阶近邻捕获了共享公共领域的顶点之间的相似性。大多数保持结构的nrl算法都属于这一类。

**DeepWalk：**DeepWalk[6]将Skip-Gram模型[55][56]的思想推广到学习潜在的顶点表示。 在网络中，通过对自然语言句子和短随机游动序列进行类比。图5给出了DeepWalk的工作流程。给定长度为$\ L，{v_1，v_2，··，v_L}$ 的随机游动序列，遵循Skip-Gram，DeepWalk利用它来预测其上下文顶点，这是通过优化问题来实现的。

$$ \min\limits_{f} -logPr(\{v_{i-t},...，v_{i+t} \} \backslash v_i|f(v_i)  )$$    注：Pr 条件概率

其中$\ \{v_{i−t}，···，v_{i+t} \} \backslash v_i$ 是t窗口大小内顶点$v_i$的上下文顶点。在条件独立性假设下，概率$Pr(\{v_{i-t},...，v_{i+t} \} \backslash v_i|f(v_i) $近似为

$$Pr(\{v_{i-t},...，v_{i+t} \} \backslash v_i|f(v_i) =\prod_{j=i-t,j\not=i}^{i+t} Pr(v_j |f(v_i))$$

按照DeepWalk的学习结构，在随机游动序列中共享相似上下文顶点的顶点应该在新的嵌入空间中得到紧密的表示。考虑到随机游动序列中的上下文顶点描述邻域结构，DeepWalk实际上表示在嵌入空间中共享相似邻域(直接或间接)的顶点，因此，二阶和高阶邻近性被保留下来。

**大规模信息网络嵌入(LINE)：**LINE[1]不是利用随机游走来捕获网络结构，而是通过显式建模一阶和二阶邻近来学习顶点表示。为了保持一阶接近度，直线使下列目标最小化：

$$ O_1 = d(\hat{p1}(·, ·), p1(·, ·)).$$

对于$(v_i，v_j)∈E$的每个顶点对$v_i和v_j，p_1(·，·)$是由它们的潜在嵌入$r_{v_i}$和${r_{v_j}}$建模的联合分布。$\hat{p}_1(vi，vj)$是它们之间的经验分布。d(·，·)是两个分布之间的距离。

为了保持二阶邻近，直线最小化了以下目标：

$O2 =\sum\limits_{vi∈V}λ_id(\hat p_2(·|v_i), p_2(·|v_i))$,

其中$p_2(·|v_i)$是每一个用顶点嵌入建模的$v_i\in V$的上下文条件分布,$\hat p_2(·|v_i)$是经验条件分布，$λ_i$是顶点vi的威望。这里，顶点上下文是由它的邻居确定的，也就是说，对于每个$v_j$，$v_j$是vi的上下文，当且仅当$(v_i，v_j)∈E$。

通过最小化这两个目标，线学习了两种保持一阶和二阶邻近性的顶点表示。并以它们的级联作为最终的顶点表示。

**GraRep**:根据DeepWalk [6]的思想，GraRep [26]扩展了skip-gram模型以捕获高阶邻近度，即共享共同k阶邻域（k≥1）的顶点应该具有相似的潜在表示。具体来说，对于 每个顶点，GraRep将其k步邻居（k≥1）定义为上下文顶点，并且对于每个1≤k≤K，学习k步顶点表示，GraRep使用skip-gram的矩阵分解版本.

$$[U^k,Σ ^k,V^k]=SVD(X^k)$$

其中，$x_k$是log k阶转移概率矩阵。顶点$v_i$的k阶表示构造为矩阵$U^k_d(Σ^k_d)^{\frac{1}{2}}$的第1行，其中$u^k_d$是$u^k$的第一d列。 $Σ^k_d$是由上d奇异值组成的对角矩阵。在学习k步顶点表示之后，GraRep将它们作为最终的顶点表示连接在一起.

**图表示的深层神经网络(DNGR)**:克服截尾随机游动在顶点上下文信息挖掘中的不足,即序列边界点获取正确上下文信息的困难以及步长和步数的确定困难。DNGR[9]利用随机冲浪模型捕捉每一对顶点之间的上下文相关性，并将它们保留为v-维顶点表示X。为了提取复杂特征和模型非线性，DNGR将堆叠去噪自动编码器（SDAE）[60]应用于高维顶点表示X，以学习深度低维顶点表示。

**结构深度网络嵌入(SDNE):**SDNE[19]是一种基于深度学习的方法，它使用半监督的深度自编码模型来捕获网络结构中的非线性。在无监督部分,SDNE通过重构v维顶点邻接矩阵表示来学习二阶邻近保持顶点表示，该表示试图最小化。$$L_{2nd}=\sum\limits_{i=1}^{|V|}||(r_{v_i}^{(0)}- \hat r_{v_i}^{(0)})\bigodot b_i||_2^2$$

其中$r_{v_i}^{(0)}=S_i$：是输入表示,$\hat r_{v_i}^{(0)}$是重构表示。$b_i$是一个权重向量，用于更多地惩罚**S**的非零元素的构造误差。

> ![1542082084727](F:\Machine-learning-and-data-science-notebook\images\网络表示学习\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542082084727.png)
>
> 图6.node2vec考虑的两种不同的邻域抽样策略：BFS和DFS。

在监督分量中，SDNE通过惩罚嵌入空间中连通顶点之间的距离来引入一阶邻近度。这一目标的损失函数定义为：$$L_{1st}=\sum\limits_{i,j=1}^{|V|}S_{i,j}||r_{v_i}^{(K)}-r_{v_j}^{(K)}||$$

其中$r^{(K)}_{v_i}$是顶点$ v_{i}$的第K层表示，K是隐藏层数。总之，SDNE使联合目标函数最小化：$L = L_{2nd} + αL_{1st} + νL_{reg}$

其中，$L_reg$是一个正则化的术语，以防止过度拟合。求解(8)的极小化后，对于顶点$v_i$，以K层表示$r_{v_i}^{(K)}$作为其表示$r_{v_i}$。

**node2vec:** 与为每个顶点定义邻域（上下文）的严格策略相反，node2vec [34]设计了一种灵活的邻域采样策略，即偏置随机游走，它在两种优先采样策略之间平滑插值，即广度优先采样(BFS)和深度优先采样(DFS)，如图6所示。在node2vec中利用的偏置随机游走可以更好地保留二阶和高阶邻近。

按照 skip-gram的架构，给定由有偏随机游动生成的邻域顶点$N(V_i)$集合，node2vec通过优化发生概率来学习顶点表示$f(V_i)$。 以顶点$v_i，f(V_i)$的表示为条件的邻域顶点$N(V_{i})$的y：$$\max \limits_f \sum\limits_{v_i\in V} log Pr(N(v_i)|f(v_i))$$

**高阶邻近保留嵌入(HOPE):**HOPE [35]学习捕获定向网络中非对称高阶邻近的顶点表示。 在无向网络中，传递性是对称的，但在有向网络中它是不对称的。 例如，在有向网络中，如果存在从顶点vi到顶点vj以及从顶点vj到顶点vk的有向链接，则更可能具有从vi到vk的有向链接，但是不具有从vk到vi的有向链接。

为了保持不对称传递性，HOPE分别学习了两个顶点嵌入向量U s，Ut∈R| V |×d，它们被称为源和目标嵌入向量。 在从四个邻近度量构建高阶邻近矩阵S之后，即Katz Index [61]，Rooted PageRank [62]，Common Neighbors和Adamic？Adar。 HOPE通过解决以下矩阵分解问题来学习顶点嵌入:$$\min\limits_{U_s,U_t}||S-U^s*U^{t^T}||^2_F$$

> ![1542088160404](F:\Machine-learning-and-data-science-notebook\images\网络表示学习\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542088160404.png)
>
> 表4.保持微观结构的NRL算法概述

非对称邻近保持图嵌入(APP)。APP[36]是另一种NRL算法，其目的是通过使用蒙特卡罗方法来近似非对称根。 PageRank邻近度[62]。与希望类似，APP对每个顶点vi有两个表示，一个表示为源角色rsvi，另一个表示为目标角色rtvi。开始的每个抽样路径 以vj结尾的表示是通过最大化目标顶点vj的出现概率来学习的，其发生概率取决于源顶点vi：Pr(Vj)=exp(rsvi·rtvj)P v∈V exp(rsvi·rtv)。(11)

图形GANGraphGAN[37]通过对抗性学习框架对连通性行为建模来学习顶点表示。受GaN(生成对抗性网)[58]的启发，GraphGAN通过两个组件工作：(I)生成器G(v=vvvc)，它适合Vc跨V连接的顶点的分布，并生成t。 (Ii)判别器D(v，vc)，它为顶点对(v，vc)输出一个连接概率，以区分G(V_X_Vc)生成的顶点对与G_n之间的连接概率。 d真相。G(v，vc)和D(v，vc)竞争的方式是：G(v，vc)尽可能地拟合真实的连接分布，生成假连接顶点对来欺骗D(v，vc)，而D(v，vc)则尝试t。 O增加其判别能力，以区分由G(v_(？)vc)产生的顶点对和基本真理。比赛由下列极小游戏完成：minθGmaxθDXvc∈V

其中gv∈Rk和dV∈Rk分别是发生器和判别器的表示向量，θD={dV}，θG={gv}。在Eq中的Minimax游戏之后。(12)求解后，gv作为最终顶点表示。

摘要：表4总结了显微结构保持NRL算法所保持的邻近性。此类别中的大多数算法保留第二个？顺序和高阶接近度，而只有LINE [1]，SDNE [19]和GraphGAN [37]考虑一阶接近度。 从方法论的角度来看，DeepWalk [6]，node2vec [34]和APP [36]采用随机游走来捕捉顶点邻域结构。 GraRep [26]和HOPE [35]通过对| V |进行因式分解来实现 ×| V | 尺度矩阵，使它们难以扩大.LINE [1]和GraphGAN [37]直接模拟连通行为，而基于深度学习的方法（DNGR [9]和SDNE [19]）学习非线性顶点表示。

4.1.2结构作用邻近保持NRL

除了局部连通模式外，顶点通常在介观水平上具有相似的结构角色，如恒星中心或团成员。结构角色接近保持NRL目标 嵌入彼此相距较远但彼此结构相似的角色的顶点。这不仅方便了下游结构角色相关的任务，而且增强了 保存NRL的显微结构。

struct2vec。struct2vec[38]首先将顶点结构角色相似性编码成一个多层图，其中每层边的权重由t处的结构角色差决定。 相应的比例。然后在多层图上执行DeepWalk[6]学习顶点表示，从而使多层图中的顶点彼此接近(具有高结构)。 E相似性)被紧密嵌入在新的表示空间中。

对于每个顶点对(vi，vj)，考虑到它们的邻居在k步内形成的k-hop邻域，它们在k，dk(vi，vj)的结构距离被定义为 Dk(vi, vj ) = Dk−1(vi, vj ) + g(s(Rk(vi)), s(Rk(vj )))

其中rk(Vi)是vi的k-hop邻域中的顶点集合，s(rk(Vi)是rk(Vi)中顶点的有序度序列，g(s(rk(Vi)，s(rk(Vj)是有序d之间的距离。 egree序列s(rk(Vi)和s(rk(Vj)。当k=0时，d0(vi，vj)是顶点vi和vj之间的度差。

GraphWave.利用谱图小波扩散模式，GraphWave[39]将顶点邻域结构嵌入到一个低维空间中，并保持了结构的接近性。 。假设，如果网络中两个远距离的顶点具有相似的结构角色，从它们开始的图小波将以相似的方式在它们的邻域中传播。

对于顶点vk，其谱图小波系数Ψk定义为 Ψk = UDiag(gs(λ1), · · · , gs(λ|V |))UTδk,

其中U是图Laplacian L的特征向量矩阵，λ1，··，λ{V}是特征值，GS(λ)=exp(−λs)是热核，δk是k的一个热向量。将Ψk作为概率分布，将Ψk中的谱小波分布模式编码为其经验特征函数：$$φ_k(t)=\frac{1}{|V|}\sum\limits_{m=1}^{|V|}e^{itΨ_{km}}$$

然后，通过采样φk(T)在d点t1、t2、··、td上的二维参数函数，得到vk的低维表示：$$f(v_k) = [Re(φ_k(t_1)), · · · ,Re(φ_k(t_d)),Im(φ_k(t_1)), · · · ,Im(φ_k(t_d))]$$

结构和邻域相似性保持网络嵌入(SNS):SNS[40]增强了一种具有结构角色邻近性的基于随机游走的方法。为了保留顶点结构角色，SNS将每个顶点表示为一个图形度向量，每个元素是给定顶点被图形的相应轨道接触的次数。图形度向量是 用于度量顶点结构角色相似度.

给定一个顶点vi，SNS使用它的上下文顶点C(Vi)和结构相似的顶点S(Vi)来预测它的存在，这是通过最大限度地提高以下概率来实现的：Pr(vi|C(vi), S(vi)) = exp(r0vi· hvi)Pu∈Vexp(r0u · hvi), (18)

其中r0 vi是vi的输出表示，hvi是用于预测vi的隐层表示，这是根据输入表示ru对C(Vi)和S(Vi)中的每个u进行聚合的。

摘要：struct2vec[38]和GraphWave[39]利用结构角色邻近来学习顶点表示，这些表示有助于特定的与结构角色相关的任务，例如顶点分类 在交通网络中，SNS[40]增强了基于随机游动的微观结构保持NRL算法的结构角色邻近性。从技术上讲，结构2ve采用了随机游动。 C和SNS，而图波采用矩阵因式分解。

#### 4.1.3 社区内邻近性保持NRL

现实世界网络展示的另一个有趣的特征是社区结构，其中顶点在同一个社区内紧密相连，但很少与vertic连接。 来自其他社区的。例如，在社交网络中，来自同一利益集团或从属关系的人往往组成一个社区。在引文网络中，关于类似研究主题的论文倾向于f。 经常相互引用。社区内保持NRL的目的是利用具有关键顶点属性的社区结构来学习信息顶点表示。

学习潜在的社会维度。基于社会维度的NRL算法试图通过成员或隶属于多个社会维度来构建社会行为者的嵌入。向内 在这些潜在的社会维度中，我们考虑了社会网络中的“社区”现象，指出具有相似属性的社会行为者往往形成群体内部比较密集的群体。 领带。因此，这个问题归结为一个经典的网络分析任务？社区检测-该任务旨在发现一组内部连接比组间连接更紧密的社区。 领带。三种聚类技术，包括模块化最大化[31]，谱聚类[32]和边缘聚类[33]被用来发现潜在的社会维度。每个社会维度 N描述了一个顶点属于似然从属关系的可能性。这些方法保存全局社区结构，但忽略了局部结构特性，例如一阶和二阶邻近性.

模块化非负矩阵分解(MNMF):M-NMF[28]增强了二阶和高阶与更广泛的社区结构的接近性，以学习更多信息丰富的顶点嵌入U∈R_xV。 *使用下列目标：

$$公式(19)$$

其中，顶点嵌入U是通过最小化KS−MU Tk 2 F来学习的，其中S∈R_x_v_x_x是点对接近矩阵，它捕捉了t时的二阶和高阶接近度。 作为代表。社区指示顶点嵌入H是通过最大化tr(HTBH)来学习的，它本质上是以B为模块矩阵的模块化最大化的目标。 。通过引入一个社区表示矩阵C，KH−UCTK2F上的极小化使得这两个嵌入相互一致。

摘要：学习潜在社会维度的算法[31]、[32]、[33]只考虑社区结构来学习顶点表示，而M-NMF[28]则整合了微观结构( 二阶和高阶邻近)与社区内的接近。这些方法主要依靠矩阵分解来检测社区结构，使其难以扩展。

#### 4.1.4 宏观结构保持NRL

宏观结构方法的目的是从宏观上保持一定的全局网络性质。最近的研究很少是为了 这个目的。

程度惩罚原则(DP)。许多真实世界的网络都呈现出宏观无尺度的特性，它描述了顶点度服从长尾分布的现象。 .E.，大多数顶点是稀疏连通的，只有少数顶点有密集的边。为了获取无标度属性，[41]提出了程度惩罚原则(Dp)：惩罚之间的邻近性。 高度顶点。然后，将该原理与两种NRL算法(即谱嵌入[5]和DeepWalk[6])结合起来，学习无标度性质的保点表示。

希拉希卡 网络表示法学习(HARP)。为了捕捉网络中的全局模式，HARP[42]对小型网络进行采样，以逼近全局结构。从 以采样网络作为初始化，推导出原始网络的顶点表示。这样，在最终表示中保留了全局结构。取得s 通过合并边缘和顶点，连续地从原始网络中抽取一系列较小的网络，并在此基础上进行层次化的顶点表示。 最小的网络到原来的网络。在HARP中，DeepWalk[6]和[1]是用来学习顶点表示的。

摘要：DP[41]和HARP[42]都是通过调整现有的NRL来实现的。 捕捉宏观结构的算法。前者试图保持无标度属性，后者则使学习的顶点表示尊重全局网络结构。 

### 4.2 无监督内容增强网络表示学习

除了网络结构外，现实世界的网络常常以丰富的内容作为顶点属性附加在一起，例如网页中的网页。 网络，引文网络中的论文，社交网络中的用户元数据。顶点属性为度量顶点之间的内容级相似性提供了直接证据。因此，网络代表 如果将顶点属性信息适当地结合到学习过程中，则可以显着地改进命名学习。最近，人们提出了几种内容增强的nrl算法。 结合网络结构和顶点属性，加强网络表示学习。

#### 4.2.1 文本相关的深度行走(TADW)

TADW[7]首先证明了DeepWalk之间的等价性 [6]和下列矩阵因式分解：$$公式20$$

其中W和H是隐嵌入，M是带转移p的顶点上下文矩阵。 k步内每个顶点对之间的可选性。然后，通过归纳矩阵因式分解[54]$$公式21$$

其中T为v，引入文本特征。 Ertex文本特征矩阵在求解(21)后，通过W和HT的级联形成最终的顶点表示。

#### 4.2.2 同质性、结构和内容增强网络表示 

尽管TADW [7]能够结合纹理特征，但它只考虑网络顶点的结构上下文，即二阶和高阶邻近，但忽略了其学习框架中的重要同音特性（一阶邻近）。 HSCA [20]被提议同时集成同音，结构上下文和顶点内容，以学习有效的网络表示。

对于TADW，第一顶点vi的学习表示为WT：I，(HT：I)T，其中W：I和T：I分别是W和T的第I列。为了执行一阶邻近，hsca引入了 S是一个正则化项，用于在嵌入空间中的直接连通节点之间强制执行同质性，它被表述为

$$公式22$$

其中S是相邻矩阵。HSCA的目标是

$$公式23$$

其中λ和RAID是权衡的参数。求解上述优化问题后，将W和HT的级联作为最终的顶点表示。

#### 4.2.3 双约束Boltzmann机器(PRBM)

通过利用限制Boltzmann机器(RBM)的强度[57]，[29]设计了一种称为配对RBM(PRBM)的新模型，通过结合顶点属性和链接信息来学习顶点表示。pRBM考虑具有与二进制属性相关的顶点的网络。对于每个边(vi，vj)∈E，vi和vj的属性为v(I)和v(J)∈{0，1}m，它们的隐藏表示为h(I)和h(J)∈{0，1}d。通过最大化定义在v(I)、v(J)、h(I)和h(J)上的pRBM的联合概率来学习顶点隐藏表示：

$$公式24$$

其中θ={W∈Rd×m，b∈Rd×1，c∈Rm×1，M∈Rd×d}为参数集，Z为归一化项。为了模拟联合概率，能量函数被定义为$$公式25$$

其中，WiJ(h(I)tmh(J)强制vi和vj的潜在表示接近，而WiJ是边(vi，vj)的权重。

#### 4.2.4 保持用户配置文件的社会网络嵌入(UPP-SNE)

UPP-SNE[43]利用用户配置文件特性来增强用户在社交网络中的嵌入学习。与纹理内容特征相比，用户配置文件具有两个独特的属性：(1)用户配置文件具有噪声性、稀疏性和不完全性；(2)不同维度的用户配置特征是主题不一致的。过滤噪声并从用户配置文件中提取有用信息，UPP-SNE通过对用户配置文件特征执行非线性映射来构造用户表示，该映射以网络结构为指导。在UPPSNE中使用近似内核映射[63]从用户配置文件特征构造用户嵌入：$$公式26$$

其中XI是顶点vi的用户轮廓特征向量，I是对应的系数向量。

为了监督非线性映射的学习，使用户配置文件和网络结构相辅相成，使用了DeepWalk[6]的目标。

$$公式27$$

其中{vi−t，···，vi}\vi是给定随机游动序列中t窗口大小内顶点vi的上下文顶点。

#### 4.2.5 财产保护网络嵌入(PPNE)

为了学习内容增强顶点表示，PPNE[44]联合优化了两个目标：(1)结构驱动目标和(Ii)属性驱动目标。遵循深度行走，结构驱动的目标是使顶点共享相似的上下文顶点紧密地表示.对于给定的随机游动序列S，将结构驱动的目标表述为$$公式28$$

属性驱动的目标是使Eq学习到的顶点表示。(28)尊重顶点属性的相似性。属性驱动目标的一个实现是

$$公式29$$

其中p(u，v)是u和v之间的属性相似度，d(u，v)是嵌入空间中u和v之间的距离，pos(V)和neg(V)是顶k相似和不同顶点的集合ac。 分别服从P(u，v)

摘要：上述无监督内容增强NRL算法以三种方式结合了顶点内容特性。TADW[7]和HSCA[20]使用的第一种方法是将网络结构与 [54]通过归纳矩阵分解的顶点内容特征[54]。该过程可以看作是受网络结构约束的顶点属性的线性变换。第二种方法是执行非线性映射来构造新的顶点嵌入。 尊重网络结构。例如，pRBM[29]和UPP-SNE[43]分别使用RBM[57]和近似核映射[63]来实现这一目标。PPNE[44]使用的第三个是 DD是对结构保持优化目标的属性保持约束。

## 5 半监督网络表示学习

与顶点相连的标签信息直接表示顶点的组或类从属关系。这类标签与网络结构和顶点有很强的相关性，但并不总是一致的。 属性，并总是有助于学习信息和区分网络表示。半监督NRL算法是沿着这条线发展起来的，它利用网络中可用的顶点标签来寻求更有效的顶点表示。

### 5.1 半监督结构保持NRL

在区分表示法学习[64]、[65]的启发下，DDRW[45]提出通过共同优化DeepWalk[6]的目标来学习区分网络表示。 具有以下L2-损失支持向量分类目标

$$公式30$$

其中σ(X)=x，如果x>0，否则σ(X)=0。

因此，DDRW的共同目标被定义为$$公式31$$

其中LDW是Deekwalk的目标函数。目标(31)的目的是学习第k类的二元分类的鉴别顶点表示。DDRW被推广到使用单对REST策略来处理多类分类[66].

#### 5.1.2 最高保证金深度行走(MMDW)

相似地，MMDW[46]将矩阵分解版本DeepWalk[7]的目标与具有{(RV 1，Y1：)，··，(RVT，YT：)}训练集的多类支持向量机目标相结合：$$公式32$$

其中li=k，yk=1，ej i=1，yj=−1，ej i=0，yj=1。$$公式33$$

MMDW的共同目标是$$公式34$$

其中，LDW是DeepWalk矩阵分解版本的目标。

#### 5.1.3 换能线(Tline)

沿着相似的思路，Tline[47]被提出作为线[1]的一种半扩展，它同时学习直线的顶点表示和支持向量机分类器。给出一组标记和未标记的 Led顶点{v1，v2，··，vl}和{vl 1，··，v_x-V}，通过优化目标，在{v1，v2，··，vl}上训练多类支持向量机分类器：$$公式34$$

根据保持一阶和二阶邻近性的线公式，tline优化了两个目标函数：$$公式35$$

$$公式36$$

由于继承了线路处理大规模网络的能力，tline被认为能够以较低的时间和内存代价学习大规模网络的区分顶点表示。

#### 5.1.4 群增强网络嵌入(基因)

基因[48]以概率的方式将组(标签)信息与网络结构相结合。基因认为，如果顶点有相似的邻居或加入相似的群体，它们就应该被紧密地嵌入在低维空间中。受DeepWalk[6]和文档建模[67]，[68]，m的启发 学习组标签通知顶点表示的基因机制是通过最大限度地提高下列日志概率来实现的：$$公式37$$

其中Y是不同群的集合，WGI是用gi标记的随机游动序列集，Wˆgi是从群gi随机抽取的顶点集。

#### 5.1.5 半监督网络嵌入

SEMENE[49]分两个阶段学习半监督顶点表示。在第一阶段，SEMENE利用DeepWalk[6]框架以无监督的方式学习顶点表示。it p 指出在使用上下文顶点vi j预测t时，DeepWalk不考虑上下文顶点的顺序信息，即上下文顶点与中心顶点之间的距离。 中心顶点六。因此，半NE通过将概率pr(vi、j、vi)建模为具有j相关参数的概率PR，将顺序信息编码为DeepWalk：$$公式38$$

其中，Φ(·)是顶点表示，Ψj(·)是计算pr(vi j=vi)的参数。

在第二阶段，半NE学习一个神经网络，它调整学习的无监督顶点表示，以适应顶点标签。

### 5.2 半监督内容增强nrl

最近，更多的研究转移到开发标签和内容增强nrl算法，这些算法调查了顶点内容和 标签，以协助网络表示学习。随着内容信息的整合，学习到的顶点表示将更具有信息性，并考虑到标签信息。 红色，学习的顶点表示可以为底层分类任务高度定制。

#### 5.2.1 使用耦合神经网络框架的三方深层网络表示(TriDNR)

 TriDNR[50]从三个信息源学习顶点表示：网络结构、顶点内容和顶点标签。为了捕获顶点内容和标签信息，TriDNR采用Pa。 RAGET向量模型[67]通过最大限度地实现以下目标来描述顶点词相关性和标签词对应关系：

其中{w−b：WB}是长度为2b的上下文窗口中的一个词序列，Ci是顶点vi的类标记，L是标记顶点的一组索引。然后由Coupl实现TriDNR。 将段落向量目标与深度行走目标联系起来：

其中，LDW为深度行走最大化目标函数，α为折衷参数.

#### 5.2.2 链接文档嵌入(LDE)

LDE[51]是为了学习链接文档的表示，链接文档实际上是引文或网页网络的顶点。类似TriDNR[50]，LDE 通过对三种关系的建模来学习顶点表示，即字-文档关系、文档-文档关系和文档-标签关系。lde是通过解决以下问题来实现的。 

本文利用概率PR(WJ_x_wi，DK)对Word-Document关系进行了建模，这意味着在文档DK中，Word WJ是wi的一个相邻词。捕捉文字文档 T关系，提取三重子(wi，wj，dk)，在文档dk中产生字邻对(wi，wj)。三元组(wi，wj，dk)由P表示。 由链接文档对(di，dj)、pr(dj\di)之间的条件概率捕获。本文还考虑了文献标号关系的概率，并通过对pr(yi\di)的建模来考虑文档标签关系。 对以第一文件为条件的“易纲”类标签的不满。在(41)中，W、D和Y分别是文字、文档和标签的嵌入矩阵。

#### 5.2.3 区分矩阵因式分解(DMF)

为了增强具有鉴别能力的顶点表示能力，DMF[8]执行TADW(21)的目标，并对训练在标记顶点上的线性分类器进行经验损失最小化：

 l，j=1(mij−w ti htj)2 2xn∈L(yn1−ηtxn)2λ1 2(khk 2 f kηk2)λ2 2 kwk 2 F，(42)

其中wi是顶点表示矩阵W的第一列，TJ是顶点文本特征矩阵T的第j列，L是标记顶点的指标集。

DMF从W构造顶点表示，而不是W和HT。这是基于经验发现，W包含足够的信息来表示顶点。在(42)，xn的目标中 设置为[WT n，1]T，其中将线性分类器的截距项b合并到η中。通过交替优化W、H和η来求解优化问题(42)。一旦优化问题 解决了LEM问题，学习了识别性和信息性顶点表示和线性分类器，并共同对网络中的未标记点进行分类。

#### 5.2.4 具有嵌入的预测标签和邻居-从数据中转换或归纳(Planetoid)

Pletoid[52]利用网络嵌入和顶点属性进行半监督学习。平面图通过最小化预测结构c的损失来学习顶点嵌入。 ontext，它被表述为



其中(i，c)是顶点上下文对(vi，vc)的索引，Ei是顶点vi的嵌入，wc是上下文顶点vc的参数向量，γ∈{1，−1}表示所采样的顶点是否存在。 上下文对(i，c)为正或负。根据网络结构和顶点标签对三元组(I，c，γ)进行采样。

平面然后通过深层神经网络将学习到的顶点表示e和顶点属性x映射到隐层空间，并将这两个隐层表示连接在一起进行预处理。 通过最大限度地减少以下分类损失，删除顶点标签：



为了将网络结构、顶点属性和顶点标签集成在一起，Planetoid联合最小化了这两个目标(43)和(44)，用深神经网络学习顶点嵌入e。

#### 5.2.5 标签通知属性网络嵌入(LAIN)

Lane[30]通过将网络结构邻近性、属性亲和力和标签邻近性嵌入到统一的潜在表示中来学习顶点表示。学习的表示法是exp的。 获取网络结构和顶点属性信息，如果提供标签信息的话。其中嵌入学习分两个阶段进行。在第一阶段，顶点 将网络结构中的邻近性和属性信息映射为潜在的表示U(G)和U(A)，然后通过最大化它们之间的相关性将U(A)合并到U(G)中。在第二站 GE，Lane利用联合邻近(由U(G)确定)平滑标签信息，并将它们一致嵌入到另一个潜在表示U(Y)中，然后将U(A)、U(G)和U(Y)嵌入到a中。 统一嵌入表示H.

### 5.3 摘要，概要

我们现在总结和比较了表5中半监督的NRL算法所使用的区分学习策略的优缺点。三种策略用于实现歧视性学习。 第一种策略（即DDRW [45]，MMDW [46]，TLINE [47]，DMF [8]，SemiNE [49]）是对顶点表示强制执行分类损失最小化，即将顶点表示拟合到分类器。 这提供了在新嵌入空间中将不同类别的顶点彼此分开的直接方式。 第二策略（由GENE [48]，TriDNR [50]，LDE [51]和Planetoid [52]使用）是通过建模顶点标签关系来实现的，这样具有相同标签的顶点具有相似的矢量表示。 LANE [30]使用的第三种策略是将顶点和标签共同嵌入公共空间。

对分类器进行顶点表示的拟合可以利用顶点标签中的鉴别能力。使用这种策略的算法只需要少量的标记顶点(例如，1 (0%)比无监督的同行获得显著的性能提升。因此，在稀疏标记的情景下，它们对区分性学习更为有效。然而，拟合顶点表示 对分类器的诱捕更容易发生过度拟合。经常采用正规化和辍学[69]来解决这一问题。通过对比，建立了顶点标签关系和联合顶点EMBE模型。 布丁要求有更多的顶点标签来使顶点表示更具有判断力，但它们可以更好地捕捉类内邻近，也就是说，属于同一类的顶点更接近e。 在新的嵌入空间中。这使他们在诸如顶点聚类或可视化等任务上具有普遍的优势。

## 6 应用

一旦通过网络表示学习技术学习了新的顶点表示，就可以使用传统的基于向量的算法来解决重要的分析任务，例如顶点分类。 链接预测、聚类、可视化和推荐。通过评估它们在这些任务上的表现，也可以验证所学习的表示的有效性。

### 6.1 顶点分类

顶点分类是网络分析研究的重要内容之一。在网络中，顶点通常与描述实体某些方面的语义标签相关联，例如 作为信仰、兴趣或从属关系。在引文网络中，出版物可能带有主题或研究领域的标签，而社交网络中实体的标签则可能显示个人的信息。 休息或政治信仰。通常，由于标签成本高，网络顶点被部分或稀疏地标记，所以网络中的大部分顶点都有未知的标签。Ve问题 rtex分类的目的是预测给定部分标记网络[10]，[11]的未标记顶点的标号。由于顶点不是独立的，而是通过链接以网络的形式相互连接的，因此顶点分类应该利用这些依赖关系来对Vertic的标签进行联合分类。 埃斯。除其他外，集体分类还建议构造一组新的顶点特性，以总结邻域中的标签依赖关系，这在类中被证明是最有效的。 许多真实世界的网络[70]，[71]。

网络表示学习遵循基于网络结构自动学习顶点特征的原则。已有的研究已经评估了学习者的辨别能力。 d两个设置下的顶点表示：无监督设置(例如，[1]、[6]、[7]、[20]、[34])，其中顶点表示分别学习，然后应用区分分类 新嵌入的RS类支持向量机或Logistic回归，以及半调理设置(例如，[8]、[30]、[45]、[46]、[47])，其中表示性学习和区分性学习同时存在 从标记顶点推断出的鉴别能力直接有利于信息顶点表示的学习。这些研究证明了更好的顶点表示。 NS有助于提高分类精度。

### 6.2 链路预测

网络表示学习的另一个重要应用是链接预测[13]，[72]，它的目的是推断出Entiti对之间存在新的关系或正在出现的交互作用。 ES基于当前观察到的链接及其属性。为解决这一问题而开发的方法能够发现网络中的隐式或缺失交互，即标识。 虚假链接的建立，以及对网络演化机制的理解。链接预测技术在社交网络中被广泛应用于预测人与人之间的未知联系， 用来推荐友谊或确定可疑的关系。目前，大多数社交网络系统都在使用链接预测来自动提示高度为 准确。在生物网络中，链路预测方法已经被开发出来预测以前未知的蛋白质之间的相互作用，从而大大降低了经验方法的成本。 。读者可以参考调查文件[12]，[73]了解这一领域的最新进展。

良好的网络表示应该能够捕获网络顶点之间的显式和隐式连接，从而使应用程序能够进行链接预测。[19]和[35]预测缺少的链接 关于社交网络上的学习顶点表示。[34]还将网络表示学习应用于协作网络16和蛋白质-蛋白质相互作用网络。他们证明，在这些网络上，使用学术代表预测的链接。 与传统的基于相似的链路预测方法相比，抱怨具有更好的性能。

### 6.3 聚类

网络聚类是指将网络顶点划分成一组簇的任务，使得顶点在同一簇内密集地连接在一起，但与其他星系团中的少数顶点相连[74]。这类群集结构或社区广泛存在于生物信息学、计算机科学、物理学等广泛的网络系统中。 地质学等，并有很强的含意。例如，在生物学网络中，簇可能对应于一组具有相同功能的蛋白质；在网页网络中，群集可能是具有相似主题或相关连续体的页面。 在社交网络中，集群可能表示有相似兴趣或关联的人群。

研究人员提出了大量基于顶点间相似度或连接强度的网络聚类算法。最小-最大切割和归一化切割方法[75]， [76]寻求递归地将一个图划分成两个簇，最大限度地增加簇内连接的数量和最小化簇间连接的数量。基于模块化的方法(例如， [77]，[78])的目标是最大限度地提高聚类的模块化程度，即假设聚类的边缘是随机分布的，簇内边缘的分数减去期望的分数。网络分区 具有高模块性的Ng具有密集的簇内连接，而稀疏的簇间连接。其他一些方法(例如，[79])试图识别具有类似于桥的结构角色的节点 S和离群值。

最近的NRL方法(例如GraRep[26]、DNGR[9]、MNMF[28]和pRBM[29])使用聚类性能来评估不同网络上学习网络表示的质量。英图伊特 正确地说，更好的表示将导致更好的集群性能。这些工作遵循了一种常见的方法，即首先应用无监督的NRL算法来学习顶点表示， 然后，d对所学习的表示执行k均值聚类，以对顶点进行聚类。特别是，pRBM[29]表明，NRL方法的性能优于使用CLUS原始特性的基线。 没有学习表象。这表明有效的表征学习可以提高聚类性能。

### 6.4 可视化

可视化技术在管理、探索和分析复杂的网络数据中起着至关重要的作用。[80]调查了一系列用于从信息可视化p中可视化图形的方法。 内省。这项工作比较了各种用于图形可视化的传统布局，如基于树、三维和双曲的方法，并证明了经典的可视化技术是有效的。 适用于中小型网络；然而，当应用于大型网络时，它们将面临巨大的挑战。很少有系统能声称能有效地处理数千个顶点。 具有这个数量级的Gh网络经常出现在各种各样的应用中。因此，可视化过程中的第一步通常是减少要显示的网络的大小。一种常见的方法本质上是找到一个极低维表示法。 f保持固有结构的网络，即在低维空间中保持相似的顶点接近和不相似的顶点[17]。

网络表示学习的目标是将一个大的网络嵌入到一个低维的新的潜在空间中。在向量空间中得到新的嵌入后，流行的方法 例如t分布随机邻居嵌入(t-SNE)[81]可用于在二维或三维空间中可视化网络。以学习的顶点表示作为输入，[1]行使用t 在作者被映射到一个二维空间之后，他用tsne软件包来可视化DBLP合著者网络，并表明这条线能够将同一领域的作者聚集到同一个社区。HSCA[20] 通过可视化引用网络，说明了内容增强NRL算法的优点。半监督算法(例如tline[47]、TriDNR[50]和DMF[8])证明了 ualization结果具有更好的聚类结构，并适当地导入顶点标签。

### 6.5 推荐

除了结构、内容和顶点标签信息之外，许多社交网络还包括地理和时空信息，用户可以在网上与 兴趣点(POI)推荐的朋友，如交通、餐厅、观光地标等。这种基于位置的社交网络(Lbsn)的例子包括Foursquare、Yelp、 Facebook的位置，还有很多其他地方。对于这些类型的社交网络，POI建议用户感兴趣的对象，这取决于他们自己的上下文，比如地理位置。 用户和他们的利益。传统上，这是通过使用协作过滤等方法来解决的，以利用用户活动和地理之间的空间和时间相关性。 CAL距离[82]。然而，由于每个用户的签入记录都非常稀疏，寻找相似的用户或计算用户和位置之间的转换概率是一个重大的挑战。

最近，时空嵌入[14]，[15]，[83]已经开始学习低维密集向量来表示用户、位置和兴趣点等。因此，每个用户、位置、 NDPOI可以分别表示为一个低维向量，用于相似性搜索和其他许多分析。这种时空感知嵌入的一个固有优点是它减轻了t。 数据稀疏问题，因为学习到的低维向量通常比原始表示要密集得多。因此，它使查询任务(如top-k poi搜索)变得更多。 比传统方法准确。

### 6.6 知识图谱

知识图是数据库系统中一种新型的数据结构，它对数十亿个实体的结构化信息及其丰富的关系进行编码。知识图通常包含 一组丰富的异构对象和不同类型的实体关系。这样的网络实体形成了一个巨大的图形，现在正在为17个商业搜索引擎提供动力，以便在网上找到类似的对象。传统上，知识图搜索是通过Datab进行的。 ASE驱动的方法探索实体之间的模式映射，包括实体关系。网络表示学习的最新进展激发了知识的结构化嵌入。 基地[84]。这种嵌入方法学习了知识图实体的低维向量表示，这样就可以通过比较来实现泛型数据库查询(如top-k搜索)。 数据库中查询对象和对象的矢量表示形式。

除了使用向量表示来表示知识图实体外，研究人员还提出使用这种表示来进一步增强和完善知识图本身。 例如，知识图完成旨在发现实体之间的完全关系，最近的一项工作[85]建议使用图上下文来查找实体之间缺失的链接。T型 HIS类似于社交网络中的链接预测，但这些实体通常是异构的，一对实体也可能有不同类型的关系。

## 7 评估协议

在这一部分中，我们讨论了验证网络表示学习有效性的评估协议。这包括常用基准数据集和评估方法的总结。 然后对算法的性能和复杂度进行了比较。

7.1基准数据集

对研究界评价新开发的NRL算法与现有基线方法相比的性能具有重要作用。 已经公开了一些网络数据集，以便于对不同任务的NRL算法进行评估。我们总结了大多数公共部门使用的网络数据集的列表。 表6中的网络表示学习论文。

表6总结了可公开获取的基准数据集的主要特征，包括网络类型(有向或无定向、二进制或加权)、顶点数目、维数、o值。 f边，E，标签的数目，Y，无论网络是否是多标记的，以及网络顶点是否附加属性。在表6中，根据信息的性质n 我们将基准数据集分为八种不同的类型：

社交网络。BlogCatalog、Flickr和YouTube数据集由相应的在线社交网络平台的用户组成。对于这三个数据集，顶点标签由用户intere定义。 ST组但是用户属性不可用。facebook网络是由10个facebook自我网络组成的，其中每个顶点都包含用户配置文件属性。Amherst，Hamilton，Mich和Ro Chester[86]数据集是由来自美国相应大学的用户组成的Facebook网络，每个用户拥有6个用户配置文件功能。通常，用户配置文件的功能都是杂乱无章的，不完整的。 E，长尾分布。

语言网络语言网络维基百科[1]是由整个英语维基百科网页组成的单词共现网络。此网络上没有类标签。EMBE这个词 从这个网络中学习到的数据可以通过文字类比和文档分类来评估。

引文网。引文网络是由作者与作者之间的引文关系或论文引用关系构成的定向信息网络.它们是从不同的Datab收集的。 学术论文ASES，如DBLP和Citeseer。在常用的引文网络中，DBLP(Author引文)[1]是作者之间的加权引文网络，其边权由 由一位作者撰写并被另一位作者引用的论文数量，而DBLP(论文引文)[1]、Cora、Citeseer、PubMed和Citeseer-M10是二进制论文引文网络，它们也是二进制论文引用网络。 以顶点文本属性作为论文的内容。与社交网络中的用户配置文件功能相比，这里的顶点文本功能更以主题为中心，信息更丰富，并且可以更好地提供信息。 补充网络结构，学习有效的顶点表示。

协作网络协作网络arxiv gr-qc[88]描述了广义相对论和量子宇宙学研究领域论文的合著者关系。在这个网络里， 顶点代表作者，边表示作者之间的共同作者关系。因为顶点没有类别信息，所以这个网络被用来执行链接预测任务到valu。 学习顶点表示的质量。

网页网络网页网络(维基百科、网络知识库和政治博客[89])由真实世界的网页和它们之间的超链接组成，其中顶点代表网页和边缘索引。 从一个网页到另一个网页有一个超链接。网页文本内容通常被收集为顶点特征。

生物网络作为一种典型的生物网络，蛋白质-蛋白质相互作用网络[90]是Sapiens PPI网络的一个子图。这里的顶点代表一个蛋白质和边缘。 表明蛋白质之间存在相互作用。顶点的标签是从标记基因集[91]中得到的，并表示生物状态。

通信网络：安然电子邮件网络由安然员工之间的电子邮件通信组成，顶点是员工，边缘代表员工之间的电子邮件。雇员是劳工 根据其职能，担任7个职位(如首席执行官、总裁和经理)。

交通网络[39]使用的欧洲航空公司网络由6家在欧洲机场之间运营航班的航空公司组成：4家商业航空公司(法国航空公司、EasyJet公司、汉莎航空公司和瑞安航空公司)。 )和2家货运航空公司(葡萄牙航空公司和欧洲航空运输公司)。对于每个航空公司网络，顶点是机场，边缘代表机场之间的直航。总共有45个机场 根据其结构作用，被列为枢纽机场、区域枢纽、商业枢纽和重点城市。

> ![1542100976916](F:\Machine-learning-and-data-science-notebook\images\网络表示学习\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542100976916.png)
>
> 表6用于评价网络表示学习的基准数据集摘要。
>
>

> ![1542100991888](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1542100991888.png)

7.2评价方法

由于地面真理的不可用性，很难直接比较不同NRL算法学习到的顶点表示的质量。或者，为了评估e NRL算法在学习顶点表示上的有效性，常用的几种网络分析任务进行比较研究。

网络重建:网络重构的目的是通过预测基于内部PR的顶点之间的连接，从学习的顶点表示中重构原始网络。 顶点表示方式或顶点表示之间的相似之处。原始网络中已知的链路是评估重建性能的基本真理。精度@k和map[19]经常被使用 作为评估指标。该评价方法可以检验学习到的顶点表示是否能很好地保持网络结构，支持网络的形成。

顶点分类:作为NRL的一种评价方法，利用学习到的顶点表示作为特征，对有标记顶点进行分类器的训练。分类 未标记顶点的阳离子性能被用来评价学习顶点表示的质量。不同的顶点分类设置，包括二进制分类、多c LAS分类和多标签分类通常是根据底层网络特性进行的。对于二进位分类，f1分数被用作cri的评估值。 泰里恩。对于多类多标签分类，采用M ICRO-F1和M acro-F1作为评价标准.

顶点聚类:为了验证NRL算法的有效性，将k-均值聚类算法应用到学习的顶点表示中，实现了顶点聚类。社区一 N个网络作为评价聚类结果质量的基本真理，它是通过精度和归一化互信息(NMI)[92]来衡量的。假设是，如果 D顶点表示确实是信息丰富的，在学习的顶点表示上的顶点聚类应该能够发现社区结构。也就是说，需要良好的顶点表示。 生成良好的聚类结果。

链路预测:链路预测可以用来评估所学习的顶点表示是否信息丰富，以支持网络演化机制。在网络上执行链路预测 ，首先删除部分边缘，并从剩余的网络中学习顶点表示。最后，利用学习到的顶点表示对去除的边缘进行预测。表演 链路预测的性能用AUC和精度@k进行测量。

可视化。可视化提供了一种直观的方法，可以直观地评估学习到的顶点表示的质量。通常，t-分布随机邻居嵌入(t-sne)[81]是 应用于将学习到的顶点表示向量投影到二维空间中，使得顶点二维映射的分布易于可视化。如果顶点表示具有良好的质量，则在 同一类或同一社区中的二维空间、顶点应紧密嵌入，不同类或社区中顶点的二维映射应相距较远。

在表7中，我们总结了用于评估现有NRL算法所学习的顶点表示质量的信息网络类型和网络分析任务。 我们还提供各个NRL算法代码的超链接（如果有），以帮助感兴趣的读者进一步研究这些算法或运行实验进行比较。 总的来说，社交网络和引用网络经常被用作基准数据集，而ver？tex分类最常用作无监督和半监督设置中的评估方法。

### 7.3 经验结果

我们从文献中观察到，在不同的背景下，对不同的数据集进行实证评价。在确定最佳结果方面，经验结果缺乏一致性。 执行算法和它们的环境。因此，我们进行了基准测试实验，以公平地比较几种具有代表性的NRL算法在同一组数据集上的性能。注 这是因为半监督的nrl算法与任务有关：目标任务可能是二进制或多类，或多标签分类，或者因为它们使用不同的分类策略， 在相同的环境下，很难评估网络嵌入的有效性。因此，我们的实证研究重点是比较七种无监督的NRL算法(DeepWalk[6]，LIN)。 E[1]，node2vec[34]，MNMF[28]，TADW[7]，HSCA[20]，UPP-SNE[43])，它们是文献中最常用的两种评价方法。

我们的实证研究基于七个基准数据集：Amherst，Hamilton，Mich，Rochester，Citeseer，Cora和Facebook。 在[28]之后，对于Amherst，Hamilton，Mich和Rochester，仅使用网络结构，并使用属性“year”作为类标签，这是社区结构的良好指标。 对于Citeseer和Cora，研究区域用作类别标签。 Facebook数据集的类标签由属性“education type”给出。

#### 7.3.1 实验计划

对于基于随机游走的方法，DeepWalk、node2vec和UPP-SNE，我们一致地将步数、游走长度和窗口大小分别设为10、80和10。对于udp-sne，我们使用了 AT采用随机梯度下降法进行优化。node2vec的参数p和q设置为1，作为默认设置。对于M-NMF，我们将α和β设为1.对于所有算法，学习的维数 Tex表示被设置为256。对于直线，我们学习了128维顶点表示，其中包括一阶接近保持版本和二阶接近保持版本表示。 将它们串联在一起，得到256个维顶点表示.上述算法的其他参数都设置为默认值。

以学习顶点表示为输入，进行顶点分类和顶点聚类实验，以评价学习顶点表示的质量。顶点分类器 阳离子，我们随机选择5%和50%的样本来训练SVM分类器(用LIBLINEAR实现[66])，并在剩下的样本上进行测试。我们重复这个过程10次，并报告 平均M ICRO-F1和M acro-F1值。我们采用KMASS进行顶点聚类。为了减少随机初始化引起的方差，我们重复了20次聚类过程，并报告了t。 他平均精度和NMI值。

#### 7.3.2 业绩比较

表8和表9比较了不同算法在顶点分类和顶点聚类方面的性能。对于每个数据集，在所有基线中表现最好的方法都是粗体的.为 属性网络(Citeseer、Cora和Facebook)、下划线结果表明，在仅保留NRL算法(DeepWalk、line、node2vec和M-NMF)的结构中表现最好。

表8显示，在仅保留NRL算法的结构中，当训练率为5%时，node2vec总体达到最佳分类性能，当训练率为50%时，M- NMF在MICRO-F1方面表现最好，而DeepWalk则是M capF1的获奖者。在这里，M-NMF并不比DeepWalk、line和node2vec显示出明显的优势。这可能是因为 N-NMF的参数α和β不是优化的，必须仔细选择它们的值，以便在不同的组件之间实现良好的折衷。关于属性网络(Citeseer、Cora和FAC) (Ebook)，内容增强的NRL比仅保留NRL算法的结构性能要好得多。这证明了顶点属性在很大程度上有助于学习更多的信息顶点。 陈述。当训练率为5%时，UPP-SNE表现最好.这表明upp-sne的非线性映射提供了一种从顶点构造顶点表示的更好的方法。 与线性映射相比，在TADW和HSCA中所做的贡献更大。当训练率为50%时，TADW取得了最佳的整体分类性能，尽管在某些情况下，它略优于TADW。 d由HSCA提出。在引用网络(Citeseer和Cora)上，HSCA的性能优于TADW，而在Facebook上的性能则比TADW差。这可能是因为 Facebook社交网络的Ty弱于引文网络。为了使HSCA在Facebook上获得满意的性能，应该对保持同质性的目标进行较少的加权。

表9显示，该行在Amherst、Hamilton、Mich和Rochester上实现了最佳的集群性能。由于线的顶点表示同时捕获了一阶和二阶邻近，所以它 可以更好地维护社区结构，导致良好的聚类性能。在Citeseer、Cora和Facebook上，内容增强NRL算法UPP-SNE表现最好.

由于UPP-SNE通过非线性映射构造来自顶点的顶点表示，因此保存良好的内容信息有利于最佳的聚类性能。 在Citeseer和Cora上，node2vec比仅保留NRL算法的其他结构（包括其等效版本DeepWalk）执行得更好。 对于每个顶点上下文对（vi，vj），DeepWalk和node2vec使用两种不同的策略来近似概率Pr（vj | vi）：hierar？chical softmax [93]，[94]和负抽样[95]。 node2vec优于DeepWalk的集群性能证明了负抽样优于分层的优势
softmax，与[67]中报道的单词嵌入结果一致。

### 7.4 复杂性分析

为了更好地理解现有的NRL算法，我们在表10中详细分析了它们的时间复杂度和基本的优化方法。引入一种新的表示法来表示 迭代次数，我们使用nnz(·)表示矩阵的非零项数。简单地说，我们使用了四种解决方案来优化现有nrl算法的目标。 ms：(1)特征分解，它涉及到寻找矩阵的顶d特征向量，(2)选择优化一个变量，其余变量交替固定，(3)梯度。 下降，在每次迭代中更新所有参数，以优化总体目标；(4)随机梯度下降，在线模式下随机优化部分目标。

非监督NRL算法和半监督NRL算法都主要采用随机梯度下降来求解优化问题.这些算法的时间复杂度通常与respec成线性关系。 T表示顶点/边的数量，这使得它们可扩展到大规模网络。相反，其它优化策略通常涉及更高的时间复杂度，这是关于较高的时间复杂度。 以顶点的数目，甚至更高的比例尺的顶点数乘以边的数目。相应的NRL算法通常都是在x~(？) 保持矩阵，这是相当耗时的。努力降低矩阵分解的复杂性。例如，TADW[7]、DMF[8]和HSCA[20]利用稀疏性 原始顶点上下文矩阵。希望[35]和GraphWave[39]采用先进技术[96][97]进行矩阵特征分解。

## 8 今后的研究方向

在这一部分中，我们总结了六个潜在的研究方向和未来的挑战，以促进网络表征学习的研究。

任务依赖：到目前为止，大多数NRL算法都是独立于任务的，而特定于任务的NRL算法主要集中在半监督设置下的顶点分类上。只有V 最近，一些研究开始设计特定于任务的nrl算法，用于链路预测[35]、社区检测[98]、[99]、[100]、[101]、类不平衡学习[102]、主动学习。 [103]和信息检索[104]。使用网络表示学习作为中间层来解决目标任务的优点是，保存在 新的表示可以进一步使后续任务受益。因此，理想的特定于任务的NRL算法必须保留对特定任务至关重要的信息，以优化其性能。

理论：虽然已有的NRL算法的有效性已经通过实验进行了实证验证，但其基本工作机制还没有得到很好的理解。缺乏 F关于算法性质的理论分析，以及对良好的经验结果的贡献。为了更好地理解DeepWalk[6]、行[1]和node2vec[34]，[105]发现它们的 与拉普拉斯图的关系。然而，对网络表示学习进行深入的理论分析是必要的，因为它提供了对算法的深入理解，并有助于Interpr。 ET经验结果

动态：目前对网络表示学习的研究主要涉及静态网络。然而，在现实生活中，网络并不总是一成不变的。底层网络结构m 随着时间的推移，新的顶点/边出现，而一些旧的顶点/边消失。顶点/边也可以用一些时变信息来描述。动态网络具有独特的特性。 使静态网络嵌入失效的特性：(I)顶点内容特征可能随着时间的推移而漂移；(Ii)添加新的顶点/边需要学习或更新顶点表示。 离子是有效的；和(Iii)网络大小是不固定的。关于动态网络嵌入的工作相当有限；现有的大多数方法(例如[106]、[107]、[108])假定节点 SET是固定的，只处理因删除/添加边而引起的动力学问题。然而，一个更具有挑战性的问题是预测新的加法顶点的表示，它被引用为t。 没有“样本外”问题。一些尝试，如[52]，[109]，[110]是利用归纳学习来解决这个问题。他们从快照的网络中学习显式映射功能。 ，并利用此函数根据样本外顶点的属性或邻域结构等可用信息推断出它们的表示形式。然而，他们并没有考虑过何鸿燊。 w以增量方式更新现有的映射函数。如何在复杂动态领域设计高效的表示学习算法仍需进一步探索。

可扩展性：可伸缩性是推动网络表示学习研究的又一个驱动因素。几种nrl算法已经尝试扩展到具有l的大规模网络。 线性时间复杂度与顶点/边数有关。然而，可伸缩性仍然是一个重大挑战。我们的复杂性分析结果表明，随机游动和边缘运动 采用随机梯度下降优化的基于Deling的方法比采用特征分解和交替优化求解的基于矩阵因式分解的方法效率要高得多。 阿提。基于矩阵分解的方法在合并顶点属性和发现社区结构方面显示出了巨大的潜力，但它们的可扩展性需要改进以处理网络。 数以百万或数十亿计的顶点。基于深度学习的方法可以捕捉网络中的非线性，但其计算量通常较高.传统的深度学习架构 [111]GPU加速欧氏结构化数据训练的优势[111]。然而，网络没有这样的结构，因此需要新的解决方案来提高可伸缩性[112]。

异构和语义：异构信息网络的表示学习(HIN)是一个很有前途的研究方向。现有的大量工作集中在同质网上。 功嵌入，其中所有顶点都是相同类型的，边表示单个关系。然而，人们越来越需要研究具有不同类型的异构信息网络。 顶点和边，如DBLP、DBpedia和Flickr。HIN由不同类型的实体组成，如文本、图像或视频，实体之间的相互依赖关系非常复杂。 这使得很难度量丰富的语义和顶点之间的接近性，很难找到一个公共的、连贯的嵌入空间。[16]、[113]、[114]、[115]、[116]、[117]、[118]、 [119]、[120]、[121]研究了使用各种描述符(例如元路径或元结构)来捕获用于表示学习的远距离HIN顶点之间的语义邻近性。然而， 这方面的研究尚处于初级阶段。进一步的研究需要探索更好的方法来捕捉跨模式数据之间的邻近性，以及它们与网络架构之间的相互作用。 油温测量装置；介观结构.

另一个有趣的方向是研究有符号网络中的边语义，其中顶点既有正的关系，也有负的关系。签名网络在社交网络中无处不在。 H作为Epinions和Slashdot，允许用户与其他用户建立积极或消极的友谊/信任关系。负链接的存在使得传统的基于同质性的网络恢复。 背书学习算法不能直接应用。一些研究[122]、[123]、[124]通过直接建模链接的极性来解决签名网络表示学习问题。如何充分 有符号网络嵌入的编码网络结构和顶点属性仍然是一个有待解决的问题。

鲁棒性：现实世界中的网络经常存在噪声和不确定性，这使得传统的NRL算法无法产生稳定和鲁棒的表示。ANE(嵌入对抗性网络)[125]a 文章题目D Arga(对抗性正则化图自动编码器)[126]通过执行对抗性学习正则化器[58]来学习鲁棒顶点表示[58]。处理…存在的不确定性 边，敦促(不确定图嵌入)[127]编码边缘存在概率到顶点表示学习过程。有更多的研究成果是非常重要的。 网络表示学习的鲁棒性。

## 9 结论

本文综述了数据挖掘和机器学习领域中最先进的网络表示学习算法。我们提出了一种分类法，将现有的技术归纳为两种设置：无监督设置和半监督设置。根据他们使用的信息来源和方法 利用该方法，我们进一步将不同的方法分类为子组，回顾了每个子组中具有代表性的算法，并比较了它们的优缺点。我们总结了用于验证现有nrl算法的评估协议，比较了它们的经验性能和复杂性，并指出了一些新兴的研究方向和PROM。 伊辛扩展。我们的分类和分析不仅有助于研究人员全面了解该领域现有的研究方法，而且还为推进该领域的研究提供了丰富的资源。 RCH在网络表示学习中的应用。

## 致谢

这项工作是由美国国家科学基金会(Nsf)通过赠款IIS-1763452和澳大利亚研究理事会(ARC)通过赠款LP 160100630和DP 180100966提供的。张道坤 由中国奖学金委员会(Csc)引进，第201506300082名，澳大利亚数据61大学研究生奖学金.

## 参考文献

