​	



# 网络表示学习：综述

Daokun Zhang, Jie Yin, Xingquan Zhu Senior Member, IEEE, Chengqi Zhang Senior Member, IEEE

#### 摘要

随着信息技术的广泛使用，信息网络越来越受欢迎，以捕捉各种学科之间的复杂关系，例如社交网络，引文网络，电信网络和生物网络。分析这些网络揭示了社会生活的不同方面，例如社会结构，信息传播和沟通模式。然而，实际上，大规模的信息网络经常使网络分析任务在计算上昂贵或难以处理。最近已经提出网络表示学习作为通过保留网络拓扑结构，顶点内容和其他辅助信息将网络顶点嵌入到低维向量空间中的新学习范例。这有助于在新的向量空间中容易地处理原始网络以进行进一步分析。在本次调查中，我们对数据挖掘和机器学习领域的网络表示学习的当前文献进行了全面的回顾。我们提出新的分类法，根据潜在的学习机制，旨在保留的网络信息以及算法设计和方法，对最先进的网络表示学习技术进行分类和总结。我们总结了用于验证网络表示学习的评估协议，包括已发布的基准数据集，评估方法和开源算法。我们还进行了实证研究，以比较代表性算法在常见数据集上的性能，并分析其计算复杂性。最后，我们建议有前途的研究方向，以促进未来的研究。

**索引术语** - 信息网络，图挖掘，网络表示学习，网络嵌入。



## 1 简介

信息网络在社交网络，引文网络，电信网络和生物网络等形式的各种实际应用中变得无处不在。这些网络的规模范围从数百到数百万甚至数十亿个顶点[1]。 分析信息网络在许多学科的各种新兴应用中起着至关重要的作用。例如，在社交网络中，对于许多重要任务而言，将用户分类为有意义的社交群组是非常有帮助的，例如用户搜索，有针对性的广告和推荐; 在通信网络中，检测社区结构有助于更好地理解谣言传播过程; 在生物网络中，推断蛋白质之间的相互作用可以促进疾病的新治疗。 然而，对这些网络的有效分析在很大程度上依赖于网络的表示方式。通常，离散邻接矩阵用于表示网络，其仅捕获顶点之间的相邻关系。 实际上，这种简单的表示不能体现更复杂，更高阶的结构关系，例如路径，频繁的子结构等。因此，这样的传统方法经常使得许多网络分析任务在大规模网络上计算上昂贵且难以处理。 以社区检测为例，大多数现有算法涉及计算矩阵[2]的谱分解，其相对于顶点数至少具有二次时间复杂度。 这种计算开销使得算法很难扩展到具有数百万个顶点的大规模网络。

近年来，网络表示学习(NRL)引起了人们广泛的研究兴趣。NRL旨在学习潜在的、低维的网络顶点表示，同时保持网络拓扑结构、顶点内容和其他侧信息。在学习了新的顶点表示之后，通将传统的基于向量的机器学习算法应用到新的表示空间中，可以方便有效地完成网络分析任务。这就省去了直接应用于原始网络的复杂算法的必要性。

早期与网络表示学习有关的工作可以追溯到本世纪初，当时研究人员提出了作为降维技术的一部分的图嵌入算法。在给定一组独立的、同分布的数据点作为输入的情况下，图嵌入算法首先计算两两数据点之间的相似度，构造一个亲和图，例如k-最近邻图，然后将亲和图嵌入到低维空间中。其思想是在构造的图形所反映的高维数据几何中找到一个隐藏在高维数据几何中的低维流形结构，从而使连通点在新的嵌入空间中保持更接近。Isomap，局部线性嵌入（LLE）[4]和拉普拉斯算子图[5]是基于这个基本原理的算法实例。然而，图嵌入算法是在i.i.d.数据上设计的主要用于降维。这些算法中的大多数通常至少具有相对于顶点数量的二次时间复杂度，因此当它们应用于大规模网络时，可伸缩性是主要问题。

自2008年以来，重要的研究工作转向开发直接为复杂信息网络设计的有效和可伸缩的表示法学习技术。许多 已经提出了NRL算法，例如[6]、[7]、[8]、[9]来嵌入现有网络，在各种应用中显示出了良好的性能。这些算法将网络嵌入到一个潜在的、低维的网络中。 保留结构的邻近性和属性亲和力的空间，使得网络的原始顶点可以表示为低维向量。由此产生的紧凑的、低维的向量表示可以作为任何基于向量的机器学习算法的特征。这为在新的向量空间中轻松有效地处理各种网络分析任务铺平了道路，如节点分类[10]，[11]，链路预测[12]，[13]，聚类[2]，推荐[14]，[15]，相似搜索[16]，可视化[17]。利用向量表示来表示复杂的网络，现在已经逐渐发展到许多其他领域，如城市计算中的兴趣点推荐[15]和知识工程和数据库系统中的知识图谱搜索[18]。

### 1.1 挑战

尽管具有巨大的潜力，但网络表示学习本质上是困难的，并且面临着我们总结如下的几个关键挑战。

**结构保持**：为了学习信息丰富的顶点表示，网络表示学习应该保持网络结构，这样，在原始结构空间中彼此相似/接近的顶点就应该保持在原来的结构空间中。 在学习的向量空间中也可以类似地表示。然而，正如[19]，[20]所述，顶点之间的结构层次相似性不仅反映在局部邻域结构上，而且反映在更全局的社区结构上。因此， 在网络表示学习中，应同时保持局部结构和全局结构。

**内容保存**：除了结构信息外，许多网络的顶点都在属性上附加了丰富的内容。顶点属性不仅对网络的形成有着巨大的影响，而且为度量顶点之间的属性级相似性提供了直接的依据。因此，如果导入得当，属性内容可以补偿网络结构，以呈现更多的信息顶点表示。然而，由于这两个信息源的异质性， 如何有效地利用顶点属性使其补偿而不是恶化网络结构是一个开放的研究问题。

**数据稀疏性**：对于许多现实世界的信息网络来说，由于隐私或法律的限制，网络结构和顶点内容都存在数据稀疏的问题。在结构层，有时只观察到非常有限的链接，因此很难发现没有显式连接的顶点之间的结构级相关性。在顶点内容层次上，往往缺少许多顶点属性值，这增加了度量内容级顶点相似度的难度。因此，网络重组是一项具有挑战性的工作。 学习演示以克服数据稀疏问题。

**可伸缩性**：现实世界的网络，特别是社交网络，由数以百万或数十亿计的顶点组成。大规模的网络不仅挑战传统的网络分析任务，而且还挑战新生的网络表示学习任务。如果没有特别考虑，对于计算资源有限的大型网络，学习顶点表示可能会花费数月的时间，这实际上是不可行的，特别是对于涉及大量跟踪调优参数的情况。 因此，有必要设计能够有效学习顶点表示的NRL算法，同时保证大规模网络的有效性。

### 1.2我们的贡献

这项调查提供了一个全面的回顾了最新的网络表示法学习技术，重点是学习顶点表示。它不仅涵盖了早期维护网络结构的工作，而且还涵盖了最近将顶点内容和/或顶点标签作为辅助信息纳入学习的最新研究热潮,网络嵌入过程。希望能为研究界更好地理解(1)网络表征学习方法的新分类提供有益的指导，(2)网络表征学习方法的特点和独特性， 以及不同类型的网络嵌入方法的生态位，以及(3)资源和未来的挑战，以刺激该领域的研究。特别是，这项调查有四大贡献：

- 我们提出了新的分类法，根据潜在的学习机制、打算保存的网络信息以及算法设计和方法，对现有的网络表示学习技术进行分类。因此，这项调查为更好地了解现有工作提供了新的角度。
- 我们对最先进的网络表示学习算法进行了详细而深入的研究.与现有的图形嵌入调查相比，我们不仅审查了一个更全面的问题。 一组关于网络表示学习的研究工作，同时也为理解不同算法的优缺点提供了多方面的算法视角。
- 我们总结了用于验证网络表示学习技术的评估协议，包括已发布的基准数据集、评估方法和开源算法。我们还进行了实证研究，比较了代表性算法的性能，并详细分析了计算复杂性。
- 为了促进未来的研究，我们提出了未来网络表示学习的六个有前途的研究方向，总结了当前研究工作的局限性，并提出了新的研究思路对每个方向。

### 1.3 相关调查和差异

在最近的文献中，有一些与图嵌入和表示学习相关的调查。第一个是[21]，它回顾了一些有代表性的网络表示学习方法，并围绕表征学习的概念及其与其他相关领域的联系（如降维，深度学习和网络科学）访问了一些关键概念。 [22]从方法论角度对代表性网络嵌入算法进行了分类。 [23]回顾了一些表示学习方法，用于在编码器 - 解码器框架内嵌入单个顶点和子图，特别是那些受深度学习启发的子图。 然而，这些调查所审查的大多数嵌入式算法主要保留了净工作结构。 最近，[24]，[25]扩展到涵盖利用其他辅助信息（如顶点属性和/或顶点标签）来利用表示学习的工作。

总之，现有的调查有以下局限性。首先，他们通常只关注一个分类来对现有的工作进行分类。它们都没有提供一个多方面的视角来分析最先进的网络表示学习技术，并比较它们的优缺点。第二，现有的调查没有 没有深入分析算法的复杂性和优化方法，或者它们没有提供实证结果来比较不同算法的性能。第三，缺乏对现有资源的总结，如公开可用的数据集和开源算法，以便利今后的研究。在这项工作中，我们提供了最全面的调查，以弥补差距。我们相信这项调查对研究人员和从业者都有好处，使他们对不同的研究方法有更深入的了解。 并提供丰富的资源，以促进未来在该领域的研究。

### 1.4 综述的结构

本调查的其余部分组织如下。在第二节中，我们提供了理解问题和接下来讨论的模型所需的预备和定义。第三节提出新的分类单元。 对现有的网络表示学习技术进行分类。第4节和第5节分别回顾了两类有代表性的算法。第六节讨论了网络表示学习的成功应用。在第七节中，我们总结了用于验证网络表示学习的评估协议， 并对算法的性能和复杂度进行了比较。我们在第8节讨论了潜在的研究方向，并在第9节总结了调查结果。

## 2 符号和定义

在本节中，作为预备，我们首先定义用于讨论模型的重要术语，然后是网络表示学习问题的正式定义。为了便于表示，我们首先定义了将在整个调查过程中使用的通用符号列表，如表1所示。

|           G            |      给定信息网络      |
| :--------------------: | :--------------------: |
|         **V**          | 给定信息网络中的顶点集 |
|         **E**          |  给定信息网络中的边集  |
|       **\|V\|**        |         顶点数         |
|       **\|E\|**        |          边数          |
|           m            |       顶点属性数       |
|           d            |   学习顶点表示的维数   |
|  $\ X\in R^{\|V\|* m}$    |      顶点属性矩阵      |
|           Y            |       顶点标签集       |
|         \|Y\|          |       顶点标签数       |
| $\ Y\in R^{ \|V\| * \|Y\|}$ |      顶点标号矩阵      |

**定义1(信息网络)**：一个信息网络被定义为G=(V，E，X，Y)，其中V表示一组顶点，|V|=G中的顶点数。E⊆(V×V)表示连接顶点的一组边。 $\ X∈R^{| V |×m}$ 是顶点属性矩阵，其中m是属性的数量，元素X<sub>ij</sub>是第j个属性的第i个顶点的值。$\  Y∈R^{| V |×| Y |}$ 是顶点标签矩阵，Y是一组标签。 如果第i个顶点具有第k个标签，元素$\ Y_{ik} = 1$; 否则，$\ Y_{ik} $= -1。 由于隐私问题或信息访问困难，顶点属性矩阵X通常是稀疏的，并且顶点标签矩阵Y通常是未观察到的或部分观察到的。 对于每个  $\ (v_i，v_j）∈E$，如果信息网络G是无向的，我们有$\ \(v_j，v_i）∈E$; 如果G是有向的，则$\ \(v_j，v_i)$ 不一定属于E.如果信息网络是二进制的（未加权的），则每个边缘$\ \(v_i，v_j）∈E$也与权重$\ W_{ij}$相关联，其等于1。

从直觉上看，信息网络的产生不是毫无根据的，而是由某些潜在机制引导或支配的。其潜在机制虽然鲜为人知，但可以从信息网络中广泛存在的一些网络属性中反映出来。因此，公共网络属性对于学习顶点表示是必不可少的，这些顶点表示可以提供信息，以准确地解释信息网络。下面，我们介绍了几种常见的网络属性。

**定义2(一阶邻近)**：一阶邻近是两个连通顶点之间的局部两两邻近[1]。对于每个顶点对 $  ( v_{i}，v_j ) $，如果 $\ \(v_i，v_j)∈E$，则$\ v_i和v_j$之间的一阶邻近度为$\ w_{ij}$；否则，$\ v_i$ 和 $\ v_j$ 之间的一阶邻近度为0。一阶邻近捕获顶点之间的直接邻居关系。

**定义3(二阶邻近和高阶接近)**：二阶邻近捕获了每对顶点之间的两步关系[1]。对于每个顶点对($v_i$，$v_j$)，二阶邻近度取决于公共邻居的数目。 由两个顶点共享，也可以通过从$ v_i$到$v_j$的两步跃迁概率进行等效度量。与二阶邻近相比，高阶邻近[26]捕捉到了更多的全局结构，揭示了每一对顶点之间的k阶(k≥3)关系。对于每个顶点对($V_i$) ，高阶邻近度由k阶(k≥3)从顶点$v_i$到顶点$v_j$的跃迁概率来度量，这也可以由从$v_i$到$ v_j$的k步(k≥3)路径数来反映。 二阶和高阶近邻捕获了具有相似结构上下文的一对间接连接的顶点之间的相似性。

**定义4(结构作用邻近性)**：结构角色近邻描述了顶点之间的相似性，如链的边缘、恒星的中心和两个社区之间的桥梁等，在它们的邻域中充当着相似的角色。在通信和交通网络中，顶点的结构作用是表征其性质的重要因素。不同于一阶,二阶和高阶近邻,捕捉的相似性网络中顶点相互接近,结构作用接近试图发现遥远的顶点之间的相似性而共享等效结构的角色。如图1所示，顶点4和顶点12彼此相距很远，而它们作为恒星的中心扮演着同样的结构角色。因此，它们在结构上具有高度的接近性。

**定义5(社区内邻近性)**：内聚邻近度是同一社区内顶点之间的两两接近度。许多网络具有社区结构，其中同一社区内的顶点-顶点连接是密集的，但是社区外的顶点连接是稀疏的[27]。作为集群结构，一个社区保留了它内部顶点的某些公共属性。例如，在社会网络中，社区可能代表社会群体的利益或背景;在引文网络中，社区可能代表同一主题的相关论文。社区内的邻近性通过保持同一个社区[28]中顶点共享的公共属性来获取这种集群结构。

**顶点属性：**除了网络结构外，顶点属性还可以为度量顶点之间的内容级相似性提供直接证据。如[7]、[20]、[29]所示，顶点属性和网络结构可以互相帮助滤除噪声信息，并相互补偿，共同学习信息顶点表示。

**顶点标签：**顶点标签向某些类或组提供关于每个网络顶点的语义分类的直接信息。顶点标签受到网络结构和顶点属性的强烈影响和内在关联[30]。虽然顶点标签通常是部分观察到的，但是当与网络结构和顶点属性相结合时，它们会鼓励网络结构和顶点属性一致的标记。并帮助学习信息丰富和区分性的顶点表示。



> ![1542019744893](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542019744893.png)
>
> 图1.结构角色邻近性的一个说明性的例子。顶点4和顶点12具有相似的结构作用，但彼此相距很远。

**定义6(网络表示学习(NRL)**：给定一个信息网络G =（V，E，X，Y），通过整合E中的网络结构，X中的顶点属性和Y中的顶点标签（如果可用），网络表示学习的任务是学习映射函数 $\ f ：v →r_v∈R^d​$其中$r_v​$是顶点v的学习矢量表示，d是学习表示的维度。 变换f预先提供原始网络信息，使得在原始网络中类似的两个顶点也应该在学习矢量空间中类似地表示。

学习的顶点表示应满足以下条件：

(1)低维，即$d<<|V|$，换句话说，为了提高内存效率和后续网络分析任务的可扩展性，学习顶点表示的维数要比原始邻接矩阵表示的维数小得多;

(2)信息表示，即学习的顶点表示应保持由网络结构、顶点属性和顶点标签(如果有的话)所反映的顶点邻近性；

(3)连续的，即学习的顶点表示应该具有连续的实数，以支持后续的网络分析任务，如顶点分类、顶点聚类或异常检测, 并具有平滑的决策边界，以确保这些任务的鲁棒性。



> ![1542019955651](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542019955651.png)
>
> 图2.网络表示学习的概念观。(A)中的顶点使用基于其社区信息的ID和颜色进行索引。网络表示学习 (B)将所有顶点转化为一个二维向量空间，从而使结构接近的顶点在新的嵌入空间中彼此接近。

图2示出了使用玩具网络的网络表示学习的概念视图。在这种情况下，只考虑网络结构来学习顶点表示。提供信息 网络如图所示。2(A)，NRL的目标是将所有网络顶点嵌入到低维空间中，如图所示。2(B)。在嵌入空间中，具有结构邻近点的顶点彼此间有着紧密的表示。例如，由于顶点7和顶点8是直接连接的，一阶邻近使得它们在嵌入空间中彼此紧密相连。虽然顶点2和顶点5不是直接连接的，但由于它们具有很高的二阶邻近性，所以它们之间也是紧密地嵌入在一起的。 这两个顶点。顶点20和顶点25不直接相连，也不共享共同的直接邻居。然而，它们是由许多k阶路径(k≥3)连接起来的，这证明了它们具有高阶邻近性.因此，顶点20和顶点25也具有紧密的嵌入。与其他顶点不同，顶点10-16显然属于原始网络中的同一个社区。这种社区内的邻近性保证了这些顶点的图像在嵌入空间中也显示出清晰的簇结构。

> ![1542020140572](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/%255CUsers%255CDELL%255CAppData%255CRoaming%255CTypora%255Ctypora-user-images%255C1542020140572.png)
>
> 图3.我们将网络表示学习分为两类：无监督网络表示学习和半监督网络表示学习，这取决于顶点标签是否是可供学习。对于每一组，我们进一步将方法划分为两个子组，这取决于表示学习是基于网络拓扑结构，还是基于来自节点内容的信息。

## 3 范畴化

在本节中，我们提出了一个新的分类法来对现有的网络表示学习技术进行分类，如图3所示。分类的第一层是基于是否为学习提供顶点标签。根据这一点，我们将网络表示学习分为两类：无监督网络表示学习和半监督网络表示学习。

**无监督网络表示学习**：在此设置中，没有为学习顶点表示提供标记顶点。因此，网络表示学习被认为是一项独立于后续学习的通用任务，顶点表示是以无监督的方式学习的。

大多数现有的NRL算法都属于这一类。在新的嵌入空间中学习顶点表示后，它们被视为用于各种学习任务的任何基于向量的算法的特征。无监督的NRL算法可以根据可供学习的网络信息类型进一步分为两个子组：只保留网的无监督结构保持方法， 非监督内容增强方法，结合顶点属性和网络结构来学习联合顶点嵌入。

**半监督网络表示学习**：在这种情况下，存在一些用于表示学习的标记顶点。因为顶点标签在确定每个顶点的分类中起着至关重要的作用，并且与网络结构和顶点属性有很强的相关性，提出了利用网络中的顶点标签来寻找更有效的联合向量表示的网络表示学习方法。

在这种情况下，网络表示学习与监督学习任务(如顶点分类)相结合。为了同时优化顶点表示的学习和网络顶点的分类，常提出一个统一的目标函数。因此，对于不同的类别，学习的顶点表示可以是信息性的，也可以是区分性的。半监督NRL算法也可分为两大类：半监督结构保持法和半监督内容增广法。

> ![1542022260814](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/1542022260814.png)
> 表2.按用于学习的信息源分列的NRL算法摘要

表2根据用于表示学习的信息源总结了所有NRL算法。一般来说，信息源主要有三种类型：网络结构、顶点属性和顶点标签。大多数无监督的NRL算法专注于保留用于学习顶点表示的网络结构，并且仅少数算法（例如，TADW [7]，HSCA [8]）尝试利用顶点属性。相反，在半监督学习设置下，有一半的算法打算将顶点属性与网络结构和顶点标签相结合来学习顶点表示。在这两种设置中，大多数算法都集中于保留微观结构，而极少数算法（例如，MNMF [28]，DP [41]，HARP [42]）试图利用介观和宏观结构。

从算法的角度，可以将上述两种不同环境下的网络表示学习方法归纳为五类。

1)基于矩阵分解的方法。基于矩阵分解的方法以矩阵的形式表示网络顶点之间的联系，并利用矩阵因式分解来获得嵌入。为了保持网络结构，构造了不同类型的矩阵，如k阶转移概率矩阵、模块化矩阵或顶点上下文矩阵[7]。通过假设这种高维顶点表示只受少量潜在因素的影响，利用矩阵因式分解来嵌入高维顶点表示。 变成一个潜在的、低维的结构保持空间。因子化策略根据其目标在不同算法之间变化。例如，在模块化最大化方法[31]中，特征分解是在模块矩阵上进行的，以学习社区指示顶点表示[53]。在TADW算法[7]中，归纳矩阵分解[54]是在顶点表示学习中对顶点上下文矩阵同时保持顶点文本特征和网络结构的一种方法。尽管基于矩阵分解的方法在学习信息顶点表示方面已被证明是有效的，可伸缩性是一个主要的瓶颈，因为在一个包含数百万行和列的矩阵上进行因式分解是内存密集型的，计算成本很高，有时甚至是不可行的。 

2)基于随机游走的方法。对于可伸缩的顶点表示学习，利用随机游走来捕获顶点之间的结构关系。通过执行截断的随机游走，信息网络被转换为顶点序列的集合，其中顶点 - 上下文对的出现频率测量它们之间的结构距离。借鉴词表示学习的思想[55]，[56]，通过使用每个顶点来预测其上下文来学习顶点表示。DeepWalk[6]是利用随机游动学习顶点表示的先驱工作。node2vec[34]进一步利用有偏随机游走策略来捕获更灵活的上下文结构。

作为仅保留版本结构的扩展，DDRW[45]、gene[48]和SemNE[49]等算法将顶点标签与网络结构结合起来，以利用表示学习，PPNE[44]导入顶点属性，Tri-DNR[50]使用顶点标签和属性强制执行模型。由于这些模型可以通过在线方式进行培训，它们具有很大的扩展潜力。

> ![1542022302610](https://raw.githubusercontent.com/mp5088643/Machine-learning-and-data-science-notebook/master/images/%E7%BD%91%E7%BB%9C%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0/1542022302610.png)
>
> NRL算法的方法论分类

3)基于边建模的方法。与利用矩阵或随机游走捕捉网络结构的方法不同，基于边建模的方法直接从顶点-顶点连接中学习顶点表示。为了捕获一阶和二阶邻近，LINE [1]分别对连接的顶点建模联合概率分布和条件概率分布。为了学习链接文档的表示，LDE[51]通过最大化连接文档之间的条件概率来建模文档-文档关系。pRBM[29]通过使连通顶点的隐式RBM表示类似于彼此，从而使RBM[57]模型适应于链接数据。图GAN[37]采用生成对抗性网(GAN)[58] 精确建模顶点连通概率。与基于矩阵分解和随机游走的方法相比，基于边建模的方法效率更高。这些方法不能捕获全局网络结构，因为它们只考虑可观测的顶点连通性信息。

4)基于深度学习的方法。为了提取复杂的结构特征和学习深度、高度非线性的顶点表示，还将深度学习技术[59]、[60]应用于网络表示学习。例如，DNGR[9]将叠加去噪自动编码器(SDAE)[60]应用于高维矩阵表示，以学习深度低维顶点表示。SDNE[19]使用半实物 -监督深度自编码模型[59]在网络结构中建立非线性模型。基于深度学习的方法具有捕捉网络非线性的能力，但计算时间往往较长。传统的深度学习架构是为一维，二维， 或者三维欧氏结构化数据，但是需要在非欧几里德结构化数据(如图)上开发有效的解决方案。

5)杂交方法。其他一些方法利用上述方法的混合来学习顶点表示。例如，DP[41]用度惩罚原理增强了谱嵌入[5]和DeepWalk[6]以保持宏观无尺度性质。HARP [42]利用基于随机游走的方法（DeepWalk [6]和node2vec [34]）和基于边缘建模的方法（LINE [1]）来学习从小型采样网络到原始网络的顶点表示。

我们总结了五种网络表示学习技术，并在表3中比较了它们的优缺点。

##4 无监督网络表示学习

在本节中，我们回顾了无监督的网络表示学习方法，将它们分成两个子部分，如图3所示。在此基础上，我们总结了这两种方法的主要特点，并比较了这两种方法的不同之处。

### 4.1 无监督结构保持网络表示学习

结构保持网络表示学习是指打算保留网络结构的方法，在新的嵌入空间中，对于原始网络空间中的顶点彼此相近的概念，应该以相似的方式来表示。在这一类别中，研究工作一直集中在设计各种模型，以尽可能捕获原始网络传递的结构信息。

> ![1542075579686](F:\Machine-learning-and-data-science-notebook\images\网络表示学习\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542075579686.png)
>
> 图4.网络结构分类。

我们将学习顶点表示的网络结构归纳为三种类型：(I)微观结构，包括局部近邻，即一阶、二阶和高阶邻近，(Ii)介观结构，捕捉结构角色邻近性及群落内邻近性。以及(Iii)宏观结构，它捕捉全球网络属性，如无标度属性或小世界属性。如图4所示，下面的小节是根据我们对网络结构的分类来组织的。

#### 4.1.1 保存微观结构的NRL

这类NRL算法旨在保持其邻域中直接或间接连接的顶点之间的局部结构信息，包括一阶，二阶和高阶邻近。一阶接近捕获同质性，即直接连通的顶点趋向于彼此相似，而二阶和高阶近邻捕获了共享公共领域的顶点之间的相似性。大多数保持结构的nrl算法都属于这一类。

**DeepWalk：**DeepWalk[6]将Skip-Gram模型[55][56]的思想推广到学习潜在的顶点表示。 在网络中，通过对自然语言句子和短随机游动序列进行类比。图5给出了DeepWalk的工作流程。给定长度为$\ L，{v_1，v_2，··，v_L}$ 的随机游动序列，遵循Skip-Gram，DeepWalk利用它来预测其上下文顶点，这是通过优化问题来实现的。

$$ \min\limits_{f} -logPr(\{v_{i-t},...，v_{i+t} \} \backslash v_i|f(v_i)  )$$    注：Pr 条件概率

其中$\ \{v_{i−t}，···，v_{i+t} \} \backslash v_i$ 是t窗口大小内顶点$v_i$的上下文顶点。在条件独立性假设下，概率$Pr(\{v_{i-t},...，v_{i+t} \} \backslash v_i|f(v_i) $近似为

$$Pr(\{v_{i-t},...，v_{i+t} \} \backslash v_i|f(v_i) =\prod_{j=i-t,j\not=i}^{i+t} Pr(v_j |f(v_i))$$

按照DeepWalk的学习结构，在随机游动序列中共享相似上下文顶点的顶点应该在新的嵌入空间中得到紧密的表示。考虑到随机游动序列中的上下文顶点描述邻域结构，DeepWalk实际上表示在嵌入空间中共享相似邻域(直接或间接)的顶点，因此，二阶和高阶邻近性被保留下来。

**大规模信息网络嵌入(LINE)：**LINE[1]不是利用随机游走来捕获网络结构，而是通过显式建模一阶和二阶邻近来学习顶点表示。为了保持一阶接近度，LINE使下列目标最小化：

$$ O_1 = d(\hat{p1}(·, ·), p1(·, ·)).$$

对于$(v_i，v_j)∈E$的每个顶点对$v_i和v_j，p_1(·，·)$是由它们的潜在嵌入$r_{v_i}$和${r_{v_j}}$建模的联合分布。$\hat{p}_1(vi，vj)$是它们之间的经验分布。d(·，·)是两个分布之间的距离。

为了保持二阶邻近，LINE最小化了以下目标：

$O2 =\sum\limits_{vi∈V}λ_id(\hat p_2(·|v_i), p_2(·|v_i))$,

其中$p_2(·|v_i)$是每一个用顶点嵌入建模的$v_i\in V$的上下文条件分布,$\hat p_2(·|v_i)$是经验条件分布，$λ_i$是顶点vi的威望。这里，顶点上下文是由它的邻居确定的，也就是说，对于每个$v_j$，$v_j$是vi的上下文，当且仅当$(v_i，v_j)∈E$。

通过最小化这两个目标，线学习了两种保持一阶和二阶邻近性的顶点表示。并以它们的级联作为最终的顶点表示。

**GraRep**:根据DeepWalk [6]的思想，GraRep [26]扩展了skip-gram模型以捕获高阶邻近度，即共享共同k阶邻域（k≥1）的顶点应该具有相似的潜在表示。具体来说，对于 每个顶点，GraRep将其k步邻居（k≥1）定义为上下文顶点，并且对于每个1≤k≤K，学习k步顶点表示，GraRep使用skip-gram的矩阵分解版本.

$$[U^k,Σ ^k,V^k]=SVD(X^k)$$

其中，$x_k$是log k阶转移概率矩阵。顶点$v_i$的k阶表示构造为矩阵$U^k_d(Σ^k_d)^{\frac{1}{2}}$的第1行，其中$u^k_d$是$u^k$的第一d列。 $Σ^k_d$是由上d奇异值组成的对角矩阵。在学习k步顶点表示之后，GraRep将它们作为最终的顶点表示连接在一起.

**图表示的深层神经网络(DNGR)**:克服截尾随机游动在顶点上下文信息挖掘中的不足,即序列边界点获取正确上下文信息的困难以及步长和步数的确定困难。DNGR[9]利用随机冲浪模型捕捉每一对顶点之间的上下文相关性，并将它们保留为v-维顶点表示X。为了提取复杂特征和模型非线性，DNGR将堆叠去噪自动编码器（SDAE）[60]应用于高维顶点表示X，以学习深度低维顶点表示。

**结构深度网络嵌入(SDNE):**SDNE[19]是一种基于深度学习的方法，它使用半监督的深度自编码模型来捕获网络结构中的非线性。在无监督部分,SDNE通过重构v维顶点邻接矩阵表示来学习二阶邻近保持顶点表示，该表示试图最小化。$$L_{2nd}=\sum\limits_{i=1}^{|V|}||(r_{v_i}^{(0)}- \hat r_{v_i}^{(0)})\bigodot b_i||_2^2$$

其中$r_{v_i}^{(0)}=S_i$：是输入表示,$\hat r_{v_i}^{(0)}$是重构表示。$b_i$是一个权重向量，用于更多地惩罚**S**的非零元素的构造误差。

> ![1542082084727](F:\Machine-learning-and-data-science-notebook\images\网络表示学习\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542082084727.png)
>
> 图6.node2vec考虑的两种不同的邻域抽样策略：BFS和DFS。

在监督分量中，SDNE通过惩罚嵌入空间中连通顶点之间的距离来引入一阶邻近度。这一目标的损失函数定义为：$$\LARGE L_{1st}=\sum\limits_{i,j=1}^{|V|}S_{i,j}||r_{v_i}^{(K)}-r_{v_j}^{(K)}||$$

其中$r^{(K)}_{v_i}$是顶点$ v_{i}$的第K层表示，K是隐藏层数。总之，SDNE使联合目标函数最小化：$L = L_{2nd} + αL_{1st} + νL_{reg}$

其中，$L_reg$是一个正则化的术语，以防止过度拟合。求解(8)的极小化后，对于顶点$v_i$，以K层表示$r_{v_i}^{(K)}$作为其表示$r_{v_i}$。

**node2vec:** 与为每个顶点定义邻域（上下文）的严格策略相反，node2vec [34]设计了一种灵活的邻域采样策略，即偏置随机游走，它在两种优先采样策略之间平滑插值，即广度优先采样(BFS)和深度优先采样(DFS)，如图6所示。在node2vec中利用的偏置随机游走可以更好地保留二阶和高阶邻近。

按照 skip-gram的架构，给定由有偏随机游动生成的邻域顶点$N(V_i)$集合，node2vec通过优化发生概率来学习顶点表示$f(V_i)$。 以顶点$v_i，f(V_i)$的表示为条件的邻域顶点$N(V_{i})$的y：$$\LARGE \max  \limits_f \sum\limits_{v_i\in V} log Pr(N(v_i)|f(v_i))$$

**高阶邻近保留嵌入(HOPE):**HOPE [35]学习在有向网络中捕捉非对称高阶邻近性的顶点表示。 在无向网络中，传递性是对称的，但在有向网络中它是不对称的。 例如，在有向网络中，如果存在从顶点$ v_i $到顶点$v_j $以及从顶点$ v_j $到顶点$ v_k $的有向链接，则更可能具有从$ v_i $到$v_k$的有向链接，但是不具有从$v_k$到$ v_i $的有向链接。

为了保持不对称传递性，HOPE分别学习了两个顶点嵌入向量$U^s，U^t∈R^{| V |×d} $，它们被称为源和目标嵌入向量。 在从四个邻近度量构建高阶邻近矩阵S之后，即Katz Index [61]，Rooted PageRank [62]，Common Neighbors和AdamicAdar。 HOPE通过解决以下矩阵分解问题来学习顶点嵌入:$$\min\limits_{U_s,U_t}||S-U^s*U^{t^T}||^2_F$$

> ![1542088160404](F:\Machine-learning-and-data-science-notebook\images\网络表示学习\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542088160404.png)
>
> 表4.保持微观结构的NRL算法概述

**非对称邻近保持图嵌入(APP):**APP[36]是另一种NRL算法，其目的是通过使用蒙特卡罗方法来近似非对称根。 PageRank邻近度[62]。与HOPE类似，APP对每个顶点$v_{i}$有两个表示，一个表示为源角色$r^{s}_{v_i}$，另一个表示为目标角色$r^{t}_{v_i}$。开始的每个抽样路径 以$ v_{j} $结尾的表示是通过最大化目标顶点$v_{j}$的出现概率来学习的，其发生概率取决于源顶点$ v_i $：

$$ \LARGE{ Pr(V_j|V_i)=\frac {exp(r^s_{v_i}·r^t_{v_j})}{\sum_{v∈V} exp(r^s_{v_i}·r^t_v)}}        (11)$$

**GraphGAN:**GraphGAN[37]通过对抗性学习框架对连通性行为建模来学习顶点表示。受GAN(生成对抗性网络)[58]的启发，GraphGAN通过两个组件工作：(I)生成器$G (v|v_c)$，它适合$V$之间连接到$V_c$的顶点的分布，并生成可能的连通顶点。 (Ii)判别器$D(v，v_c)$，它为顶点对$(v，v_c)$输出一个连接概率，以区分$ G(v|v_c) $生成的顶点对与地面真值之间的连接概率。$G(v|v_c)$和$D(v，v_c)$竞争的方式是：$ G(v|v_c)$尽可能地拟合真实的连接分布，生成假连通顶点对来欺骗$D(v,v_c)$，而$D(v，v_c)$则试图增加它的判别能力，从而将 $G(v|v_c)$ 产生的顶点对与实际真值区分开来。竞争是通过以下极小极大博弈来实现的：$$ \Large \min\limits_{θ_G}\max\limits_{θ_D}\sum \limits _{v_c∈V} (E_{v\sim Pr_{true(·|v_c)}}[logD(v,v_c;θ_D)]+E_{v\sim G(v|v_c;θ_G)}[log(1 − D(v, v_c; θ_D))  (12) $$

在这里，$G(v|v_c;θ_G)和D(v，v_c；θ_D)$定义如下：

$$ \Large (v|v_c;θ_G)=\frac{exp(g_v,g_{v_c})}{\sum_{v\neq v_c}exp(g_v,g_{v_c})}, $$

$$ \Large D(v, v_c; θ_D)=\frac{1}{1 + exp(d_v · d_{v_c})}$$

其中$g_v∈R_k$和$d_V∈R_k$分别是发生器和判别器的表示向量，$θ_D={d_V}，θ_G={g_v}$。在公式中的极小极大博弈之后。(12)求解后，$g_v$作为最终顶点表示。

**概要：**表4总结了保留显微结构的邻近性的NRL算法。这个类别中的大多数算法保持二阶和高阶邻近性，而只有LINE [1]，SDNE [19]和GraphGAN [37]考虑了一阶临近性。 从方法论的角度来看，DeepWalk [6]，node2vec [34]和APP [36]采用随机游走来捕捉顶点邻域结构。 GraRep[26]和HOPE[35]的实现是通过对$|V| \times |V| $维矩阵进行因式分解，使其难以扩展。LINE [1]和GraphGAN [37]直接模拟连通行为，而基于深度学习的方法（DNGR [9]和SDNE [19]）则学习非线性顶点表示。

#### 4.1.2 结构作用邻近保持NRL

除了局部连通模式外，顶点通常在介观水平上具有相似的结构角色，如恒星中心或团成员。结构角色接近保持NRL目标 嵌入彼此相距较远但彼此结构相似的角色的顶点。这不仅方便了下游结构角色相关的任务，而且增强了 保存NRL的显微结构。

**struct2vec：**struct2vec[38]首先将顶点结构角色相似性编码成一个多层图，其中，每一层边缘的权重由相应尺度上的结构角色差异决定。 然后在多层图上执行DeepWalk[6]学习顶点表示，这样，在新的表示空间中，多层图(具有高结构角色相似性)中的顶点就被紧密地嵌入到了新的表示空间中。

对于每个顶点对$(v_i，v_j)$，考虑到它们的邻域在k步内形成的k-hop邻域，它们在k级的结构距离，$D_k(v_i，v_j)$被定义为

$D_k(v_i, v_j ) = D_{k−1}(v_i, v_j ) + g(s(R_k(v_i)), s(R_k(v_j )))    $                                          $(14)$

其中$R_k(V_i)$是$ v_{i} $的k-hop邻域中的顶点集合，$s(R_k(V_i)$是$R_k(V_{i})$中顶点的有序度序列，$g(s(R_k(V_i)，s(R_k(V_j)))$是有序度序列$s(R_k(V_i)，s(R_k(V_j))$之间的距离。当k=0时，$D_0(v_i，v_j)$是顶点$v_i $和$ v_{j} $之间的度差。

**GraphWave：**利用谱图小波扩散模式，GraphWave[39]将顶点邻域结构嵌入到一个低维空间中，并保持了结构作用的接近性。假设，如果网络中两个远距离的顶点具有相似的结构角色，从它们开始的图小波将以相似的方式在它们的邻域中传播。

对于顶点$v_{k}$，其谱图小波系数$Ψ_k$定义为 $Ψ_k = UDiag(g_s(λ_1), · · · , g_s(λ_{|V |}))U^Tδ_k$,

其中$U$是图Laplacian L的特征向量矩阵，$λ_1，··，λ_{|V|}$是特征值，$g_s(λ)=exp(−λs)$是热核，$δ_k$是k的独热向量。将$ Ψ_k $作为概率分布，将$ Ψ_k $中的谱小波分布模式编码为其经验特征函数：$$ \Large \phi_k(t)=\frac{1}{|V|}\sum\limits_{m=1}^{|V|}e^{itΨ_{km}}$$

然后，通过采样$\phi k(T)$在d点$t_1、t_2、··、t_d$上的二维参数函数，得到$v_k$的低维表示：$$f(v_k) = [Re(φ_k(t_1)), · · · ,Re(φ_k(t_d)),Im(φ_k(t_1)), · · · ,Im(φ_k(t_d))]$$

**结构和邻域相似性保持网络嵌入(SNS):** SNS[40]增强了一种具有结构角色邻近性的基于随机游走的方法。为了保留顶点结构角色，SNS将每个顶点表示为一个图形度向量，每个元素是给定顶点被图形的相应轨道接触的次数。图形度向量是用于度量顶点结构角色相似度.

给定一个顶点$ v_{i} $，SNS使用它的上下文顶点$C(V_i)$和结构相似的顶点$S(V_i)$来预测它的存在，这是通过最大限度地提高以下概率来实现的：$$ \Large Pr(v_i|C(v_i), S(v_i)) = \frac{exp(r^`_{v_i}· h_{v_i})}{\sum_{u∈V}exp(r^`_u · h_{v_i})} $$, (18)

其中$r`_{v_i}$是$v_i$的输出表示，$h_{v_i}$是用于预测$ v_{i} $的隐层表示，对$C(V_i)$和$S(V_i)$中的每个u,都是从输入表示$r_u$聚合而来的。

**概要：**struct2vec[38]和GraphWave[39]利用结构角色邻近性来学习顶点表示，从而促进特定的结构角色相关任务，例如，交通网络中的顶点分类，而SNS[40]增强了基于随机游动的微观结构保持NRL算法的结构角色邻近性。技术上讲，struct2vec和SNS采用随机游走，图形波采用矩阵因式分解。

#### 4.1.3 社区内邻近性保持NRL

现实世界网络展示的另一个有趣的特征是社区结构，其中，顶点在同一社区内密集地连接在一起，但却很少与其他社区的顶点相连。例如，在社交网络中，来自同一个利益集团或附属机构的人通常组成一个社区。在引文网络中，关于类似研究主题的论文往往相互引用。社区内保持NRL的目的是利用具有关键顶点属性的社区结构来学习信息顶点表示。

**学习潜在的社会维度：**基于社会维度的NRL算法试图通过成员或隶属于多个社会维度来构建社会行为者的嵌入。为了推断出这些潜在的社会维度，本文考虑了社会网络中的“社区”现象，指出社会行动者共享相似的属性，通常会形成群体内部关系更紧密的群体。因此，这个问题可以归结为一个经典的网络分析任务-----社区检测，其目的是发现一组群内连接比组间连接更密集的群。三种聚类技术，包括模块化最大化[31]，谱聚类[32]和边缘聚类[33]被用来发现潜在的社会维度。每个社会维度描述了一个顶点属于一个看似可信的从属关系的可能性。这些方法保存全局社区结构，但忽略了局部结构特性，例如一阶和二阶邻近性.

**模块化非负矩阵分解(MNMF):**M-NMF[28]增强了二阶和高阶与更广泛的社区结构的接近性，以学习更多信息丰富的顶点嵌入$U∈R^{|V| \times d}$ 使用下列目标：

$$\min\limits_{M,U,H,C}||S-MU^T||^2_F + α||H-UC^T||^2_F-βtr(H^TBH)     $$              公式(19)

 $$ s.t., M ≥ 0, U ≥ 0, H ≥ 0, C ≥ 0, tr(H^TH) = |V |,$$

其中，顶点嵌入$U$是通过最小化$||S-MU^T||^2_F$来学习的，其中$S∈R^{|V| \times |V|}$是点对接近矩阵， 作为表示法的时候，它捕捉了t时的二阶和高阶接近度。社区指示顶点嵌入$H$是通过最大化$tr(H^TBH)$来学习的，它本质上是以B为模块化矩阵的模块化最大化的目标 。通过引入一个社区表示矩阵C，$||H-UC^T||^2_F$上的极小化使得这两个嵌入相互一致。

**摘要：**学习潜在社会维度的算法[31]、[32]、[33]只考虑了学习顶点表示的社区结构，而M-NMF[28]将微观结构(二阶和高阶邻近)与群落内的邻近性结合起来。这些方法主要依靠矩阵分解来检测社区结构，这使其难以扩展。

#### 4.1.4 宏观结构保持NRL

宏观结构保持方法的目的是在宏观上保持某些全局网络性质。最近只有很少的研究为此目的而开展。

**程度惩罚原则(DP)：**许多真实世界的网络都呈现出宏观无尺度的特性，它描述了顶点度服从长尾分布的现象。也就是说，大多数顶点是稀疏连通的，只有少数顶点具有密集的边。为了捕捉无标度属性，[41]提出了度惩罚原则(DP)：惩罚高度顶点之间的邻近性。惩罚之间的邻近性。 高度顶点。然后，将该原理与两种NRL算法(即谱嵌入[5]和DeepWalk[6])结合起来，学习无标度性质的保持顶点表示。

**网络层次表示学习(HARP)：**为了捕捉网络中的全局模式，HARP[42]对小型网络进行采样，以逼近全局结构。从 以采样网络作为初始化，推导出原始网络的顶点表示。从抽样网络中学习到的顶点表示作为初始网络的初始化，从而推断出原始网络的顶点表示。这样，在最终表示中保留了全局结构。 为了获得平滑的解，一系列较小的网络被连续地从原始网络中通过合并边和顶点来采样，顶点表示从最小网络到原始网络的层次结构推断。在HARP中，使用DeepWalk[6]和LINE[1]来学习顶点表示。

**摘要：**DP[41]和HARP[42]都是通过调整现有的NRL算法来捕捉宏观结构来实现的。 前者试图保持无标度属性，后者则使学习的顶点表示尊重全局网络结构。

### 4.2 无监督内容增强网络表示学习

除了网络结构外，现实世界的网络常常以丰富的内容作为顶点属性附加在一起，例如网页网络中的网页、引文网络中的论文和社交网络中的用户元数据。顶点属性为度量顶点之间的内容级相似性提供了直接证据。因此，如果将顶点属性信息适当地融入学习过程中，网络表示学习就可以得到显著的改善。近年来，一些内容增强的NRL算法被提出，将网络结构和顶点属性结合起来，以加强网络表示学习。

#### 4.2.1 文本关联DeepWalk(TADW)

TADW[7]首先证明了DeepWalk之间的等价性 [6]和下列矩阵因式分解：

$$\min\limits_{W,H}||M-W^TH||^2_F+\frac{λ}{2}(||W||^2_F + ||H||^2_ F)$$ (20)

其中W和H是隐嵌入，M是带转移p的顶点上下文矩阵。 k步内每个顶点对之间的可选性。然后，通过归纳矩阵因式分解[54]$$\min\limits_{W,H}||M-W^THT||^2_F+\frac{λ}{2}(||W||^2_F + ||H||^2_ F)$$     (21) 

其中T是顶点文本特征矩阵。在求解(21)后，通过W和HT的级联形成最终的顶点表示。

#### 4.2.2 同质性、结构和内容增强网络表示 

尽管TADW [7]能够结合纹理特征，但它只考虑网络顶点的结构上下文，即二阶和高阶邻近性，但忽略了学习框架中重要的同质性(一阶邻近性)。 HSCA[20]被提出同时集成同质性、结构化上下文和顶点内容，以学习有效的网络表示。

对于TADW，第i个顶点$ v_{i} $的学习表示为$[W^T_{:i},(HT_{:i})^T]^T$其中$W_{:i}$和$T_{:i}$分别是W和T的第i列。为了执行一阶邻近，HSCA引入了一个正则化项来强制嵌入空间中直接连接的节点之间的同质性，它被表述为

$$ \mathcal R (W,H) = \frac{1}{4}{S_{ij}||[\begin{matrix}W_{:i} \\HT_{:i} \end{matrix}]-[\begin{matrix}W_{:j} \\HT_{:j} \end{matrix}]||} $$                   (公式22) 

其中S是相邻矩阵。HSCA的目标是

$$ \min\limits_{W,H}||M-W^THT||^2_F+\frac{λ}{2}(||W||^2_F + ||H||^2_ F) +\mu\mathcal R (W,H)$$ $$公式23$$

其中λ和$\mu $是权衡的参数。求解上述优化问题后，将$W$和$HT$的级联作为最终的顶点表示。

#### 4.2.3 双约束Boltzmann机器(PRBM)

[29]利用限制玻尔兹曼机(RBM)[57]的优势，设计了一种新的模型，称为配对RBM (pRBM)，通过结合顶点属性和链接信息来学习顶点表示。pRBM考虑具有与二进制属性相关的顶点的网络。对于每个边$(v_i，v_j )∈E$，$v_i和v_j$的属性为$v^{(i)}和v^{(j)}∈ \{ {0，1} \}^m$，它们的隐式表示为$h^{(i)}和h^{(j)}∈{0，1}^d$。通过最大化定义在$v^{(i)},v^{(j)},h^{(i)}和h^{(j)}$上的pRBM的联合概率来学习顶点隐藏表示：

$$Pr(V^{(i)},V^{(j)},h^{(i)},h^{(j)}, w_{ij} ; θ)$$   $$公式24$$

$$ = exp(-E(V^{(i)},V^{(j)},h^{(i)},h^{(j)}) )/ Z$$

其中$θ={W∈R^{d×m}，b∈R^{d×1}，c∈R^{m×1}，M∈R^{d×d}}$为参数集，Z为归一化项。为了模拟联合概率，能量函数被定义为

$$E(V^{(i)},V^{(j)},h^{(i)},h^{(j)}, w_{ij}) =                                   $$

$$− w_{ij} (h^{(i)})^TMh^{(j)} − (h^{(i)})TWv^{(i)} − c^Tv^{(i)}  − b^Th^{(i)} − (h^{(j)} TW^{(i)}  − c^Tv^{(i)}  − b^Th^{(j)} ,$$ $$公式25$$ 

其中，$w_{ij} (h^{(i)})^TMh^{(j)}$强制$v_i和v_j$的潜在表示接近，而$W_{ij}$是边$(v_i，v_j)$的权重。

#### 4.2.4 保持用户配置文件的社会网络嵌入(UPP-SNE)

UPP-SNE[43]利用用户配置文件特性来增强用户在社交网络中的嵌入学习。与纹理内容特征相比，用户配置文件具有两个独特的属性：(1)用户配置文件具有噪声性、稀疏性和不完全性；(2)不同维度的用户配置特征是主题不一致的。过滤噪声并从用户配置文件中提取有用信息，UPP-SNE通过对用户配置文件特征执行非线性映射来构造用户表示，该映射以网络结构为指导。

在UPPSNE中使用近似内核映射[63]从用户配置文件特征构造用户嵌入：

$$ f(v_i)= \varphi(x_i)=\frac{1} {\sqrt{d}}[cos(µ^T_1 x_i),···，cos(µ^T_d x_i),sin(µ^T_1 x_i),···sin(µ^T_d x_i),]$$    $$公式26$$

其中$X_i$是顶点$v_i$的用户轮廓特征向量，$\mu_i$是对应的系数向量。

为了监督非线性映射的学习，使用户配置文件和网络结构相辅相成，使用了DeepWalk[6]的目标。

$$ \min\limits_{f} -logPr(\{v_{i-t},...，v_{i+t} \} \backslash v_i|f(v_i)  )$$    $$公式27$$

其中$\{v_{i-t},...，v_{i+t} \}\backslash v_i$是给定随机游动序列中t窗口大小内顶点$v_i$的上下文顶点。

#### 4.2.5 属性保护网络嵌入(PPNE)

为了学习内容增强顶点表示，PPNE[44]联合优化了两个目标：(1)结构驱动目标和(Ii)属性驱动目标。

DeepWalk之后，结构驱动目标的目标是使顶点共享相似的上下文顶点紧密表示。对于给定的随机游动序列S，结构驱动目标为

$$\min D_T=\prod\limits_{v∈S} \prod\limits_{u∈context(v)}Pr(u|v).$$

$$公式28$$

属性驱动目标的目标是使顶点表示通过公式来学习(28)。尊重顶点属性的相似性。属性驱动目标的一个实现是

$$\min D_N=\sum\limits_{v∈S} \sum\limits_{u∈pos(v)∪neg(v)}P(v, u)d(v, u)$$

$$公式29$$

其中$P(u，v)$是u和v之间的属性相似度，$d(u，v)$是嵌入空间中u和v之间的距离，$pos(v)和neg(v)$分别是P(u，v)中的top-k相似点和不同顶点的集合。

**总结**：以上的无监督内容增强NRL算法以三种方式结合顶点内容特性。首先，TADW[7]和HSCA[20]使用的是通过归纳矩阵分解[54]将网络结构与顶点内容特征相结合。该过程可以看作是受网络结构约束的顶点属性的线性变换。第二种方法是执行非线性映射，构造尊重网络结构的新顶点嵌入。例如，pRBM[29]和UPP-SNE[43]分别使用RBM[57]和近似核映射[63]来实现这一目标。PPNE[44]使用的第三种方法是在结构保持优化目标中加入一个属性保留约束。

## 5 半监督网络表示学习

与顶点相连的标签信息直接表示顶点的组或类从属关系。这类标签与网络结构和顶点属性之间有很强的相关性，虽然并不总是一致的，但在学习信息性和区分性网络表示时总是有帮助的。半监督NRL算法是沿着这条线发展起来的，它利用网络中可用的顶点标签来寻求更有效的顶点表示。

### 5.1 半监督结构保持NRL

第一组半监督NRL算法旨在同时优化保持网络结构和识别学习的表示学习。因此，来自顶点标签的信息可以帮助提高学习顶点表示的代表性和鉴别能力。

#### 5.1.1 区分深度随机行走(DDRW)

DDRW[45]受到判别表示学习的启发[64]、[65]，提出通过联合优化DeepWalk[6]的目标以及以下L2-loss支持向量分类目标来学习判别网络表示：

$$ \mathcal{L}_c=C\sum\limits_{i=1}^{|V|}(σ(1 − Y_{ik}β^Tr_{v_i}))^2+\frac{1}{2}β^Tβ $$

$$公式30$$

其中$σ(X)=x$，如果x>0，否则$σ(X)=0$。

因此，DDRW的共同目标被定义为

$$公式31$$

$$   \mathcal{L}= η\mathcal{L}_{DW}+\mathcal{L}_c$$

其中$\mathcal{L}_{DW}$是Deekwalk的目标函数。目标(31)的目的是学习第k类的二元分类的鉴别顶点表示。DDRW被推广到使用单对REST策略来处理多类分类[66].

#### 5.1.2 最高边缘深度行走(MMDW)

相似地，MMDW[46]将矩阵分解版本DeepWalk[7]的目标与具有${(R_{v_1}，Y_{1：})，··，(R_{v_t}，Y_{T:}})$训练集的多类支持向量机目标相结合：

$$\min\limits_{W,ξ}\mathcal{L}_{SVM}=\min\limits_{W,ξ}\frac{1}{2}||W||^2_2+C\sum\limits_{i=1}^Tξ_i$$

$$s.t. w^T_{l_i}r_{v_i} − w^T_jr_{v_i} ≥ e^j_i − ξ_i, ∀i, j$$

$$公式32$$

其中$l_i=k，y_k=1，e^j_i=1，y_{ij}=−1，e^j_i=0，y_j=1。$$$公式33$$

MMDW的共同目标是$$公式34$$

其中，LDW是DeepWalk矩阵分解版本的目标。

#### 5.1.3 转换式LINE(Tline)

类似地，TLINE[47]是[1]行的半监督扩展，它同时学习直线s顶点表示和SVM分类器。给定一组标记和未标记顶点{ v1、v2,···,六世}和{六世+ 1,···,v | | },TLINE列车多支持向量机分类器在{ v1、v2,···,六世}通过优化目标:

$$公式34$$

根据保持一阶和二阶邻近性的线公式，tline优化了两个目标函数：$$公式35$$

$$公式36$$

由于继承了线路处理大规模网络的能力，tline被认为能够以较低的时间和内存代价学习大规模网络的区分顶点表示。

#### 5.1.4 群增强网络嵌入(GENE)

基因[48]以概率的方式将组(标签)信息与网络结构相结合。GENE假设顶点应该紧密嵌入在低维空间中，如果它们共享相似的邻居或加入相似的组。受DeepWalk[6]和文档建模[67],[68]启发,学习小组的基因标签的机制通知顶点表示是通过最大化对数概率如下：$$公式37$$

其中Y是不同群的集合，WGI是用gi标记的随机游动序列集，Wˆgi是从群gi随机抽取的顶点集。

#### 5.1.5 半监督网络嵌入

SemiNE[49]在两个阶段学习半监督顶点表示。在第一阶段，SemiNE利用DeepWalk[6]框架以无监督的方式学习顶点表示。指出DeepWalk没有考虑上下文顶点的顺序信息，即，上下文顶点与中心顶点之间的距离，使用上下文顶点vi+j来预测中心顶点vi。因此,SemiNE编码订单信息到DeepWalk概率建模公关(vi + j | vi)jdependent参数:

$$公式38$$

其中，Φ(·)是顶点表示，Ψj(·)是计算pr(vi j=vi)的参数。

在第二阶段，半NE学习一个神经网络，它调整学习的无监督顶点表示，以适应顶点标签。

### 5.2 半监督内容增强nrl

最近，更多的研究转移到开发标签和内容增强nrl算法，这些算法调查了顶点内容和 标签，以协助网络表示学习。随着内容信息的整合，学习到的顶点表示将更具有信息性，并考虑到标签信息。 红色，学习的顶点表示可以为底层分类任务高度定制。

#### 5.2.1 使用耦合神经网络框架的三方深层网络表示(TriDNR)

 TriDNR[50]从三个信息源学习顶点表示：网络结构、顶点内容和顶点标签。为了捕获顶点内容和标签信息，TriDNR采用Pa。 RAGET向量模型[67]通过最大限度地实现以下目标来描述顶点词相关性和标签词对应关系：

其中{w−b：WB}是长度为2b的上下文窗口中的一个词序列，Ci是顶点vi的类标记，L是标记顶点的一组索引。然后由Coupl实现TriDNR。 将段落向量目标与深度行走目标联系起来：

其中，LDW为深度行走最大化目标函数，α为折衷参数.

#### 5.2.2 链接文档嵌入(LDE)

LDE[51]是为了学习链接文档的表示，链接文档实际上是引文或网页网络的顶点。类似TriDNR[50]，LDE 通过对三种关系的建模来学习顶点表示，即字-文档关系、文档-文档关系和文档-标签关系。lde是通过解决以下问题来实现的。 

本文利用概率PR(WJ_x_wi，DK)对Word-Document关系进行了建模，这意味着在文档DK中，Word WJ是wi的一个相邻词。捕捉文字文档 T关系，提取三重子(wi，wj，dk)，在文档dk中产生字邻对(wi，wj)。三元组(wi，wj，dk)由P表示。 由链接文档对(di，dj)、pr(dj\di)之间的条件概率捕获。本文还考虑了文献标号关系的概率，并通过对pr(yi\di)的建模来考虑文档标签关系。 对以第一文件为条件的“易纲”类标签的不满。在(41)中，W、D和Y分别是文字、文档和标签的嵌入矩阵。

#### 5.2.3 区分矩阵因式分解(DMF)

为了增强具有鉴别能力的顶点表示能力，DMF[8]执行TADW(21)的目标，并对训练在标记顶点上的线性分类器进行经验损失最小化：

 l，j=1(mij−w ti htj)2 2xn∈L(yn1−ηtxn)2λ1 2(khk 2 f kηk2)λ2 2 kwk 2 F，(42)

其中wi是顶点表示矩阵W的第一列，TJ是顶点文本特征矩阵T的第j列，L是标记顶点的指标集。

DMF从W构造顶点表示，而不是W和HT。这是基于经验发现，W包含足够的信息来表示顶点。在(42)，xn的目标中 设置为[WT n，1]T，其中将线性分类器的截距项b合并到η中。通过交替优化W、H和η来求解优化问题(42)。一旦优化问题 解决了LEM问题，学习了识别性和信息性顶点表示和线性分类器，并共同对网络中的未标记点进行分类。

#### 5.2.4 具有嵌入的预测标签和邻居-从数据中转换或归纳(Planetoid)

Pletoid[52]利用网络嵌入和顶点属性进行半监督学习。平面图通过最小化预测结构c的损失来学习顶点嵌入。 ontext，它被表述为



其中(i，c)是顶点上下文对(vi，vc)的索引，Ei是顶点vi的嵌入，wc是上下文顶点vc的参数向量，γ∈{1，−1}表示所采样的顶点是否存在。 上下文对(i，c)为正或负。根据网络结构和顶点标签对三元组(I，c，γ)进行采样。

平面然后通过深层神经网络将学习到的顶点表示e和顶点属性x映射到隐层空间，并将这两个隐层表示连接在一起进行预处理。 通过最大限度地减少以下分类损失，删除顶点标签：



为了将网络结构、顶点属性和顶点标签集成在一起，Planetoid联合最小化了这两个目标(43)和(44)，用深神经网络学习顶点嵌入e。

#### 5.2.5 标签通知属性网络嵌入(LAIN)

Lane[30]通过将网络结构邻近性、属性亲和力和标签邻近性嵌入到统一的潜在表示中来学习顶点表示。学习的表示法是exp的。 获取网络结构和顶点属性信息，如果提供标签信息的话。其中嵌入学习分两个阶段进行。在第一阶段，顶点 将网络结构中的邻近性和属性信息映射为潜在的表示U(G)和U(A)，然后通过最大化它们之间的相关性将U(A)合并到U(G)中。在第二站 GE，Lane利用联合邻近(由U(G)确定)平滑标签信息，并将它们一致嵌入到另一个潜在表示U(Y)中，然后将U(A)、U(G)和U(Y)嵌入到a中。 统一嵌入表示H.

### 5.3 摘要，概要

我们现在总结和比较了表5中半监督的NRL算法所使用的区分学习策略的优缺点。三种策略用于实现歧视性学习。 第一种策略（即DDRW [45]，MMDW [46]，TLINE [47]，DMF [8]，SemiNE [49]）是对顶点表示强制执行分类损失最小化，即将顶点表示拟合到分类器。 这提供了在新嵌入空间中将不同类别的顶点彼此分开的直接方式。 第二策略（由GENE [48]，TriDNR [50]，LDE [51]和Planetoid [52]使用）是通过建模顶点标签关系来实现的，这样具有相同标签的顶点具有相似的矢量表示。 LANE [30]使用的第三种策略是将顶点和标签共同嵌入公共空间。

对分类器进行顶点表示的拟合可以利用顶点标签中的鉴别能力。使用这种策略的算法只需要少量的标记顶点(例如，1 (0%)比无监督的同行获得显著的性能提升。因此，在稀疏标记的情景下，它们对区分性学习更为有效。然而，拟合顶点表示 对分类器的诱捕更容易发生过度拟合。经常采用正规化和辍学[69]来解决这一问题。通过对比，建立了顶点标签关系和联合顶点EMBE模型。 布丁要求有更多的顶点标签来使顶点表示更具有判断力，但它们可以更好地捕捉类内邻近，也就是说，属于同一类的顶点更接近e。 在新的嵌入空间中。这使他们在诸如顶点聚类或可视化等任务上具有普遍的优势。

## 6 应用

一旦通过网络表示学习技术学习了新的顶点表示，就可以使用传统的基于向量的算法来解决重要的分析任务，例如顶点分类。 链接预测、聚类、可视化和推荐。通过评估它们在这些任务上的表现，也可以验证所学习的表示的有效性。

### 6.1 顶点分类

顶点分类是网络分析研究的重要内容之一。在网络中，顶点通常与描述实体某些方面的语义标签相关联，例如 作为信仰、兴趣或从属关系。在引文网络中，出版物可能带有主题或研究领域的标签，而社交网络中实体的标签则可能显示个人的利益或政治信仰。通常，由于标签成本高，网络顶点被部分或稀疏地标记，所以网络中的大部分顶点都有未知的标签。顶点分类问题的目的是预测一个部分标记网络[10]，[11]的未标记顶点的标号。由于顶点不是独立的，而是通过链接以网络的形式相互连接，顶点分类应该利用这些依赖关系来联合分类顶点的标签。其中，集体分类建议构建一组新的顶点特性，用于总结邻域内的标签依赖关系，这已被证明在对许多现实网络进行分类时最为有效[70]，[71]。

网络表示学习的原理与基于网络结构的顶点特征自动学习相同。现有的研究评估了学习顶点表示在两种情况下的鉴别能力:无监督设置(如[1]、[6]、[7]、[20]、[34])，分别学习顶点表示，然后在新的嵌入中应用SVM或logistic回归等判别分类器，以及半监督设置(如[8]、[30]、[45]、[46]、[47])，同时处理表示学习和判别学习，因此，从标记顶点推断出的判别能力可以直接促进信息顶点表示的学习。这些研究证明，更好的顶点表示有助于提高分类准确率。

### 6.2 链路预测

网络表示学习的另一个重要应用是链接预测[13]，[72]，它的目的是根据当前观察到的链接及其属性，推断出实体对之间是否存在新的关系或正在出现的相互作用。为解决这一问题而开发的方法可以发现网络中隐含的或缺失的交互，识别虚假的链接，以及理解网络演化机制。链接预测技术在社交网络中被广泛应用于预测人与人之间的未知联系， 用来推荐友谊或确定可疑的关系。目前的大多数社交网络系统都在使用链接预测来自动地给朋友提供高精度的建议。在生物网络中，链路预测方法被用来预测以前未知的蛋白质之间的相互作用，从而大大降低了经验方法的成本。读者可以参考[12][73]的调查论文了解这一领域的最新进展。

良好的网络表示应该能够捕获网络顶点之间的显式和隐式连接，从而使应用程序能够进行链接预测。[19]和[35]基于社交网络上学习到的顶点表示来预测缺失的链接。[34]还将网络表示学习应用于协作网络16和蛋白质-蛋白质相互作用网络。他们证明，在这些网络上，使用学术代表预测的链接。 与传统的基于相似的链路预测方法相比，抱怨具有更好的性能。

### 6.3 聚类

网络聚类是指将网络顶点划分成一组簇的任务，使得顶点在同一簇内密集地连接在一起，但与其他星系团中的少数顶点相连[74]。这类群集结构或社区广泛存在于生物信息学、计算机科学、物理学等广泛的网络系统中。 地质学等，并有很强的含意。例如，在生物学网络中，簇可能对应于一组具有相同功能的蛋白质；在网页网络中，群集可能是具有相似主题或相关连续体的页面。 在社交网络中，集群可能表示有相似兴趣或关联的人群。

研究人员提出了大量基于顶点间相似度或连接强度的网络聚类算法。最小-最大切割和归一化切割方法[75]， [76]寻求递归地将一个图划分成两个簇，最大限度地增加簇内连接的数量和最小化簇间连接的数量。基于模块化的方法(例如， [77]，[78])的目标是最大限度地提高聚类的模块化程度，即假设聚类的边缘是随机分布的，簇内边缘的分数减去期望的分数。网络分区 具有高模块性的Ng具有密集的簇内连接，而稀疏的簇间连接。其他一些方法(例如，[79])试图识别具有类似于桥的结构角色的节点 S和离群值。

最近的NRL方法(如GraRep[26]、DNGR[9]、MNMF[28]、pRBM[29])使用聚类性能来评估不同网络上学习网络表示的质量。直观地说，更好的表示将导致更好的集群性能。这些工作遵循了一种常见的方法，即首先应用无监督NRL算法来学习顶点表示，然后在学习的表示上执行k-means聚类来聚类顶点。特别地，pRBM[29]表明NRL方法优于使用原始特征进行聚类而不需要学习表征的基线。这说明有效的表示学习能够提高聚类性能。

### 6.4 可视化

可视化技术在管理、探索和分析复杂的网络数据方面发挥着关键作用。[80]从信息可视化的角度研究了一系列用于可视化图形的方法。本文比较了传统的可视化图形的布局方法，如树图、三维图和基于双曲线的方法，证明了经典的可视化技术对于中小型网络是有效的;然而，当应用于大型网络时，它们面临着巨大的挑战。很少有系统能够有效地处理数千个顶点，尽管这种数量级的网络经常出现在各种各样的应用中。因此，可视化过程中的第一步通常是减少显示网络的大小。一种常见的方法本质上是找到一个维持固有结构的网络的极低维表示。在低维空间[17]中，保持相似顶点接近，不同顶点远离。

网络表示学习具有同样的目标，即将一个大的网络嵌入到一个新的低维度的潜在空间中。在矢量空间中获得新的嵌入后，可以使用诸如t-分布随机邻居嵌入(t-SNE)[81]等流行的方法将网络可视化到二维或三维空间中。通过将学习到的顶点表示作为输入，LINE[1]在将作者映射到二维空间后，使用tSNE包将DBLP合著者网络可视化，并表明LINE能够将同一领域的作者聚集到同一个社区。HSCA[20]通过可视化引用网络说明了内容增强NRL算法的优点。半监督算法(如TLINE[47]、TriDNR[50]、DMF[8])表明，可视化结果具有更好的聚类结构，顶点标签被正确导入。

### 6.5 推荐

除了结构、内容和顶点标签信息,许多社交网络还包括地理和时空信息,用户可以在网上分享他们的经验和他们的朋友的兴趣点(POI)建议,例如,交通、餐饮、观光地标,等等。这些基于位置的社交网络(LBSN)的例子包括Foursquare, Yelp, Facebook,和许多其他人。对于这些类型的社交网络，POI推荐的目的是根据用户的上下文(比如用户的地理位置和兴趣)推荐用户感兴趣的对象。传统上，这是通过使用协同过滤等方法来解决的，利用用户活动和地理距离之间的时空相关性[82]。但是，由于每个用户的签入记录都非常稀疏，因此查找相似的用户或计算用户和位置之间的转换概率是一个重大挑战。

最近出现了时空嵌入[14]、[15]、[83]，学习低维密集向量来表示用户、位置、兴趣点等。因此，每个用户、位置、POI可以分别表示为一个低维向量，用于相似度搜索和许多其他分析。这种时空感知嵌入的一个固有优势是，它减轻了数据稀疏问题，因为学习到的低维向量通常比原始表示要密集得多。因此，它使得查询任务(如top-k POI搜索)比传统方法更加精确。

### 6.6 知识图谱

知识图谱是数据库系统中一种新型的数据结构，它对数十亿个实体的结构化信息及其丰富的关系进行编码。知识图通常包含大量异构对象和不同类型的实体关系。这种网络化的实体形成了一个巨大的图形，现在正为许多商业搜索引擎提供动力，以便在网上找到类似的目标。传统上，知识图搜索是通过数据库驱动的方法来探索实体之间的模式映射，包括实体关系。最近网络表示学习的进步激发了知识库的结构化嵌入[84]。这种嵌入方法学习了知识图实体的低维向量表示，通过比较查询对象和数据库对象的向量表示，可以执行通用数据库查询，如top-k搜索。

除了使用向量表示来表示知识图实体外，研究人员还提出使用这种表示来进一步增强和完善知识图本身。 例如，知识图完成旨在发现实体之间的完全关系，最近的一项工作[85]建议使用图上下文来查找实体之间缺失的链接。T型 HIS类似于社交网络中的链接预测，但这些实体通常是异构的，一对实体也可能有不同类型的关系。

## 7 评估协议

在这一部分中，我们讨论了验证网络表示学习有效性的评估协议。这包括常用基准数据集和评估方法的总结。 然后对算法的性能和复杂度进行了比较。

### 7.1 基准数据集

基准数据集在研究社区评估新开发的NRL算法与现有基准方法相比的性能方面起着重要作用。一些网络数据集已经公开，以促进NRL算法在不同任务的评估。我们在表6中总结了大多数已发表的网络表示学习论文使用的网络数据集列表。

表6总结了公开的基准数据集的主要特点,包括网络的类型(直接的或间接的、二进制或加权),数量的顶点 |V|,边数|E|,数量的标签 $|\mathcal Y |$,不管是否网络多标记,以及网络的顶点是否附加属性。在表6中，根据信息网络的属性，我们将基准数据集分为8种不同的类型

**社交网络:**BlogCatalog、Flickr和YouTube数据集是由相应的在线社交网络平台的用户组成的。对于这三个数据集，顶点标签由用户兴趣组定义，但是用户属性不可用。Facebook网络是10个Facebook自我网络的组合，每个顶点都包含用户配置文件属性。Amherst、Hamilton、Mich和Rochester[86]数据集是由来自美国大学的用户组成的Facebook网络，每个用户都有6个用户资料特征。通常，用户配置文件特性是嘈杂的、不完整的和长尾分布的。

**语言网络：**语言网络维基百科[1]是一个词共现网络构建的整个英语维基百科页面。这个网络上没有类标签。通过词类类比和文献分类的方法，对从该网络中学习到的嵌入词进行了评价。

**引文网络：**引文网络是由作者-作者引文关系或纸质引文关系构成的定向信息网络。它们来自不同的学术论文数据库，如DBLP和Citeseer。中常用的引文网络,DBLP(AuthorCitation)[1]是一个加权引文网络作者之间定义的边缘重量由一个作家写的论文数量和引用的其他作者,而DBLP(PaperCitation)[1],科拉,Citeseer,PubMed和Citeseer-M10二进制论文引文网络,也附有顶点文本属性文件的内容。与社交网络中的用户配置文件特性相比，这里的顶点文本特性更以主题为中心，信息更丰富，可以更好地补充网络结构以学习有效的顶点表示。

**协作网络：**协作网络Arxiv GR-QC[88]描述了广义相对论和量子宇宙学研究领域论文的合著关系。在这个网络中，顶点表示作者，而边表示作者之间的合著者关系。由于顶点没有类别信息，因此该网络用于链路预测任务，以评估学习顶点表示的质量。

**网页网络：**网页网络(Wikipedia, WebKB和Political Blog[89])是由现实世界中的网页和超链接组成的，其中顶点代表一个网页，而边缘表示从一个网页到另一个网页有超链接。网页文本内容通常作为顶点特性收集。

**生物网络：**蛋白-蛋白相互作用网络作为一种典型的生物网络[90]是智人PPI网络的子图。这里的顶点代表一个蛋白质，边缘表示蛋白质之间存在相互作用。顶点的标签来自于hallmark基因集合[91]，表示生物状态。

**通信网络：**安然电子邮件网络是由安然员工之间的电子邮件通信形成的，顶点是员工，边代表员工之间的电子邮件通信。根据员工的职能，将其划分为7个角色(如CEO、总裁和经理)。

**交通网络：**[39]中使用的欧洲航空公司网络由6家在欧洲机场之间运营的航空公司组成:4家商业航空公司(法航、易捷航空、汉莎航空和瑞安航空)和2家货运航空公司(葡萄牙航空公司和欧洲航空运输公司)。对于每个航线网络，顶点表示机场，边表示机场之间的直达航班。总共有45个机场根据其结构角色被标记为枢纽机场、区域枢纽机场、商业枢纽机场和重点城市。

> ![1542100976916](F:\Machine-learning-and-data-science-notebook\images\网络表示学习\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542100976916.png)
>
> 表6用于评价网络表示学习的基准数据集摘要。
>
>

> ![1542100991888](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1542100991888.png)

### 7.2 评价方法

由于标准答案的不可获得性，很难直接比较不同NRL算法得到的顶点表示的质量。另外，为了评估NRL算法在学习顶点表示上的有效性，常用几种网络分析任务进行比较研究。

**网络重建：**网络重构的目的是根据顶点表示之间的内积或相似度来预测顶点之间的连接，从而从学习到的顶点表示中重建原始网络。原始网络中已知的链路作为评价重构性能的依据。precision@k和MAP[19]常常被用作评估指标。这种评价方法可以检验学习顶点表示是否能很好地保持网络结构和支持网络的形成。

**顶点分类：**作为NRL的一种评价方法，顶点分类是将学习到的顶点表示作为特征来训练标记顶点上的分类器。利用无标记顶点的分类性能来评价学习顶点表示的质量。不同的顶点分类设置，包括二类分类、多类分类和多标签分类，通常根据底层网络特性进行。对于二类分类，以F1分数作为评价标准。对于多分类和多标签分类，采用M icro-F1和M acro-F1作为评价标准。

**顶点聚类：**为了验证NRL算法的有效性，将k-means聚类算法应用到学习的顶点表示中，进行顶点聚类。将网络中的社区作为地面真实来评估聚类结果的质量，其质量由精确度和NMI(规范化互信息)来衡量[92]。假设，如果学习的顶点表示确实提供了信息，那么在学习的顶点表示上的顶点集群应该能够发现社区结构。也就是说，好的顶点表示可以产生好的聚类结果。

**链接预测：**链接预测可以用来评估学习到的顶点表示是否提供信息以支持网络演化机制。为了在网络上执行链接预测，首先删除一部分边，然后从剩余的网络中学习顶点表示。最后，用学习过的顶点表示来预测删除的边。链路预测的性能通过AUC和precision@k进行测量。

**可视化：**可视化提供了一种直观地评估学习顶点表示的质量的方法。通常，t分布随机邻域嵌入(t-SNE)[81]被用于将学习到的顶点表示向量投影到二维空间中，在二维空间中，顶点2-D映射的分布可以很容易地可视化。如果顶点表示质量较好，在二维空间中，同一类或共同体内的顶点应紧密嵌入，不同类或共同体内顶点的二维映射应相距很远。

在表7中，我们总结了用于评估现有NRL算法获得的顶点表示质量的信息网络的类型和网络分析任务。我们还提供了相关NRL算法代码的超链接，以帮助感兴趣的读者进一步研究这些算法或进行实验比较。 总的来说，社交网络和引文网络经常被用作基准数据集，顶点分类是在非监督和半监督设置中最常用的评估方法。

### 7.3 经验结果

我们从文献中观察到，经验评价往往是在不同的设置下对不同的数据集进行的。实证结果缺乏一致性，无法确定最佳执行算法及其环境。因此，我们进行了基准实验，比较了几种代表性NRL算法在同一数据集上的性能。注意，由于半监督NRL算法依赖于任务:目标任务可能是二进制或多类，或多标签分类，或由于它们使用不同的分类策略，因此很难评估在相同设置下网络嵌入的有效性。因此，我们的实证研究重点比较了文献中最常用的两种非监督NRL算法(DeepWalk[6]、LINE[1]、node2vec[34]、MNMF[28]、TADW[7]、HSCA[20]、UPP-SNE[43])顶点分类和顶点聚类。

我们的实证研究基于七个基准数据集:Amherst, Hamilton, Mich, Rochester, Citeseer,Cora和Facebook。在[28]之后，对于Amherst,Hamilton,Mich和Rochester，只使用网络结构，用属性“year”作为类标签，这是一个很好的社区结构指标。对于Citeseer和Cora，研究区域作为class label。Facebook数据集的类标签由“教育类型”属性给出。

#### 7.3.1 实验计划

对于基于随机漫步的方法DeepWalk, node2vec和UPP-SNE，我们分别将步数、步幅和窗口大小分别设置为10、80、10。对于UPP-SNE，我们使用了由随机梯度下降法优化的实现。node2vec的参数p和q设置为1，作为默认设置。对于M-NMF,我们设置α和β1。对于所有算法，学习顶点表示的维数都设置为256。对于直线，我们学习了128维的顶点表示，分别使用第一级接近保留版本和第二级接近保留版本，并将它们连接在一起，以获得256维的顶点表示。上述算法的其他参数都设置为默认值。

以学习顶点表示为输入，进行顶点分类和顶点聚类实验，评价学习顶点表示的质量。对于顶点分类，我们随机选择5%和50%的样本来训练一个SVM分类器(带有LIBLINEAR实现[66])，并在剩下的样本上进行测试。我们重复这个过程10次，并报告平均M icro-F1和M acro-F1值。采用K-means进行顶点聚类。为了减少随机初始化引起的方差，我们将聚类过程重复20次，并报告平均精度和NMI值。

#### 7.3.2 业绩比较

表8和表9比较了不同算法在顶点分类和顶点聚类方面的性能。对于每个数据集，在所有基线中表现最好的方法都是粗体的.为 属性网络(Citeseer、Cora和Facebook)、下划线结果表明，在仅保留NRL算法(DeepWalk、line、node2vec和M-NMF)的结构中表现最好。

由表8可知，在仅保留NRL算法的结构中，当训练比为5%时，node2vec的分类性能整体最佳，当训练比为50%时，M- nmf在M icro-F1方面表现最佳，而DeepWalk则是M acroF1的优胜者。在这里，M-NMF并不比DeepWalk、LINE和node2vec显示出显著的优势。这可能是由于参数α和βN-NMF不是经过调优;必须仔细选择它们的值，以便在不同组件之间实现良好的权衡。在有属性的网络(Citeseer, Cora和Facebook)上，内容增强NRL的表现要比只保留NRL算法的结构好得多。这证明了顶点属性在很大程度上有助于学习更有用的顶点表示。当训练比率为5%时，UPP-SNE表现最好。这表明UPP-SNE非线性映射比线性映射提供了更好的方式从顶点属性构造顶点表示，如TADW和HSCA中所做的那样。当训练率为50%时，TADW总体分类性能最好，但在某些情况下，HSCA略优于TADW。在引文网络(Citeseer和Cora)上，HSCA比TADW表现更好，而在Facebook上却比TADW表现更差。这可能是因为Facebook社交网络的同质性弱于引文网络。为了使HSCA在Facebook上达到令人满意的性能，应该减少对同源保持目标的权重。

由表9可知，LINE在Amherst、Hamilton、Mich和Rochester上的聚类性能最好。当线顶点表示同时捕获第一级和第二级邻近时，它可以更好地保持社区结构，从而获得良好的集群性能。在Citeseer、Cora和Facebook上，内容增强NRL算法UPP-SNE表现最好。由于UPP-SNE通过非线性映射从顶点属性构造顶点表示，保存良好的内容信息有利于最佳的聚类性能。在Citeseer和Cora上，node2vec比其他只保留NRL算法的结构表现要好得多，包括它的等效版本DeepWalk。对于每个顶点上下文对(vi, vj)， DeepWalk和node2vec使用两种不同的策略来近似概率Pr(vj |vi):分级softmax[93]、[94]和负采样[95]。node2vec在DeepWalk上的聚类性能较好，证明了负采样优于层次化softmax，这与文献[67]中报道的单词嵌入结果一致。

### 7.4 复杂性分析

为了更好地理解现有的NRL算法，我们在表10中详细分析了它们的时间复杂度和潜在的优化方法。引入新的符号I来表示迭代次数，我们使用nnz(·)来表示矩阵的非零项数。总而言之，现有NRL算法的目标优化采用了四种解决方案:(1)特征分解,发现top-d特征向量矩阵,(2)选择优化,优化一个变量与其余变量交替固定,(3)梯度下降,更新所有参数在每个迭代优化的总体目标,和(4)随机梯度下降优化部分客观随机在线模式。

无监督和半监督NRL算法的优化问题多采用随机梯度下降法。这些算法的时间复杂度通常与顶点/边的数量成线性关系，这使得它们可以扩展到大型网络。相比之下，其他的优化策略通常涉及到更高的时间复杂度，对于顶点的数量是二次的，或者更大，顶点的数量乘以边的数量。NRL算法通常在|V | |V |结构保留矩阵上进行分解，这是非常耗时的。为了减少矩阵分解的复杂性，已经作出了努力。例如，TADW[7]、DMF[8]和HSCA[20]利用了原始顶点上下文矩阵的稀疏性。希望[35]和GraphWave[39]采用先进技术[96][97]进行矩阵特征分解。

## 8 今后的研究方向

在这一节中，我们总结了六个潜在的研究方向和未来的挑战，以促进对网络表示学习的研究。

**任务依赖:**到目前为止，大多数现有的NRL算法都是独立于任务的，而特定于任务的NRL算法主要集中在半监督设置下的顶点分类。直到最近，一些研究才开始设计针对任务的NRL算法，用于链接预测[35]、社区检测[98]、[99]、[100]、[101]、类不平衡学习[102]、主动学习[103]和信息检索[104]。使用网络表示学习作为中间层来解决目标任务的优点是，在新表示中保留的尽可能好的信息可以进一步使后续任务受益。因此，理想的任务特定NRL算法必须保留对特定任务至关重要的信息，以优化其性能。

**理论:**虽然已有的NRL算法的有效性已经通过实验得到了实证证明，但其工作机理还不清楚。关于算法的性质以及什么有助于产生好的实证结果，目前还缺乏理论分析。为了更好地理解DeepWalk[6]、LINE[1]和node2vec[34]，[105]发现了它们与Laplacians图的理论联系。然而，对于网络表示学习的深入的理论分析是必要的，因为它提供了对算法的深入理解和帮助解释经验结果。

**动态：**目前关于网络表示学习的研究主要集中在静态网络上。然而，在现实生活中，网络并不总是静态的。潜在的网络结构可能会随着时间的推移而进化。，新顶点/边出现，旧顶点/边消失。顶点/边也可以用一些时变信息来描述。动态网络具有使静态网络嵌入失效的独特特性:(i)顶点内容特征可能随时间推移而漂移;增加新顶点/边需要学习或更新顶点表示才能有效;(三)网络规模不固定。动态网络嵌入的工作相当有限;现有的大多数方法(例如，[106]，[107]，[108])假设节点集是固定的，只处理删除/添加边所引起的动态。然而，一个更有挑战性的问题是预测新添加顶点的表示形式，这被称为样本外问题。一些尝试，如[52]，[109]，[110]利用归纳学习来解决这个问题。他们在快照上从网络中学习一个显式映射函数，并使用该函数根据可用信息(如属性或邻域结构)推断出样本外顶点的表示形式。但是，他们没有考虑如何增量地更新现有的映射函数。如何在复杂的动态领域中设计出高效的表示学习算法还需要进一步的探索。

**可扩展性**：可扩展性是推进网络表示学习研究的另一个驱动因素。一些NRL算法已经尝试在顶点/边的数量上扩展到具有线性时间复杂度的大型网络。然而，可伸缩性仍然是一个主要挑战。复杂度分析结果表明，采用随机梯度下降法的随机游走方法和基于边缘建模的方法比采用本征分解和替代优化方法的矩阵分解方法更有效。基于矩阵分解的方法在合并顶点属性和发现社区结构方面有很大的前景，但是它们的可扩展性需要改进，以处理具有数百万或数十亿顶点的网络。基于深度学习的方法可以捕获网络中的非线性，但其计算代价通常很高。传统的深度学习体系结构利用GPU加速欧氏结构化数据的训练[111]。然而，网络没有这种结构，因此需要新的解决方案来提高可伸缩性[112]。

**异构和语义：**异构信息网络的表示学习(HIN)是一个很有前途的研究方向。现有的大量工作集中在同质网上。 功嵌入，其中所有顶点都是相同类型的，边表示单个关系。然而，人们越来越需要研究具有不同类型的异构信息网络。 顶点和边，如DBLP、DBpedia和Flickr。HIN由不同类型的实体组成，如文本、图像或视频，实体之间的相互依赖关系非常复杂。 这使得很难度量丰富的语义和顶点之间的接近性，很难找到一个公共的、连贯的嵌入空间。[16]、[113]、[114]、[115]、[116]、[117]、[118]、 [119]、[120]、[121]研究了使用各种描述符(例如元路径或元结构)来捕获用于表示学习的远距离HIN顶点之间的语义邻近性。然而， 这方面的研究尚处于初级阶段。进一步的研究需要探索更好的方法来捕捉跨模式数据之间的邻近性，以及它们与网络架构之间的相互作用。 油温测量装置；介观结构.

另一个有趣的方向是研究有符号网络中的边语义，在有符号网络中，顶点具有正负关系。签名网络在社交网络中无处不在，例如Epinions和Slashdot，允许用户与其他用户形成积极或消极的友谊/信任连接。负链的存在使得传统的基于同构的网络表示学习算法无法直接应用。一些研究[122]，[123]，[124]通过直接建模链路的极点来处理有符号网络表示学习。如何充分编码有符号网络嵌入的网络结构和顶点属性仍然是一个悬而未决的问题。

**鲁棒性:**现实世界的网络经常是嘈杂和不确定的，这使得传统的NRL算法不能产生稳定和健壮的表示。ANE(对抗性网络嵌入)[125]和ARGA(对抗性正则图自动编码器)[126]通过实施对抗性学习规则化器[58]来学习鲁棒顶点表示。为了处理边存在的不确定性，URGE (uncertainty Graph embedded)[127]将边存在概率编码到顶点表示学习过程中。加强对网络表示学习鲁棒性的研究对于提高网络表示学习的鲁棒性具有重要意义。

## 9 结论

本文综述了数据挖掘和机器学习领域中最先进的网络表示学习算法。我们提出一种分类法，将现有技术总结为两种设置:无监督设置和半监督设置。根据他们使用的信息源和使用的方法，我们进一步将每个设置中的不同方法分类为子组，回顾每个子组中的代表性算法，比较它们的优缺点。总结了现有NRL算法验证的评价方案，比较了其实证性能和复杂性，并指出了一些新兴的研究方向和有希望的扩展。我们的分类和分析不仅帮助研究人员全面了解该领域现有的方法，而且为推进网络表示学习的研究提供了丰富的资源。

## 致谢

这项工作是由美国国家科学基金会(Nsf)通过赠款IIS-1763452和澳大利亚研究理事会(ARC)通过赠款LP 160100630和DP 180100966提供的。张道坤 由中国奖学金委员会(Csc)引进，第201506300082名，澳大利亚数据61大学研究生奖学金.

## 参考文献

