## 图中的注意力模型
##### 摘要：
&#160; &#160; &#160; &#160;图结构的数据很自然地出现在许多不同的应用范围里。通过将数据表示为图，我们可以捕捉实体（比如节点）与实体之间彼此的关系。许多有用的见解可以从图形结构数据中获得，正如不断增长的专注于图挖掘的工作所证明的那样。但是，在现实世界中，图形可能很大 - 具有许多复杂模式 - 并且噪声可能会对有效图形挖掘造成问题。处理这个问题的有效方法是将“注意力”纳入图挖掘解决方案。<br>
&#160; &#160; &#160; &#160;注意机制允许方法专注于图的任务相关部分，帮助它做出更好的决策。 在这项工作中，我们对图形注意力模型的新兴领域的文献进行了全面而有针对性的调查。我们引入了三种直观的分类法来对现有工作进行分组。 这些基于问题设置（输入和输出的类型），所使用的关注机制的类型和任务（例如，图分类，链接预测等）。我们通过详细的示例激励我们的分类法，并从独特的角度使用每个分类法来调查竞争方法。<br>
&#160; &#160; &#160; &#160;最后，我们强调该领域的一些挑战，并讨论未来工作的有希望的方向。
##### 关键字：
计算方法→人工智能; 机器学习; 逻辑和关系学习;计算数学→图算法;组合学; 图论;信息系统→数据挖掘;计算理论→图算法分析;流，次线性和近线性时间算法;
## 1 简介
&#160; &#160; &#160; &#160;那些可以被自然地建模为图的数据可以在各种各样的领域中被找到，包括全球网络[Albert et al。 1999]，生物信息学[Pei et al。2005]，神经科学[Lee et al。2017]，化学信息学[Duvenaud等。2015]，社交网络[Backstrom和Leskovec2011]，科学引文和合作[Liu et al。2017]，城市计算[郑等人。 2014]，推荐系统[Deng et al。2017]，传感器网络[Aggarwal et al。2017]，流行病学[Moslonka？Lefebvre et al。2011]，异常和欺诈分析[Akoglu et al。2015]和生态学[Allesina et al。2005]。<br>

&#160; &#160; &#160; &#160;例如，可以使用图表来捕获社交网络上的用户之间的交互，其中节点表示用户，并且链接表示用户交互或友谊 [Backstrom和Leskovec 2011]。 另一方面，在化学信息学中，我们可以通过将原子视为节点并将原子之间的键视为边来构建分子图[Duvenaud et al。2015年]。

&#160; &#160; &#160; &#160;相关领域已经出现大量相关的工作。我们大致分类为图形挖掘领域[Aggarwal和Wang 2010]，专注于从图形数据中获取洞察力。 在这方面已经研究了许多有趣和重要的问题。 这些包括图表分类[Duvenaudet al。 2015]，链接预测[Sun et al。 2011]，社区检测[Girvan and Newman 2002]，功能性脑网络发现[Bai et al。 2017]，节点分类和聚类[Perozzi等。 2014年]，影响最大化[He and Kempe 2014]不断出现新问题建议。<br>

![图1](images/2018/11/1.png)

&#160; &#160; &#160; &#160;如图所示。注意是用来给每种类型的邻居赋予重要性的。链接大小表示数量注意，我们想要适用于每个邻居。在这个例子中，我们通过关注a看到了这一点node的同学们，我们可以更好的预测目标感兴趣的活动类型。用颜色看最好。<br>
&#160; &#160; &#160; &#160;然而，在现实世界中，图形在结构上既大又复杂[Ahmed et al.2014; Leskovec and Faloutsos 2006]以及嘈杂[he and Kempe 2014]。 这些对图表挖掘技术构成了巨大的挑战，特别是在性能方面[Ahmed et al。2017]。<br>
&#160; &#160; &#160; &#160;许多技术被提出来来解决这个问题。例如，由[Wu et al. 2015]提出的利用同一个图的多种视图来改善分类性能的方法。而[Zhang et al. 2016a]在类似的动机下利用辅助非图数据作为侧视图。另一种流行的技术涉及识别任务相关或有区别的子图[Shi et al. 2012; Zhu et al. 2012].<br>

&#160; &#160; &#160; &#160;最近，出现了一种解决上述问题的新方法，这是通过将注意力集中到图形挖掘解决方案中来实现的。[Velickovic et al.2018]. 注意机制通过允许模型“专注于输入的最相关部分来做出决策”来帮助模型。最初在深度学习社区引入了注意力，以帮助模型处理数据的重要部分[Bahdanau et al. 2015; Mnih et al. 2014].

&#160; &#160; &#160; &#160;该机制已成功应用于解决各种任务的模型。[Mnih et al. 2014]使用注意力来瞥见输入图像的相关部分以进行图像分类;另一方面，[Xu et al. 2015]使用注意力集中于图像字幕任务的图像的重要部分。同时[Bahdanau et al. 2015]利用注意力机制，通过在输出句子中生成相应单词时分配反映输入句子中不同单词重要性的权重，来帮助改善机器翻译任务。最后注意力机制也都同样被用于图像[Yang et al. 2016]和自然语言问答任务[Kumar et al. 2016] .然而，涉及注意力的大部分工作已经在计算机视觉和自然语言处理领域中完成了。<br>

最近以来,人们对图表的注意力模型越来越感兴趣，并且各种各样的技术开始被提出。[Choi et al. 2017; Feng et al. 2016; Han et al. 2018; Lee et al. 2018;Ma et al. 2017; Velickovic et al. 2018].尽管在所有这些论文中注意力的定义略有不同，但竞争方法确实有共同点，因为注意力被用于使模型能够专注于图的任务相关部分。

&#160; &#160; &#160; &#160;我们在第六节更精确地来讨论这些定义，这些应用注意力机制到图中的方法的主要策略。图1显示了在图表设置中注意何时有用的激励示例。

&#160; &#160; &#160; &#160;特别是，在图表上使用注意力的主要优点可归纳如下：
>（1）注意力机制允许模型避免或忽略图形的嘈杂部分，从而改善信噪比（SNR）[Lee et al。2018; Mnih]。2014。<br>
>（2）注意允许模型为图中的元素（例如，节点邻域中的不同邻居）分配相关性得分，以突出显示具有最多任务相关信息的元素，进一步提高SNR [Velickovic et al 2018]<br>
>（3）注意力也为我们提供了一种方法，使模型的结果更具解释性[Choi et al.2017;Velickovic et al. 2018]
例如，通过分析模型对不同的注意力在医学本体图中的组件，我们可以确定导致特定医学状况的主要因素。[Choi et al。2017]

&#160; &#160; &#160; &#160;在本文中，我们对图表中的文献进行了全面而有针对性的评论。 据我们所知，这是关于这一主题的第一部着作。 我们引入了三种不同的分类法，将现有工作分组为直观的类别。 然后，我们通过详细的示例激励我们的分类法，并使用每个分类法从特定的角度来调查竞争方法。 特别是，我们通过问题设置（由输入图的类型和主要问题输出定义），所使用的注意类型以及任务（例如，节点分类，图表分类等）对现有工作进行分组）。<br>
&#160; &#160; &#160; &#160;在以前的工作中，已经研究了不同类型的图（例如，齐次图，异构图，有向非循环图等）具有不同的属性（例如，归因，加权或未加权，有向或无向）和不同的输出（例如，节点嵌入,链接嵌入，图形嵌入）。第一个分类法允许我们从这个角度来调查这个领域。第二种分类法解决了在图表中应用注意力的主要策略。然后，我们引入了一个最终的分类法，按应用领域对方法进行分组; 这向读者展示了已经解决了哪些问题，或许更重要的是，揭示了尚未应用注意力模型的基于图形的重要问题。 图2显示了建议的分类法。<br>
&#160; &#160; &#160; &#160;此外，我们还总结了图表关注领域尚未解决的挑战，并为未来的工作提供了有希望的方向。

### 1.1 主要贡献
本项工作的主要贡献如下：

>（1）我们引入了三种直观的分类法来分类各种图形注意模型，并使用这些分类法调查现有方法。 据我们所知，这是关于图注意重要领域的第一次调查。<br>
>（2）我们通过从分类学的角度讨论和比较不同的图表注意力模型来明晰每个分类法<br>
>（3）我们强调在图表关注领域尚未解决的挑战。<br>
>（4）我们还就这一新兴领域未来工作的潜在领域提出建议

![图2](images/2018/11/图2.png)
建议的分类法基于（a）问题设置，（b）使用的注意类型和（c）任务或问题来对图注意模型进行分组。

### 1.2 本文的范围
&#160; &#160; &#160; &#160;在本文中，我们将重点研究和分类各种适用于图形的技术（我们在第2节中给出了注意的一般定义）。这些方法中的大多数都将图形作为输入并解决了一些基于图形的问题，例如链接预测[Sun et al。 2011]，图分类/回归[Duvenaud et al。 2015]，或节点分类[Kipf和Welling 2017]。但是，我们也考虑了关注图形的方法，尽管图形只是问题的几种输入类型之一。<br>
&#160; &#160; &#160; &#160;我们不会尝试调查未明确应用注意力的一般基于图形的方法的广泛领域，已经对此进行了多项工作，每项工作都有一个特定的重点。[Cai et al. 2018; Getoor and Diehl 2015; Jiang et al. 2013]
### 1.3 综述的结构
&#160; &#160; &#160; &#160;本次综述的其余部分安排如下。 我们首先在第2节中介绍有用的符号和定义。然后我们使用第3节到第5节来讨论使用我们的主要分类法的相关工作（图2a）。 我们组织了第3-5节，使得现有工作中的方法按照它们计算的主要嵌入类型进行分组（例如，节点嵌入，边缘嵌入，图形嵌入或混合嵌入）; 然后将这些方法进一步划分为它们支持的图形类型。 在第6节和第7节中，我们转向不同的视角，并使用其余的分类法（图2b和图2c）来指导讨论。 然后，我们将在第8节讨论挑战以及未来工作的有趣机会。最后，我们在第9节中总结了综述结果。
## 2 问题的形式
&#160; &#160; &#160; &#160;在本节中，我们定义了讨论中出现的不同类型的图形; 我们还介绍了相关的符号。<br>
&#160; &#160; &#160; &#160;定义1（同构图）<br>
&#160; &#160; &#160; &#160;设G =（V，E）是图，其中V是节点（顶点）的集合，E是V中节点之间的边集。更进一步，让A=[A<sub>ij</sub>]为n*n,对于n为|V|，G的邻接矩阵，其中A<sub>ij</sub> = 1。存在（v<sub>i</sub>，v<sub>j</sub>）∈E和A<sub>ij</sub> = 0.否则。 当A是二进制矩阵（即，A<sub>ij</sub>∈{0,1}）时，我们认为该图是未加权的。注意，A也可以编码边权重;对于（v<sub>i</sub>，v<sub>j</sub>）∈E，A<sub>ij</sub> = w，给定权重w∈R。 在这种情况下，图表被称为加权。 此外，如果A<sub>ij</sub> = A<sub>ji</sub>，对于任何1≤i，j≤n，则图表是无向的，否则它是有向的。<br>
&#160; &#160; &#160; &#160;给定邻接矩阵A，我们可以构造一个右随机矩阵T - 也称为转移矩阵 - 它只是A，行标准化为1。另外，给定顶点v∈V，让Γv是节点v邻域中的顶点集合（例如，邻域的流行定义就是v的单跳邻域，或者Γv= {w |（v，w）∈Ë}）<br>
&#160; &#160; &#160; &#160;定义2（异构图）。<br>
&#160; &#160; &#160; &#160;异构图被定义为G =（V，E），其由一组节点对象V和连接V中的节点的一组边E组成。异构图还具有节点类型映射函数θ：V→TV和边界类型映射函数，其定义为ξ：E→TE，其中TV和TE分别表示节点类型集和边缘类型。节点i的类型表示为θ（i）（其可以是作者，论文或异构书目网络中的会议），而边缘的类型e =（i，j）∈E表示为ξ（ i，j）=ξ（e）（例如，共同作者关系，或“发表于”关系）<br>
&#160; &#160; &#160; &#160;注意，异构图有时被称为类型化网络。 此外，二分图是具有两种节点类型和单边类型的简单异构图。 同构图只是异构图的一个特例，其中| TV | = | TE | = 1。<br>
&#160; &#160; &#160; &#160;定义3（归因图）<br>
设G =（V，E，X）为属性图，其中V是一组节点，E是一组边，X是节点输入属性的n×d矩阵，其中每个x¯i（或Xi： ）是节点vi∈V的属性值的d维（行）向量，xj（或X：j）是对应于X的第j个属性（列）的n维向量。或者，X也可以是 ×d边缘输入属性矩阵。 这些可能是实值的，也可能不是。
定义4（有向无环图（DAG））没有循环的有向图<br>
&#160; &#160; &#160; &#160;定义1-3中定义的图形的不同“类别”可以是定向的或非定向的，也可以是加权的或未加权的。其他类别的图表来自于组成不同的“图类”（定义1-3）。 例如，定义归因异构网络G =（V，E，TV，TE，X）是直截了当的。<br>
&#160; &#160; &#160; &#160;定义5（路径）<br>
&#160; &#160; &#160; &#160;长度为L的路径P被定义为唯一索引i1，i2，...的序列。。。 ，iL + 1这样（vit，vit + 1）∈E对于所有1≤t≤L。路径的长度定义为它包含的边数。<br>

&#160; &#160; &#160; &#160;与允许循环的遍历不同，路径不会，因此如果没有循环（所有节点都是唯一的），则walk是路径。 请注意，路径是一种特殊的图形，具有非常严格的结构，其中所有节点最多具有2个邻居。 ![我们现在给出关于图注意概念的一般但正式的定义如下：](images/2018/11/图2.png)
定义6（图注意）<br>
&#160; &#160; &#160; &#160;给定目标图形对象（例如，节点，边缘，图形等），v<sub>0</sub>和v<sub>0</sub>的“邻域”中的一组图形对象{v<sub>1</sub>，···，v <sub>|Γv0|</sub> }∈Γ<sub>v0</sub>（邻域是特定于任务的，可以表示节点的ℓ-hop邻域或更一般的东西）。 注意被定义为函数f'：{v0}×Γ<sub>v0</sub>→[0,1]，它将Γv0中的每个对象映射到相关性分数，该相关性分数告诉我们想要给予特定邻居对象多少关注。 此外，通常期望Í|Γv0| i = 1f'（v<sub>0</sub>，v<sub>i</sub>）= 1。<br>
&#160; &#160; &#160; &#160;表1中提供了整个手稿中使用的符号摘要
## 3 基于注意力机制的节点/边嵌入
&#160; &#160; &#160; &#160;在本节，以及接下来的两个部分中，我们介绍了各种图注意力模型，并通过问题设置对它们进行分类。为便于参考，我们展示了表2中的所有调查方法，注意在每个提议的分类法下突出它们的属性。我们现在开始定义传统的节点嵌入问题。<br>
&#160; &#160; &#160; &#160;定义7（节点嵌入）给定图G =（V，E），其中V作为节点集，E作为边集，节点嵌入的目的是学习函数f：V→R<sup>k</sup>，使得每个节点i∈V映射到k - 维向量z<sub>i</sub>，其中k«| V |。节点嵌入矩阵表示为**Z**.<br>
&#160; &#160; &#160; &#160;作为输出给出的学习节点嵌入随后可以用作挖掘和机器学习算法的输入，用于各种任务，例如链路预测[Sun et al。 2011]，分类[Velickovic等。 2018年]，社区检测[Girvan and Newman 2002]和角色发现[Rossi and Ahmed 2015]。<br>
&#160; &#160; &#160; &#160;我们现在定义基于注意力的节点嵌入问题如下：<br>
&#160; &#160; &#160; &#160;定义8（基于注意的节点嵌入）<br>
&#160; &#160; &#160; &#160;给定与上面相同的输入，我们学习函数f：V→R<sup>k</sup>，其将每个节点i∈V映射到嵌入向量z<sub>i</sub>。另外，我们学习第二函数f'：V×V→[0,1]以将“注意权重”分配给目标节点i的邻域Γ<sub>i</sub>中的元素。函数f'定义每个邻居j，对于j∈Γ<sub>i</sub>，相对于目标节点i的重要性。通常，对于所有i，f'被约束为具有σ<sub>j</sub>∈Γif'（i，j）= 1，对于所有k <Γ<sub>i</sub>，f'（i，k）= 0。基于注意力的节点嵌入的目标是为节点i及其更相似或更重要的邻居分配类似的嵌入，即δ（z<sub>i</sub>，z<sub>j</sub>）>δ（z<sub>i</sub>，z<sub>k</sub>）iff f'（i，j）> f'（i，k），其中δ是某种相似性度量。<br>
&#160; &#160; &#160; &#160;上面，我们定义了基于注意力的节点嵌入; 基于注意力的边缘嵌入也可以类似地定义<br>
&#160; &#160; &#160; &#160;在这里，我们使用单个部分来讨论节点和边嵌入，因为在基于注意力的边嵌入方面还没有很多工作，并且因为这两个问题非常相似。 我们现在根据它们支持的图表类型对基于注意力的节点/边缘嵌入进行计算的不同方法进行分类。
![](images/2018/11/4.png)

### 3.1 同构图
&#160; &#160; &#160; &#160;计算基于注意力的节点嵌入的大多数工作都集中在同构图上。 此外，所有方法都假设图表被归因，尽管只需要节点标签（在这种情况下，属性大小d = 1）。<br>
&#160; &#160; &#160; &#160;[Abu-El-Haija]等人提出的方法。 2017年，名为AttentionWalks，最让人想起流行的节点嵌入方法，如DeepWalk和node2vec [Grover和Leskovec 2016; Perozzi等。 因此，随机游走用于计算节点上下文。 给定具有其对应的转移矩阵T的图G和上下文窗口大小c，我们可以计算在每个节点处开始行走的预期次数，通过以下方式访问其他节点：<br>
>$$E[D|a_1, · · · , a_c] =   I_n\sum_{i=0}^{n}a_i(T)^i$

&#160; &#160; &#160; &#160;这里的I<sub>n</sub>是size-n单位矩阵，a<sub>i</sub>，1≤i≤c，是可学习的注意权重，**D**是游走分布矩阵，其中D<sub>uv</sub>编码节点u预期访问节点v的次数，假定从每个节点开始步行。在这种情况下，注意力因此被用于引导步行朝向更广泛的邻域或将其限制在更窄的邻域内（例如，当权重是头重脚轻时）。这解决了必须进行网格搜索以识别最佳超参数的问题，因为研究表明这对性能有显着影响[Perozziet al。 2014] - 请注意，对于DeepWalk，权重固定为$a_i = 1-\frac{i-1}{c}\\$ 。<br>
&#160; &#160; &#160; &#160;另一种基于注意力的方法在精神上与DeepWalk，node2vec和LINE等方法非常相似，因为它使用共生信息来学习节点嵌入是GAKE这种方法。重要的是要指出GAKE从知识三元组（h，t，r）构建图形，通常称为知识图形，其中h和t是由关系r连接的项。基于三个三元组（Jose，Tagalog，speaks），（Sato，Nihongo,speaks）和（Jose，Sinigang，eat），我们可以构建一个简单的异构图，其中包含三种类型的节点，即{person，language，food}和 两种类型的关系，即{speak，eats}。然而，我们将GAKE归类为同构图方法，因为它似乎没有区分不同类型的关系或节点类型（请参阅metapath2vec [Dong et al.2017]，了解明确模拟不同类型关系的节点嵌入方法和异构图中的节点）。相反，该方法采用一组知识三元组，并通过取每个三元组（h，t，r）并分别添加h和t作为头部和尾部节点来构建有向图，同时从头部到尾部添加边缘 （他们还添加了反向链接）。 GAKE与DeepWalk或node2vec等方法之间的主要区别在于它们在计算节点的上下文时包含边。他们定义了三个不同的上下文来获得相关主题（节点或边缘），正式地说它们最大化了对数似然：<br>
$$ \sum_{s \in V ∪E	}\sum_{c \in \Gamma_s} \log Pr(s |c)$$
其中$Γ_s$ 是定义s的邻域上下文的一组节点和/或边。 为了在图中给出给定主题的最终节点嵌入，他们使用注意力机制来获得最终嵌入$z_s=\sum_{c \in \Gamma_s}$ α（c）$z'_c$。其中 $\Gamma'_s$是α的一些邻域上下文。α（c）定义上下文对象c的注意权重，$z'_c$是c的学习嵌入。<br>
&#160; &#160; &#160; &#160;另一方面，GAT和AGNN等方法通过结合明确的注意机制来扩展图卷积网络（GCN）。 回想一下，GCN能够使用传播规则通过输入图的结构传播信息：
$$ H^{(l+1)}=σ(\check{D}^\frac{-1}{2}\check A \check D \frac{-1}{2}H^l W^l ) $$
其中 $\ H^l$ 表示在具有 $\ H_0=X$的GCN的层L处的学习的嵌入矩阵。此外 $\ \check A =A+I_n$ 是附加自循环的无向图A的邻接矩阵.另一方面，矩阵 $\ \check{D}$ 被定义为 $\ \check{A}$ 的对角矩阵，换句话说，$\ \check{D}_{i,i}= \sum_i{ \check A_{i,j}}$ .因此，项 $\ \check D^\frac{-1}{2} \\$
计算由 $\ \check A \\$ 定义的图的对称归一化（类似于归一化图拉普拉斯算子）。 最后，$\ W_l \\$ 是级别l的可训练权重矩阵，σ（·）是非线性，如ReLU，Sigmoid或tanh.<br>
GCN的工作方式类似于Weisfeiler-Lehman算法的端到端可微分版本，其中每个附加层允许我们扩展和集成来自更大邻域的信息。<br>
&#160; &#160; &#160; &#160;然而，因为我们在传播中使用术语  $\ \check{D}^\frac{-1}{2}\check A \check D \\$，所以我们基本上应用由其度数归一化的相邻节点的特征的加权和。GAT和AGNN基本上引入了注意权重值，以确定我们希望从节点u的角度给予相邻节点v多少。 这两种方法的不同之处主要在于注意力的定义方式（我们在第6节中对此进行了阐述）。此外，引入了“多关注”的概念，其基本上定义了图中一对对象之间的多个关注头（例如，权重）。<br>
&#160; &#160; &#160; &#160;PRML是另一种学习边嵌入的方法，但它们使用不同的策略来定义注意力。 在[赵等人。 2017]，定义了路径长度阈值L和递归神经网络[Gers et al。 2000]在节点对u和v之间学习长度为1，...，L的路径的基于路径的嵌入。注意力以两种方式定义。 首先，注意用于识别沿路径的重要节点，这有助于计算中间路径嵌入。 其次，注意力然后优先考虑更多指示链路形成的路径，并且当这些路径嵌入被集成以形成最终边缘嵌入时，这用于突出重要或任务相关的路径嵌入。 我们可以将此方法视为基于注意力的模型，该模型计算节点对的Katz中介性得分
### 3.2异构图
&#160; &#160; &#160; &#160;据我们所知，为异构图计算注意力引导节点嵌入的唯一工作是EAGCN [Shang et al。2018。 非常类似于GAT和AGNN，[Shang et al。 2018]建议关注GCN [Duvenaud et al。 2015; Kipf和Welling 2017]。 但是，它们处理的情况是，可以有多种类型的链接连接图形中的节点。 因此，他们建议使用“多重注意”，如[Velickovic et al.2018]，并且每个注意机制都考虑仅由特定链接类型定义的邻居。 尽管作者使用图形回归任务验证了EAGCN，但是该方法可以在没有对节点级任务进行太多调整的情况下使用，因为EAGCN中的图形嵌入仅仅是学习的注意引导节点嵌入的连接或总和。<br>
&#160; &#160; &#160; &#160;然而，上述工作假定给定的异构网络仅具有一种类型的节点，即  $\ | \Gamma_V | = 1 \\$
### 3.3其他特殊情况
&#160; &#160; &#160; &#160;与基于注意力的图形嵌入领域不同，对于出现在某些领域中的特殊类型的图形（例如，表示为DAG的医学本体，或表示为某些星图的某些异构网络），似乎没有计算注意力引导的节点嵌入的工作。<br>
&#160; &#160; &#160; &#160;由于上述方法适用于一般图形，因此它们应适用于更具体类型的图形，并且应该能够将它们直接应用于这些情况。<br>
## 4基于注意力的图嵌入
同样，我们通过定义传统的图嵌入问题开始讨论.<br>
定义9（图嵌入）:给定一组图形，图形嵌入的目的是学习一个函数f : $\ G →R^k \\$ 其将每个输入图G∈**G**映射到长度为k的嵌入向量z。此外，我们学习第二函数f'，其将“注意权重”分配给不同的 给定图的子图允许模型在计算其嵌入时优先考虑图的更重要部分（即子图）。
