# 网络规模推荐系统的图卷积神经网络

摘要：最近关于图形结构数据的深层神经网络的进展已经导致了推荐系统基准的最先进的性能。然而，使这些方法具有实用性和可伸缩性。 具有数十亿项目和数亿用户的网络规模推荐任务仍然是一个挑战。

在这里，我们描述了一个大规模的深度推荐引擎，我们开发并部署在Pinterest上。我们提出了一种数据有效的图卷积网络(GCN)算法PinSage，该算法将有效的随机游动和图卷积相结合，生成不相关的节点(即项)的嵌入。 对图结构和节点特征信息进行排序。与以往的gcn方法相比，我们提出了一种新的基于高效随机游走的方法来构造卷积，并设计了一种依赖于困难和困难的训练策略。 

我们在Pinterest上部署PinSage，并在一个图形上对75亿个例子进行训练，其中30亿个节点代表引脚和板，180亿个边。根据离线度量，用户研究和 A/B测试，PinSage产生了比类似的深度学习和基于图表的替代方案更高质量的建议。据我们所知，这是迄今为止深图嵌入的最大应用，也为基于图形卷积体系结构的新一代web规模推荐系统铺平了道路。 

## 1.简介

深度学习方法在推荐系统应用中扮演着越来越重要的角色，被用于学习有用的低维嵌入图像、文本，甚至单个用户[9,12]。使用深度模型学习的表示可以用来补充，甚至取代传统的推荐算法，如协同过滤。这些学习表示具有很高的实用价值，因为它们可以在各种推荐任务中重用。例如，使用深度模型学习的项嵌入可以用于项-项推荐，也可以用于推荐的主题集合(例如播放列表或“提要”内容)。

近年来，这一领域取得了重大进展，特别是新的深度学习方法的发展，这些新的深入学习方法能够对图形结构数据进行学习，这对于 推荐应用程序(例如，利用用户对项交互图以及社交图)[6、19、21、24、29、30]。

在这些最近的进步中，最突出的是被称为图卷积网络(GCNS)的深度学习架构的成功[19，21，24，29]。GCNS背后的核心思想是学习如何 o使用神经网络迭代地从本地图邻域聚合特征信息(图1)。这里，一个单独的“卷积”操作从 节点的单跳图邻域，通过叠加多个这样的卷积，信息可以传播到图的很远的区域。与纯粹基于内容的深度模型(如递归神经网络[3])不同，GCNS既利用了内容信息，也利用了图形结构。基于gcn的方法已经建立了一个新的计数标准。 S推荐系统基准(调查见[19])。然而，在基准任务上的这些收益尚未转化为实际生产环境中的收益。

主要挑战是将基于GCN的节点嵌入的训练和推理扩展到具有数十亿个节点和数百亿个边的图。扩大全球氯化萘的规模是困难的，因为 在大数据环境中工作时，他们的设计所依据的许多核心假设都会被违反。或者，所有现有的基于gcn的推荐系统都需要在培训期间对全图Laplacian进行操作-当底层图有数十亿个节点时，这种假设是不可行的。 它的结构在不断发展。

**现在的工作：**在这里，我们介绍了一个高度可伸缩的GCN框架，我们已经开发并部署在Pinterest的生产中。我们的框架是一个基于随机游走的gcn，名为Pinsage，它在一个大型图上运行 

> ![1555314474956](F:\Machine-learning-and-data-science-notebook\images\网络规模推荐系统的图卷积神经网络\1555314474956.png)图1：使用深度-2卷积的模型体系结构概述(最好用颜色查看)。左：一个小示例输入图。右：计算嵌入h(2)的2层神经网络。 使用节点A的前一层表示h(1)A及其邻域N(A)(节点B、C、D)的a。(然而，邻域的概念是普遍的，并不是所有的邻居都需要包括在内(3.2节)。底部：计算输入图的每个节点的嵌入的神经网络。而Ne  共享参数；γ表示重要池函数；瘦矩形框表示密集连接的多层神经网络。

- 快速卷积：传统的GCN算法通过将特征矩阵乘以全图Laplacian的幂来执行图卷积。相反，我们的PinSage算法执行效率 通过对节点周围的邻域进行采样，并从该抽样邻域动态构造计算图，从而实现局部化卷积。这些动态构建的计算图(图1)指定了如何围绕特定节点进行局部卷积，从而减少了训练过程中对整个图的操作。
- 生产者-消费者小型批次结构：我们开发了一个生产者-消费者架构，用于构建小型批次，以确保在模型培训期间最大限度地利用GPU。一个大内存cpu约束的生成器有效地采样节点网络邻域并获取必要的特征来定义局部卷积，而一个gpu约束的TensorFlow模型则使用这些预定义的计算图来高效地运行随机梯度。
- 有效的MapReduce推断：给定一个经过充分训练的gcn模型，我们设计了一个高效的MapReduce管道，它可以将经过训练的模型分发给数十亿个节点，同时生成嵌入。 模仿重复的计算。

除了这些在可扩展性方面的基本进步，我们还介绍了新的培训技术和算法创新。这些创新提高了代表学习的质量。 由PinSage编写，在下游推荐系统任务中导致显着的性能提升：

- 通过随机游动构造卷积：利用节点的完整邻域进行卷积(图1)将产生巨大的计算图，因此我们采取抽样的方法。然而，随机抽样 LING是次优的，我们提出了一种利用短随机游程对计算图进行采样的新方法。。另一个好处是，每个节点现在都有一个重要评分，这是我们在池/聚合步骤中使用的。
- 重要性池：图卷积的一个核心组件是图中局部邻域特征信息的聚合。我们介绍了一种衡量节点特征的重要性的方法。 基于随机步态相似性度量的Res在这种聚合中，导致离线评估指标中46%的性能增益。
- 课程训练:我们设计了一个课程训练方案，在训练过程中给算法输入更困难的例子，使算法的性能提高12%

我们已经在Pinterest部署了PinSage来执行各种推荐任务，这是一种流行的内容发现和管理应用程序，用户可以在该应用程序中与引脚进行交互，这是在线的可视书签 内容(例如，他们想做的食谱，或者他们想买的衣服)。总的来说，pinterest是世界上最大的用户管理图片，有超过20亿个独特的引脚被收集到超过10亿块板中。

通过广泛的离线指标、受控用户研究和A/B测试，我们表明，与其他可伸缩的基于深度内容的推荐算法相比，我们的方法在项目-项目推荐任务(即，以及一个“homefeed”推荐任务。在离线排名指标中，我们比最佳执行基准提高了40%以上，在面对面的人工评估中，我们的推荐在60%的情况下是首选的，而A/B测试显示，在不同设置下，用户参与度提高了30%至100%。

据我们所知，这是迄今为止最大规模的深图嵌入应用，并为基于图卷积体系结构的新一代推荐系统铺平了道路。

## 2 相关工作

我们的工作建立在最近对图形结构数据的深入学习方法的一些进展的基础上。

图数据的神经网络的概念首先在Gori等人中作了概述。[15]并在斯卡塞利等人案中作了进一步阐述。(2009)[27]。然而，这些最初的深入学习方法 G在图上需要运行昂贵的神经“消息传递”算法来收敛，而在大型图上则昂贵得令人望而却步。门控图序列神经元的一些局限性 AlNetworks[22]-它采用了现代递归神经结构-但是这种方法在计算上仍然很昂贵，并且主要应用在节点小于10，000个的图上。

最近，大量的方法依赖于“图卷积”或图卷积网络(GCNS)的概念。这种方法起源于Bruna等人的工作。(2013年) 在此基础上发展了一种基于谱图的图卷积式[7]。在这项工作之后，许多作者提出了这些谱的改进、扩展和近似。 卷积[6，10，11，13，18，21，24，29，31]，导致关于基准(如节点分类、链路预测以及推荐系统任务(例如mov)的最新结果。 ieLens基准[24]。这些方法一直优于基于矩阵分解或随机游动的技术(例如node2vec[17]和DeepWalk[26])，它们的成功导致了 对将基于全球氯化萘的方法应用于从推荐系统[24]到药物设计[20，31]的应用的兴趣激增[20，31]。Hamilton等人(2007 B)[19]和Bronstein等人。(2017)[6]提供c 对最近进展的综合调查。

然而，尽管gcn算法取得了成功，以前的任何工作都没有成功地将它们应用到具有数十亿节点和边缘的生产规模数据中，这一限制主要是由于以下事实造成的。 传统的GCN方法需要在训练过程中对整个图Laplacian进行操作。在这里，我们填补了这一空白，并表明gcns可以在一个生产规模的推荐系统中运行。 m涉及数十亿节点/项目的设置。我们的工作还展示了gcns在现实环境中对推荐性能的实质性影响。

 我们修改的GCNS的归纳变体，以避免对整个图Laplacian进行操作。我们从根本上改进了GraphSAGE，消除了将整个图存储在GPU备忘录中的限制 RY，使用低延迟的随机游走来对生产者-消费者架构中的图邻域进行采样。我们还介绍了一些新的训练技术，以提高性能和MapReduce推断 扩展到具有数十亿个节点的图表。

最后，请注意，像node2vec[17]和DeepWalk[26]这样的图嵌入方法不能在这里应用。首先，这些是无监督的方法。第二，它们不能包含节点特征信息。 。第三，它们直接学习节点的嵌入，因此模型参数的数量与图的大小成线性关系，这对我们的设置是不允许的。

## 3 方法

在本节中，我们将描述PinSage体系结构和培训的技术细节，以及使用经过训练的PinSage模型高效生成嵌入的MapReduce管道。

我们的方法的关键计算工作是局部图卷积的概念。1为了生成节点(即一个项)的嵌入，我们应用了多个卷积模块。 来自节点的本地图形邻域的eGate特征信息(例如，可视化、文本特性)(图1)。每个模块学习如何从一个小图邻域聚合信息，并通过 通过叠加多个这样的模块，我们的方法可以获得关于本地网络拓扑的信息。重要的是，这些本地化卷积模块的参数在所有节点之间都是共享的，这使得 该方法的参数复杂度与输入图大小无关。

3.1 问题设置