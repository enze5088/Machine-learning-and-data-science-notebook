# **GraphGAN:**用生成对抗网进行图形表示学习

摘要：图表示学习的目标是将图中的每个顶点嵌入到低维向量空间中。现有的图表示学习方法可分为两类：学习图中底层连通性分布的生成模型，以及预测一对顶点之间边缘存在概率的识别模型。本文提出了一种将上述两类方法统一起来的图形表示学习框架graphgan，其中生成模型和判别模型进行博弈论上的极大极小博弈。具体地说，对于一个给定的顶点，生成模型尝试在所有其他顶点上匹配其底层的真实连通性分布，并生成“假”样本来愚弄识别模型，而识别模型尝试检测采样的顶点是来自地面真值还是由生成模型生成。随着这两个模型之间的竞争，它们都可以交替地和迭代地提高性能。此外，在考虑生成模型的实现时，我们提出了一种新的图SoftMax来克服传统的SoftMax函数的局限性，证明它满足规范化、图结构感知和计算效率的期望特性。通过对现实数据集的大量实验，我们证明了在各种应用中，graphgan比最先进的基线在链路预测、节点分类和推荐方面取得了实质性的进展。

## 介绍

图表示学习又称网络嵌入，旨在将图（网络）中的每个顶点表示为一个低维向量，这有助于对顶点和边进行网络分析和预测。学习的嵌入能够为各种实际应用带来好处，如链路预测（gao、denoyer和gallinari 2011）、节点分类（tang、aggarwal和liu 2016）、建议（yu等人2014年）、可视化（Maaten和Hinton 2008年）、知识图表示（Lin等人2015），聚类（Tian等人2014年）、文本嵌入（Tang、Qu和Mei，2015年）和社交网络分析（Liu等人2016年）。最近，研究人员研究了将表示学习方法应用于各种类型的图，如加权图（Grover和Leskovec 2016）、有向图（Zhou等人2017年），签名图（Wang等人2017b），异质图（Wang等人以及归因图（Huang、Li和Hu 2017）。此外，之前的几项工作还试图在学习过程中保留特定的属性，例如全球结构（Wang、Cui和Zhu 2016）、社区结构（Wang等人2017c）、群体信息（Chen、Zhang和Huang，2016）和不对称传递性（Ou等人2016）。

可以说，大多数现有的图形表示学习方法可以分为两类。第一种是生成图表示学习模型（Perozzi、Al Rfou和Skiena，2014年；Grover和Leskovec，2016年；Zhou等人。2017年；Dong、Chawla和Swami 2017年；Li等人 2017年a）。与传统的生成模型（如高斯混合模型（Lindsay 1995）或潜在Dirichlet分配（Blei、Ng和约旦2003）相似，生成图表示学习模型假设，对于每个顶点vc，存在一个潜在的真实连接分布ptrue（v vc），这意味着vc的连接偏好（或relev在图中的所有其他顶点上。因此，可以将图中的边视为这些条件分布生成的观测样本，这些生成模型通过最大化图中边的可能性来学习顶点嵌入。例如，Deepwalk（Perozzi、Al-Rfou和Skiena 2014）使用随机游走对每个顶点的“上下文”顶点进行采样，并尝试最大化观察给定顶点上下文顶点的日志可能性。node2vec（grover和leskovec，2016）进一步扩展了这一想法，提出了一个有偏差的随机行走过程，在为给定顶点生成上下文时提供了更大的灵活性。

第二种图形表示学习方法是识别模型（Wang等人2018年；曹、陆、徐2016年；王、崔、朱2016年；李等。2017b年）。与生成模型不同，识别图表示学习模型不将边缘视为从底层条件分布生成的，而是旨在学习直接预测边缘存在的分类器。通常，识别模型将两个顶点vi和vj联合作为特征，并根据图中的训练数据预测两个顶点之间存在边的概率，即p边（vi，vj）。例如，sdne（wang，cui，and zhu 2016）使用顶点的稀疏邻接向量作为每个顶点的原始特征，并应用自动编码器在边存在的监督下提取顶点的简短和浓缩特征。PPNE（Li等人2017b）直接学习顶点嵌入，在正样本（连接顶点对）和负样本（断开顶点对）上进行监督学习，同时在学习过程中保留顶点的固有属性。

虽然生成模型和识别模型通常是两种不相交的图形表示学习方法，但它们可以被视为同一硬币的两面（Wang等人2017年a）。事实上，Line（Tang等人2015年）已经就隐式结合这两个目标（一线和二线）进行了初步试验。最近，生殖对抗网（Gan）（Goodfellow等人2014）受到了广泛关注。通过设计一个博弈理论的极大极小博弈来结合生成和识别模型，gan及其变体在各种应用中获得成功，例如图像生成（denton等人2015），序列生成（Yu等人2017），对话生成（Li等人2017c），信息检索（Wang等人2017a），以及领域适应（张、巴茨雷和贾科拉，2017）。

本文以Gan为灵感，提出了一种将生成性思维与识别性思维相结合的图形表示学习新框架——graph gan。具体来说，我们的目标是在图形学习过程中训练两个模型：1）生成器G（v_vc），它试图尽可能地匹配底层的真实连通性分布ptrue（v_vc），并生成最有可能与vc连接的顶点；2）鉴别器D（v，vc），它试图区分连接良好的顶点对F。并计算了V和VC之间是否存在边的概率。在所提出的图形中，G发生器和D鉴别器在极大极小博弈中扮演两个角色：在鉴别器的指导下，G发生器试图产生最难以区分的“伪”顶点，而鉴别器则试图在基本真理和“伪”之间划出一条清晰的界线，以避免被愚弄。通过发电机。这场游戏中的竞争促使他们都提高了自己的能力，直到生成器与真实的连接分布不可区分。

在GraphGAN框架下，我们研究了发生器和鉴别器的选择。不幸的是，我们发现传统的SoftMax函数（及其变体）不适用于生成器，原因有两个：1）SoftMax对一个给定顶点对图中所有其他顶点的处理是相同的，缺乏对图结构和邻近度信息的考虑；2）SoftMax的计算涉及到图中所有的顶点，以及ICH耗时且计算效率低下。为了克服这些局限性，在graphgan中，我们提出了一种新的生成器graph softmax的实现。图SoftMax为图中的连接分布提供了新的定义。我们证明了图的Softmax满足规范化、图结构感知和计算效率的期望特性。因此，我们提出了一种基于随机游走的发电机在线发电策略，该策略与图SoftMax的定义相一致，可以大大降低计算复杂度。

根据经验，我们将graphgan应用于三个真实场景，即链接预测、节点分类和建议，使用五个真实的图形结构数据集。实验结果表明，在图形表示学习领域，与最新的基线相比，GRAPHGAN取得了显著的进步。具体来说，在链路预测方面，graphgan优于基线0.59%至11.13%，在节点分类方面，两者的准确性均优于基线0.95%至21.71%。此外，graphgan将精度@20提高了至少38.56%，建议召回@20至少52.33%。我们将graphgan的优越性归因于其统一的对抗性学习框架，以及能够自然地从图形中捕获结构信息的邻近感知图形Softmax的设计。

## 图生成对抗网络

在这一部分中，我们介绍了graphgan的框架，并讨论了生成器和鉴别器的实现和优化细节。在此基础上，我们提出了一个实现为生成函数的图SoftMax，并证明了它优于传统的SoftMax函数。

### GraphGAN框架

我们为图形表示学习建立了生成对抗网，如下所示。设g=（v，e）为给定图，其中v=v1，…，vv表示顶点集，e=e i j v i，j=1表示边集。对于给定的顶点vc，我们将n（vc）定义为直接连接到vc的顶点集，其大小通常比顶点总数v小得多。我们将顶点vc的底层真实连通性分布表示为条件概率ptrue（v_vc），它反映了vc对v中所有其他顶点的连通性偏好分布。从这个角度来看，n（vc）可以看作是从ptrue（v_vc）中提取的一组观测样本。根据图G，我们旨在学习以下两个模型：

generator g（v_vc；θg），试图近似底层真实连通性分布ptrue（v_vc），并从顶点集v生成（或选择，如果更精确的话）最可能与vc连接的顶点。

鉴别器d（v，vc；θd），用于区分顶点对（v，vc）的连通性。d（v，vc；θd）输出单个标量，表示v和vc之间存在边的概率。

生成器G和鉴别器D作为两个对手：生成器G试图完美地拟合ptrue（v_vc），并生成与vc的实邻相似的相关顶点来欺骗鉴别器，而鉴别器D则相反，试图检测这些顶点是vc的实邻还是由vc生成的实邻。其对应的G。形式上，G和D是在玩以下两个具有值函数V（G，D）的最小最大博弈：

$\begin{aligned} \min _{\theta_{G}} \max _{\theta_{D}} V(G, D) &=\sum_{c=1}\left(\mathbb{E}_{v \sim p_{\text { true }}\left(\cdot | v_{c}\right)}\left[\log D\left(v, v_{c} ; \theta_{D}\right)\right]\right.\\ &+\mathbb{E}_{v \sim G\left(\cdot | v_{c} ; \theta_{G}\right)}\left[\log \left(1-D\left(v, v_{c} ; \theta_{D}\right)\right)\right] ) \end{aligned}$

![1561451670227](F:\Machine-learning-and-data-science-notebook\images\用生成对抗网络进行图表示学习\1561451670227.png)

根据式（1），通过交替地最大化和最小化值函数v（g，d），可以得到发电机和鉴别器的最佳参数。graphgan框架如图1所示。在每次迭代中，鉴别器d都使用ptrue（·vc）（绿色顶点）中的正样本和Generator g（·vc；θg）（蓝色条纹顶点）中的负样本进行训练，Generator g在d的指导下更新为策略梯度（在本节后面详细介绍）。G和D之间的竞争促使他们都改进了他们的方法，直到G与真正的连通性分布不可区分。我们将讨论D和G的实现和优化，如下所示。

### 鉴别器优化

给定真实连通分布中的正样本和发生器中的负样本，鉴别器的目标是最大化将正确标签分配给正样本和负样本的对数概率，如果d相对于θd是可微的，则可以通过随机梯度上升来求解。，我们将d定义为两个输入顶点内积的乙状函数：

$D\left(v, v_{c}\right)=\sigma\left(\mathbf{d}_{v}^{\top} \mathbf{d}_{v_{c}}\right)=\frac{1}{1+\exp \left(-\mathbf{d}_{v}^{\top} \mathbf{d}_{v_{c}}\right)}$

其中，d v，d vc∈rk分别是判别器d的顶点v和vc的k维表示向量，θd是所有dv的并集，任何判别模型都可以作为d，如sdne（Wang，Cui，and Zhu 2016），我们将进一步研究判别器的选择，以备今后的工作。注意，等式（2）只涉及v和vc，这表明给定一个样本对（v，vc），我们只需要通过提高它们的梯度来更新dv和dvc：

$\nabla_{\theta_{D}} V(G, D)=\left\{\begin{array}{l}{\nabla_{\theta_{D}} \log D\left(v, v_{c}\right), \text { if } v \sim p_{\text { true }}} \\ {\nabla_{\theta_{D}}\left(1-\log D\left(v, v_{c}\right)\right), \text { if } v \sim G}\end{array}\right.$

### 生成器优化

与鉴别器相比，生成器的目标是最小化鉴别器正确地为G生成的样本分配负标签的日志概率。换句话说，发电机改变其近似连接性分布（通过其参数θg）以增加其生成样本的分数，如D所判断的那样。因为V的采样是离散的，如下（Schulman等人2015年；Yu等人2017年），我们建议通过政策梯度计算相对于θg的v（g，d）梯度：

$\begin{aligned} & \nabla_{\theta_{G}} V(G, D) \\=& \nabla_{\theta_{G}} \sum_{c=1}^{V} \mathbb{E}_{v \sim G\left(\cdot | v_{c}\right)}\left[\log \left(1-D\left(v, v_{c}\right)\right)\right] \end{aligned}$

$=\sum_{c=1}^{V} \sum_{i=1}^{N} \nabla_{\theta_{G}} G\left(v_{i} | v_{c}\right) \log \left(1-D\left(v_{i}, v_{c}\right)\right)$
$=\sum_{c=1}^{V} \sum_{i=1}^{N} G\left(v_{i} | v_{c}\right) \nabla_{\theta_{G}} \log G\left(v_{i} | v_{c}\right) \log \left(1-D\left(v_{i}, v_{c}\right)\right)$
$=\sum_{c=1}^{V} \mathbb{E}_{v \sim G\left(\cdot | v_{c}\right)}\left[\nabla_{\theta_{G}} \log G\left(v | v_{c}\right) \log \left(1-D\left(v, v_{c}\right)\right)\right]$

为了理解上述公式，值得注意的是，在GV(G，D)中的梯度是通过log-概率log1/d(v，Vc；(d)，which加权的logglogg(v|Vc；(g)的梯度上的预期总和； 从直觉上讲，H表示负样本概率较高的顶点将“拖曳”发生器G，使其远离自身，因为我们在θG上应用了梯度下降。

我们现在讨论G的实现，一种简单的方法是将生成器定义为所有其他顶点上的一个Softmax函数(Wang等人)。(2007 A)，即，$G\left(v | v_{c}\right)=\frac{\exp \left(\mathbf{g}_{v}^{\top} \mathbf{g}_{v_{c}}\right)}{\sum_{v \neq v_{c}} \exp \left(\mathbf{g}_{v}^{\top} \mathbf{g}_{v_{c}}\right)}$

其中，g v，g vc∈Rk分别是G发生器顶点v和vc的k维表示向量，θg是所有gv的并集，在此设置下，为了更新每次迭代中的θg，我们根据式（5）计算出近似连通性分布g（v vc；θg），根据g随机抽取一组样本（v，vc），并且通过随机梯度下降更新θg。SoftMax为g中的连通性分布提供了一个简洁直观的定义，但它在图表示学习中有两个限制：1）等式（5）中SoftMax的计算涉及到图中的所有顶点，这意味着对于每个生成的样本v，我们需要计算梯度θg log g（v_vc；θg）并更新所有v证书。这在计算上效率很低，尤其是对于具有数百万顶点的实际大型图。2）图结构编码了顶点之间丰富的邻近信息，但是SoftMax完全忽略了图中结构信息的利用，因为它对顶点没有任何区别。最近，Hierarchical Softmax（Morin和Bengio，2005）和Negative sampling（Mikolov等人2013）是SoftMax的热门替代产品。虽然这些方法在一定程度上减轻了计算量，但都没有考虑到图形的结构信息，因此在应用于图形表示学习时无法达到令人满意的效果。

### 图的softmax发生器

为了解决上述问题，在graphgan中，我们提出了一种新的替代SoftMax的生成器graph SoftMax。图Softmax的关键思想是定义一种新的计算G发生器（·vc；θg）中连通性分布的方法，该方法满足以下三个理想特性：

- 标准化。发电机应产生有效的概率分布，即v=vc g（v_vc；θg）=1。
- 图形结构感知。生成器应该利用图的结构信息来近似真实的连通性分布。直观地说，对于图中的两个顶点，它们的连通概率应该随着最短距离的增加而下降。
- 计算效率高。G（v vc；θg）的计算与全软最大值的计算不同，只需在图中包含少量顶点。

我们将详细讨论图SoftMax，如下所示。为了计算连通性分布g（·vc；θg），我们首先从顶点vc开始对原始图g执行广度优先搜索（bfs），这为我们提供了一个基于vc的bfs树tc。给定tc，我们将nc（v）表示为tc中v的邻域集（即直接连接到v的顶点），包括其父顶点和所有子顶点（如果存在）。对于一个给定的顶点v及其邻域vi∈nc（v），我们将vi给定v的关联概率定义为

$p_{c}\left(v_{i} | v\right)=\frac{\exp \left(\mathbf{g}_{v_{i}}^{\top} \mathbf{g}_{v}\right)}{\sum_{v_{j} \in \mathcal{N}_{c}(v)} \exp \left(\mathbf{g}_{v_{j}}^{\top} \mathbf{g}_{v}\right)}$

它实际上是一个超过nc（v）的softmax函数。要计算g（v_vc；θg），请注意，每个顶点v可以通过tc中根vc的唯一路径到达。表示路径为pvc→v=（vr0，vr1，…，vrm），其中vr0=vc和vrm=v。
然后，图Softmax定义g（v_vc；θg）如下：

$G\left(v | v_{c}\right) \triangleq\left(\prod_{j=1}^{m} p_{c}\left(v_{r_{j}} | v_{r_{j-1}}\right)\right) \cdot p_{c}\left(v_{r_{m-1}} | v_{r_{m}}\right)$

式中，pc（····）是等式（6）中定义的相关概率。我们证明了所提出的图SoftMax满足上述三个性质，即图SoftMax是规范化的、图结构感知的、计算效率高。

定理1.$\sum_{v \neq v_{c}} G\left(v | v_{c} ; \theta_{G}\right)=1$ in graph softmax.