# 一个关于网络嵌入的教程

$Haochen Chen^1, Bryan Perozzi^2, Rami Al-Rfou^2, and Steven Skiena^1$

$2018年10月19日$

## 摘要

网络嵌入方法旨在学习网络中节点的低维潜在表示。这些表示可以用作图形上各种任务的特性，例如分类、聚类、链接预测和可视化。在这个调查中，我们通过总结和分类这个研究领域的最新进展，对网络嵌入进行了概述。首先讨论了网络嵌入的理想性质，简要介绍了网络嵌入算法的发展历史。然后，我们讨论了在不同场景下的网络嵌入方法，如监督学习和非监督学习、同构网络的学习嵌入和异构网络的学习嵌入等。我们进一步展示了网络嵌入的应用，并对该领域的未来工作进行了总结。

## 1 简介

从社交网络到万维网，网络提供了一种无处不在的方式来组织各种真实世界的信息。给定网络结构，预测与图中的每个节点相关联的缺失信息(通常称为属性或标签)通常是可取的。这些缺失的信息可以代表数据的许多方面，例如，在社交网络上，它们可能代表一个人所属的社区，或者web上文档内容的类别。

由于信息网络可以包含数十亿的节点和边缘，因此在整个网络上执行复杂的推理过程是非常棘手的。为了解决这个问题，人们提出了一种技术——网络嵌入。中心思想是找到一个映射函数，将网络中的每个节点转换为低维的潜在表示形式。然后，这些表示可以用作图形上常见任务的特性，如分类、聚类、链接预测和可视化。

综上所述，我们寻求学习具有以下特点的网络嵌入：

- 适应性——真实的网络在不断进化;新的应用程序不应该要求再次重复学习过程。
- 可伸缩性——真实的网络本质上通常很大，因此网络嵌入算法应该能够在短时间内处理大规模的网络。
- 社区意识——潜在表示之间的距离应该代表一个度量标准来评估网络中相应成员之间的相似性。这允许在同质网络中进行泛化。
- 低维度——当标记数据稀缺时，低维度模型可以更好地推广，并加快收敛和推理。
- 持续——我们需要潜在的表示来模拟连续空间中的部分社区成员。除了提供关于社区成员的细致入微的观点外，持续的表示在社区之间有平滑的决策边界，这允许更健壮的分类。

> ![1542809210324](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542809210324.png)
>
> 图1:网络嵌入方法学习R d中网络中节点的潜在表示。这种学习的表示法编码了社区结构，因此很容易被标准的分类方法所利用。这里，在Zachary s空手道网络[60]上使用DeepWalk[33]生成R 2中的潜在表示。注意输入图中的社区结构与嵌入之间的对应关系。顶点颜色表示输入图的基于模块化的聚类。

作为一个激动人心的例子，我们展示了将DeepWalk[33](一种广泛使用的网络嵌入方法)应用到研究得很好的空手道网络的结果，如图1所示。如图1所示，这个网络通常由强制定向布局表示。图1b显示了具有两个潜在维度的DeepWalk的输出。除了惊人的相似性外，请注意(1b)的线性可分离部分对应于输入图(1a)中通过模块化最大化找到的集群(以顶点颜色表示)。

其余的调查组织如下。我们首先提供网络嵌入的一般概述，并给出一些定义和符号，这些将在以后使用。在第2节中，我们介绍了没有属性的同构网络上的无监督网络嵌入方法。第3节回顾了在带属性网络和部分标记网络上的嵌入方法。然后，第4节讨论了异构网络嵌入算法。本文进一步阐述了网络嵌入的应用，并对今后的工作进行了总结。

### 1.1 网络嵌入的简史

传统上，图形嵌入是在降维上下文中描述的。经典的降维技术包括主成分分析(PCA)[51]和多维标度(MDS)[25]。这两种方法都试图将一个$n\times m$矩阵m表示为一个$n \times k$矩阵，其中k << n.对于图，m通常是一个$n\times $矩n阵，其中可以是邻接矩阵、归一化拉普拉斯矩阵或全对最短路径矩阵等。这两种方法都能够捕获线性结构信息，但无法发现输入数据中的非线性。

**PCA -** PCA计算一组正交主分量，其中每个主分量是原始变量的线性组合。这些分量的数量可以等于或小于m，这就是PCA可以作为降维技术的原因。主分量计算完成后，每个原始数据点都可以投影到由其确定的低维空间中。对于一个方阵，PCA的时间复杂度是$O(n^3)$ 。

**MDS -** 多维标度(MDS)将M的每一行投影到一个k维向量上，使原始特征矩阵M中不同对象之间的距离在k维空间中保持得最好。具体地说,让$y \in R^k $对象的坐标在嵌入空间中,度量MDS最小化以下应力函数:

$$ Stress(y_1, y_2, · · · , y_n) =(\sum\limits_{i,j=1,2...,n}(M_{ij}-||y_i-y_j||^2))^{\frac{1}{2}}$$

精确的MDS计算需要对M的变换进行特征分解，这需要$O(n^3)$时间。

在本世纪初，其他方法如ISOMAP[44]和局部线性嵌入(LLE)[39]被提出来保持非线性流形的整体结构。我们注意到，对于任何类型的数据集，都抽象地定义了这两种方法，并首先将数据点预处理为图，以获取局部邻域性能。

**Isomap -** ISOMAP[44]是MDS的扩展，目的是在输入数据的邻域图中保持测地距离。邻域图G是通过将每个节点i与比一定距离更近的节点连接起来，或者是与i的k近邻节点连接起来的邻域图G。然后，将经典的MDS应用于G中，将数据点映射到一个低维流形，在G中保持测地线距离。

**局部线性嵌入(LLE) - **不像MDS(保持特征向量之间的两两距离)，LLE[39]仅利用数据点的局部邻域，不尝试估计远距离数据点之间的距离。LLE假设输入数据本质上是从一个潜在的流形中抽取的，并且一个数据点可以从它的邻居的线性组合中重建。重构误差可以定义为:

$$E(W)=\sum|x_i-\sum\limits_jW_{ij}X_j|^2$$

其中W是表示数据点 $j's$对$i's$重构的贡献的权重矩阵，通过最小化上述损失函数计算得到。由于$W_{ij}$反映了输入数据的不变几何性质，因此它可以用来查找从数据点$x_i$到低维表示$y_i$的映射。为了计算$y_i $, LLE最小化了以下嵌入成本函数

$$Φ(Y)=\sum|y_i-\sum\limits_jW_{ij}Y_j|^2$$

由于$W_{ij}$是固定的，可以证明这个代价函数可以通过寻找辅助矩阵的特征向量来最小化。

一般来说，这些方法都能在小型网络上提供良好的性能。然而，这些方法的时间复杂度至少是二次型的，这使得它们不可能在大型网络上运行。 

另一类常用的降维技术是利用从图中导出的矩阵的光谱特性(例如特征向量)来嵌入图的节点。Laplacian特征映射(LE)[3]，用与其k-最小非平凡特征值相关的特征向量表示图中的每个节点。Laplacian对图的光谱性质进行编码，对图的切割信息进行编码，并在图分析[15]方面有丰富的应用历史。设$W_{ij}$为节点i与j之间连接的权重，则可构造对角权矩阵D：$$D_{ii}=\sum\limits_{j}W_{ji}$$

拉普拉斯矩阵M是$L=D-W$

特征向量问题的解：$Lf = λDf $

可作为输入图的低维嵌入。

Tang和Liu[43]使用Laplacian图的特征向量进行社交网络分类。他们认为网络中的节点(参与者)与不同的潜在从属关系相关联。另一方面，这些社会维度也应该是连续的，因为行动者可能有不同程度的关联到一个从属关系。另一种类似的方法是SocDim[42]，它利用模块化矩阵的光谱特性作为网络中的潜在社会维度。然而，这些方法的性能已经被证明低于基于神经网络的方法[33]，我们稍后将对此进行讨论。

### 1.2 深度学习时代

DeepWalk[33]是第一个使用表示学习(或深度学习)社区技术的网络嵌入方法。DeepWalk将节点作为单词，生成短的随机漫步作为句子，在网络嵌入和单词嵌入之间架起了一座桥梁。然后，将Skip-gram[29]等神经语言模型应用到这些随机游动中，获得网络嵌入。从那以后，基于几个原因，DeepWalk可以说是最流行的网络嵌入方法。

首先，随机游动可以根据需要产生。由于Skip-gram模型也根据样本进行了优化，因此random walk和Skip-gram的结合使DeepWalk成为了一种在线算法。其次，DeepWalk是可扩展的。生成随机游动和优化Skip-gram模型的过程都是有效的，并行性很好。最重要的是，DeepWalk引入了图形深度学习的范例，如图3所示。

DeepWalk范例的第一部分是选择一个与输入图相关的矩阵，其中DeepWalk选择了random walk转移矩阵。实际上，还有许多其他的选择也被证明是可行的，例如归一化拉普拉斯矩阵和邻接矩阵的幂。

> ![1542853479584](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542853479584.png)
>
> 图2:使用深度行走和几种早期降维技术对空手道图形进行二维嵌入。输入为DeepWalk和SVD的邻接矩阵，其他四种方法的测地线矩阵。

第二步是图抽样，其中节点序列从所选矩阵隐式抽样。注意，这个步骤是可选的。一些网络嵌入算法直接计算精确的矩阵元素并在其上建立嵌入模型。然而，在很多情况下，由于以下两个原因，图抽样是一个有利的中间步骤。首先，取决于选择的矩阵，它可以花费二次时间来计算它的精确元素;一个例子是计算邻接矩阵的幂级数。在这个场景中，图形抽样作为一种可扩展的方法来近似矩阵。其次，与难以建模的大规模稀疏图相比，符号序列更容易被深度学习模型处理。序列建模有很多容易获得的深度学习方法，如RNNs和CNNs。深度遍历通过截断的随机遍历生成序列样本，有效地扩展了图节点的邻域。

第三步是从生成的序列(或第一步中的矩阵)中学习节点嵌入。在这里，DeepWalk采用Skip-gram作为学习节点嵌入的模型，这是学习单词嵌入最高效的算法之一。

DeepWalk范式非常灵活，可以通过两种可能的方式进行扩展：

1. 所建模的图形的复杂性可以扩展。例如，HOPE[31]旨在嵌入有向图，sin[49]和SNE[59]是嵌入有符号网络的方法。相比之下，[55,40,61,12,50]的方法被设计用于属性化网络嵌入。最近的许多工作[7,64,27,22,53]也试图嵌入异构网络。除了这些非监督方法外，网络嵌入算法[46,57,24]已经被提出用于图上的半监督学习。我们将在第3节和第4节详细讨论这些方法。
2. DeepWalk的两个关键组成部分，即从潜在矩阵中采样序列和从采样序列中学习节点嵌入，所使用的方法的复杂性可以得到扩展。许多网络嵌入工作是DeepWalk基本框架的扩展。例如，[41,21,34,5]提出了新的序列采样策略，而[48,6,5]提出了新的建模采样序列的状态。这些方法将在第2节中进一步分析。

> ![1542853688537](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542853688537.png)图3：对图进行深度学习的范例-DeepWalk对每个构建块的设计选择

###1.3 符号和定义

在这里，我们将介绍我们将在整个调查中使用的某些概念的定义.

- **定义1(图)：**一个简单的无向图G = (V, E)是n个顶点v1, v2，···，vn的集合V和一组边E，它们是顶点的无序对。换句话说，无向图中的边没有方向。G的邻接矩阵A是一个n - n矩阵，如果vi和vj之间有一条边Aij = 1，否则Aij = 0。除非另有说明，我们使用图和网络来表示一个简单的无向图。
- **定义2(网络嵌入)：**对于一个给定的网络,网络嵌入是一个映射函数Φ:V→7 R | |×d,d V | |。这种映射Φ定义了潜在表示每个节点(或嵌入)v∈v。同样,我们使用Φ(v)来表示节点的嵌入向量v。
- **定义3(有向图)：**有向图G = (V, E)是n个顶点v1, v2，···，vn的集合V和一组边E，它们是顶点的有序对。有向图和无向图的唯一区别是有向图中的边是有向的。
- **定义4(异构网络)：**异构网络是具有多种节点类型或多种边缘类型的网络G = (V, E)。形式上，G与节点类型映射fv: v O、v v和边缘类型映射fe: eq、e e相关联，其中O是所有节点类型的集合，Q是所有边缘类型的集合。
- **定义5(签名图)**签名图是这样一个图，其中每个边e都与权重w(e){1,1}相关联。权值为1的边表示节点之间的正链接，权值为-1的边表示负链接。签名图可以用来反映协议或信任。

##2 无监督网络嵌入

在本节中，我们介绍了在简单的无向网络上的网络嵌入方法。我们首先对现有的方法进行分类，然后在每个分类中引入几个有代表性的方法。

最近的可扩展网络嵌入算法受到了神经语言模型[4]和word嵌入的启发，特别是[29,30,32]。Skip-gram[29]是一种高效的单词嵌入学习方法。其核心思想是学习在句子中预测临近词的嵌入。附近的单词C(wi)(或上下文的话)一个句子中的某些词wi w1,w2,···,wT通常定义为一组词,在一个预定义的窗口大小k,即wi k···,wi 1,wi + 1,···,wi + k。具体来说,Skip-gram模型最小化以下目的:

$$ J=-\sum\limits_{u∈C(w)}log Pr(u|w)$$                                (7)

$Pr(u|w)$是使用分层或抽样的Softmax函数来计算的。

$$ \Large Pr(u|w)=\frac{exp(Φ(w) · Φ`(u))}{\sum_{u∈W} exp(Φ(w) · Φ`(u))} $$

这里，$Φ`(u)$是u作为上下文词时的分布式表示，W是词汇表的大小。

综上所述，跳格模型包括两个阶段。第一阶段确定每个句子中每个单词的上下文单词，而第二阶段最大化观察给定中心单词的上下文单词的条件概率。

通过捕捉语言建模与网络建模的内在相似性，DeepWalk[33]提出了一种学习网络嵌入的两阶段算法。DeepWalk的类比是，网络中的节点可以被认为是人工语言中的单词。与学习单词嵌入的Skip-gram模型类似，DeepWalk的第一步是识别每个节点的上下文节点。通过生成截断随机漫步在网络(类似于句子),v v的上下文节点可以被定义为节点的集合在一个窗口大小k在每个随机游走序列,它可以被看作是一个组合的节点从v年代1-hop,2-hop,k-hop邻居。换句话说，DeepWalk从A、A2、A3、···Ak的组合中学习网络嵌入，其中Ai是邻接矩阵的i次幂。一旦确定了上下文节点，第二步与原来的Skip-gram模型相同:学习嵌入，这可以最大化预测上下文节点的可能性。DeepWalk使用了与Skip-gram相同的优化目标和优化方法，但原则上也可以使用任何其他语言模型。

算法1中的第3-9行显示了DeepWalk的核心。外循环指定的次数,γ的随机漫步在每个节点。我们可以将每次迭代看作是对数据进行传递，在此传递期间对每个节点进行一次遍历。在每次遍历的开始，DeepWalk生成一个随机顺序遍历顶点。

在内部循环中，DeepWalk遍历图的所有顶点。对于每个节点$v_i$，生成一个random walk $|W_{v_i} |= t$，然后用于更新网络嵌入(第7行)，选择Skip-gram算法作为更新节点表示的方法。

随后关于图形嵌入的大部分工作都遵循了DeepWalk中提出的两阶段框架，两个阶段都有变化。表1总结了几种网络嵌入方法按照不同的上下文节点的定义和分类学习嵌入的方法:

![1542855838835](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542855816862.png)

> ![1542855936942](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542855936942.png)
>
> 表1：按上下文节点来源分类的无监督网络嵌入方法和表示学习方法

LINE[41]采用广度优先的搜索策略来生成上下文节点:只有距离给定节点最多两跳的节点才被认为是其相邻节点。此外，它使用负采样[30]来优化Skip-gram模型，与DeepWalk中使用的softmax[29]不同。

Node2vec[21]是DeepWalk的扩展，它引入了一种将BFS风格和DFS风格的社区探索相结合的有偏随机步行过程。

walklet[34]表明DeepWalk通过a、A2、···、Ak的加权组合学习网络嵌入。特别是，如果i < j, DeepWalk总是比Aj更偏向Ai。为了避免上述缺点，Walklets提出从A, A2，···，Ak的每个部分学习多尺度网络嵌入。由于计算人工智能的时间复杂度至少是网络中节点数量的二次元，所以walklet通过跳过短随机遍历节点来近似计算人工智能。进一步学习了A的不同幂的网络嵌入，在不同的粒度上捕获网络的结构信息。

GraRep[5]同样利用不同尺度下的节点共现信息，将图邻接矩阵提升到不同的幂。将奇异值分解(SVD)应用于邻接矩阵的幂，得到节点的低维表示。walklet和GraRep有两个主要的区别。首先，GraRep计算Ai的确切内容，而walklet则近似计算。其次，GraRep采用SVD获得精确分解的节点嵌入，而Walklets采用的是Skip-gram模型。有趣的是，Levy和Goldberg[26]证明了带有负抽样(SGNS)的skip-gram隐式分解了节点和各自上下文节点之间的PMI矩阵。总之，GraRep使用一个噪音更小的进程生成网络嵌入，但是walklet被证明具有更大的可伸缩性。

> ![1542856440222](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542856440222.png)图4:根据所选择的表示的规模，可以对非常不同的信息进行编码。如图4a、4b和4c所示，是Cora网络中v35到其他顶点的距离分布，在不同的网络表示尺度上。较粗的表示(如A5)会使分布变平，使更大的社区靠近源顶点。图4d、4e、4f分别为Cora网络中顶点v35(箭头所示)的余弦距离对应的heatmap，通过一系列依次较粗的表示。附近的顶点是红色的，远处的顶点是蓝色的。

目前讨论的模型依赖于一些手动选择的参数来控制图中每个节点的上下文节点的分布。对于DeepWalk，窗口大小w决定上下文节点。此外，所使用的Skip-gram模型还隐藏了超参数，这些超参数根据示例在上下文中所占的位置来决定示例的重要性。对于walklet和GraRep，应该事先确定图邻接矩阵的幂。选择这些超参数是非常重要的，因为它们将显著影响网络嵌入算法的性能。

GraphAttention[2]提出了一个学习多尺度表示的注意模型，该模型能最好地预测原始图中的链接。不再预先确定超参数来控制上下文节点分布，GraphAttention自动学习对图转移矩阵的powerseries的关注。形式上，设D R |V | |V |为随机游动的共现矩阵，P(0)为初始随机游动起始位置矩阵。GraphAttention参数化的期望与概率分布Q = D (Q1、Q2,···, QC):

$$ E[D|Q_1,Q_2,...Q_C]=\tilde P^{(0)}\sum\limits_{k=1}^C Q_k( \mathcal T )^{k}$$ (9)

这种概率分布可以通过数据本身的反向传播来学习，例如将其建模为带有参数的softmax层(q1，…qk)

![1542857232601](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542857232601.png)

这使得每个图都可以学习它自己的分布Q，它具有预定的稀疏性和衰减形式。

深度学习方法的表现力使其适合于嵌入网络。SDNE[48]学习使用深度自动编码器保持两跳邻居之间的接近性的节点表示。通过最小化相邻节点之间的欧氏距离，进一步保持了相邻节点之间的接近性。DNGR[6]是另一种基于深度神经网络的网络嵌入学习方法。他们采用随机的浏览策略来获取图形结构信息。他们进一步将这些结构信息转换成PPMI矩阵，并训练一个堆叠去噪自编码器(SDAE)嵌入节点。

所有这些论文都关注于嵌入简单的无向图。在下一节中，我们将介绍嵌入具有不同属性的图的方法，例如有向图和有符号图。

### 2.1 有向图嵌入

上一节讨论的图形嵌入是为了在无向网络上操作而设计的。然而，如[66]所示，通过使用有向随机游动作为网络的训练数据，它们可以自然地推广到有向图。最近还提出了几种用于有向图建模的方法。

HOPE [[31]是一种专门为有向图设计的图形嵌入方法。HOPE是一个非对称及物性保留图嵌入的通用框架，它包含了几个流行的接近度测量方法，比如Katz index、root PageRank和common neighbour作为特例。利用广义SVD有效地解决了HOPE的优化目标。

abul - el - haija等人[1]为每个节点提出了两个独立的表示，一个节点是源节点，另一个节点是目标节点。从这个意义上说，边缘嵌入可以简单地看作是源嵌入和目的嵌入的源的连接。这些边缘表示(在2.2节中进一步讨论)隐式地保留了图形的有向性。

### 2.2 边嵌入

像链接预测这样的任务需要对图形边缘进行精确建模。一个无监督的方法构造一个表示边缘e =(u,v)是应用二元运算符在$\phi(u)和\phi(v)$

$$\phi(u,v)=\phi(u) \circ \phi(v)$$

> ![1542858391005](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542858391005.png)
>
> 图5:[1]中边缘表示方法的描述。左边是一个图，以点红色显示了一个随机游走，其中节点u、v在这个游走中很近(即在一个可配置的上下文窗口参数中)。该方法访问节点的可训练嵌入Yu和Yv，并将它们作为输入输入到Deep Neural Network (DNN) f中。DNN分别为节点u和v输出流形坐标f(Yu)和f(Yv)。低秩不对称投影将f(Yu)和f(Yv)转换为它们的源和目的表示，g使用它们表示一条边。



在node2vec[21]中，考虑了平均、Hardmard积、L1距离和L2距离等几个二元算子。然而，这些对称二进制运算符总是为边(u, v)和(v, u)分配相同的表示，而忽略了边的方向。

为了缓解这个问题，abul - el - haija等人提出通过低秩不对称投影来学习边缘表示。他们的方法包括三个步骤。在第一步中，使用node2vec学习每一个u V的嵌入向量Yu R D。这时,一个款fθ:D R D是学会减少嵌入向量的维数。最后,为每个节点对(u, v),低秩不对称投影变换(Yu)和f(青年志愿)到相应的表示源和目的节点,和φ(u, v)被表示为:

$$\phi(u,v)=f(Y_u)^T × M × f(Y_v)$$

其中M是低秩投影矩阵。图5进一步说明了模型的体系结构。

### 2.3 有符号图嵌入

回想一下，在有符号图中，权值为1的边表示节点之间的正链接，而权值为-1的边表示负链接。

sin[49]是一种基于深度神经网络的有符号网络嵌入学习模型。基于结构平衡理论，节点应该更接近朋友(与正面边缘相连)而不是敌人(与负面边缘相连)。正弦通过最大化朋友嵌入相似度和敌人嵌入相似度之间的边界来保持这种特性。形式上，给定一个三元组p = (vi, vj, vk)， vi, vj, vk V，其中vi和vj有一个正链接，而vi和vk有一个负链接，下面的性质成立

> ![1542859029944](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542859029944.png)

f是一个两节点之间的相似性度量嵌入和δ是一个可调。然而，在真实的社交网络中，负面链接比正面链接要少得多。因此，对于网络中的许多节点来说，这样的三元组可能不存在，因为它们的2跳网络中只有正链路。为了解决这个问题，额外的虚拟节点v0以一个负链接连接到这些节点。同样，给定一个三元组p = (vi, vj, v0)，其中一个正连杆连接vi和vj，而一个负连杆连接vi和v0，我们有另一个目标函数:

![1542859113962](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542859113962.png)

通过共同最小化公式 13和公式 14来学习节点嵌入。

SNE[59]是一种用于有符号网络嵌入的对数双线性模型。SNE通过线性组合上下文节点的表示来预测目标节点的表示。为了捕获节点之间的有符号关系，将两个符号类型的向量合并到log-bilinear模型中。

### 2.4 子图嵌入

研究的另一个分支是嵌入更大比例的图形组件，例如图形子结构或整个图形。Yanardag和Vishwanathan[54]提出了deep graph kernel，这是一个用于在图中建模子结构相似性的通用框架。传统上，两个图G和g0之间的核是由![1542860920860](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542860920860.png)

其中h·，·ih表示RKHS H中的点积

已经开发了许多子结构来计算这个内核，例如图、子树和最短路径。然而，这些表示无法揭示不同但相似的子结构之间的相似性。也就是说，即使两个图只差一条边或一个节点，它们仍然被认为是完全不同的。这个核定义导致了对角优势问题:一个图只与它自己相似，而与其他图不同。为了克服这个问题，Yanardag和Vishwanathan[54]提出了一个可选的内核定义，如下所示![1542860989069](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1542860989069.png)

其中M为输入图中所有子结构对之间的相似矩阵。

为了构建M，他们的算法首先生成图子结构的共现矩阵。然后，在共生矩阵上训练Skip-gram模型，得到子结构的潜在表示，然后用于计算M。

### 2.5 改进网络嵌入的元策略

尽管神经方法在网络嵌入方面取得了成功，但迄今为止所有的方法都有几个共同的弱点。首先，它们都是局限于节点周围的结构的局部方法。DeepWalk和node2vec采用短随机漫步来探索节点的局部邻域，而LINE则关注更紧密的关系(节点最多两跳之间的距离)。这种对局部结构的关注隐式地忽略了长距离的全局关系，所学习的表示可能无法揭示重要的全局结构模式。其次，它们都依赖于使用随机梯度下降法[29]求解的非凸优化目标，而[29]可能会陷入局部极小值(例如，可能由于初始化不佳)。换句话说，这些学习网络嵌入的技术可以偶然地学习嵌入配置，这些嵌入配置忽略了输入图的重要结构特征。

> ![1542861097756](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1542861097756.png)
>
> 图6:线条和竖琴二维嵌入的比较，两个不同的图形。观察竖琴的嵌入如何更好地保持了环形和平面的高阶结构。

为了解决这些问题，HARP[11]提出了一个元策略来嵌入图数据集，以保留高阶结构特征。HARP递归地将原始图中的节点和边合并，得到一系列结构相似的连续较小的图。这些合并的图，每个都有不同的粒度，为我们提供了原始图的全局结构的视图。从最简化的形式开始，每个图形都用来学习一组初始表示，这些表示可以作为嵌入下一个更详细的图形的良好初始化。这个过程重复，直到我们在原始图中得到每个节点的嵌入。

HARP是一个通用的元策略，用于改进嵌入图形的所有最先进的神经算法，包括DeepWalk、LINE和Node2vec。图6展示了竖琴范式的有效性，通过可视化从线条的二维嵌入和对线条的改进，竖琴(线条)。我们所考虑的每个小图形都有一个明显的全局结构(一个环(6a)和一个网格(6d))，很容易通过一个力定向布局[23]暴露出来。中心图为环形(6b)和网格(6e)通过直线获得的二维嵌入。在这些嵌入中，全局结构丢失(即，环和平面无法识别)。然而，使用HARP改进直线(右)所产生的嵌入捕获了给定图的局部和全局结构(6c, 6f)。

## 3 属性化网络嵌入

以上讨论的方法仅利用网络结构信息来获得网络嵌入。然而，现实网络中的节点和边缘通常与其他特性相关联，这些特性称为属性。例如，在Twitter等社交网络站点中，用户(节点)发布的文本内容是可用的。因此，网络嵌入方法也应该借鉴节点属性和边缘属性的丰富内容。在下面的讨论中，我们假设属性只与节点相关联，因为大多数现有的工作都集中于利用节点属性。针对不同类型的属性提出了不同的策略。特别是，研究人员对两类属性感兴趣:高级特性(如文本或图像)和节点标签。

这些高阶特征通常是节点的高维稀疏特征，因此通常使用无监督文本嵌入或图像嵌入模型将这些稀疏特征转换为密集嵌入特征。一旦了解了嵌入特性，主要的挑战是如何将它们合并到现有的网络嵌入框架中。

TADW[55]研究节点与文本特性相关联的情况。TADW的作者首先证明DeepWalk本质上都是一个转移概率矩阵M∈V R | |×V | |成两个低维矩阵W∈R d×V | |和H∈R d×| V V d < < | | |。灵感来自于这个结果,TADW包含了文本特征矩阵T∈R英尺×V | |进入矩阵分解过程,通过分解M到W的产物,H和T .最后,W和H×T的节点连接的潜在的表示。

另一种思路是联合建模网络结构和节点特征。直观上，除了在同一邻域内强制节点间嵌入相似度外，我们还应该在特征向量相似的节点间强制嵌入相似度。CENE[40]是一种网络嵌入方法，它联合建模节点中的网络结构和文本内容。CENE将文本内容视为一种特殊类型的节点，并利用节点链接和节点嵌入链接。优化的目标是将这两种类型的链路的损失降至最低。HSCA[61]是一种属性图的网络嵌入方法，它同时对同质图、网络拓扑结构和节点特征进行建模。

除了文本属性，节点标签是另一种重要的属性类型。在引文网络中，与论文相关的标签可能是论文发表的地点或发表年份。在社交网络中，人们的标签可能是他们所属的群体。结合标签信息的一种典型方法是联合优化产生节点嵌入和预测节点标签的损失。GENE[12]考虑与节点相关联的组信息的情况。GENE遵循了DeepWalk的思想，但它并不是仅预测随机行走序列中的上下文节点，而是作为优化目标的一部分预测上下文节点的组信息。Wang等人提出了一种基于模块化非负矩阵因子分解的网络嵌入方法，该方法保留了网络中的社区结构。在节点层次上，模型保留了节点间的一阶和二阶近似，并进行了矩阵分解;在社区层面，在矩阵分解过程中应用模块化约束项进行社区检测。

在现实网络中，节点标签仅对部分节点可用也是很常见的。在这种情况下，针对节点标签和网络结构的联合学习，开发了半监督网络嵌入方法。Planetoid[57]是一种半监督网络嵌入方法，通过联合预测图中每个节点的标签和上下文节点来学习节点表示。它可以在感应和转导的情况下工作。Maxmargin DeepWalk (MMDW)[46]是一种半监督的方法，它在部分标记的网络中学习节点表示。MMDW由两部分组成:第一部分是基于矩阵分解的节点嵌入模型，第二部分将学习到的表示作为特征，在标记的节点上训练一个最大边距SVM分类器。通过引入偏梯度，两部分参数可以共同更新。

## 4 异构网络嵌入

回想一下异构网络有多个节点或边类。为了对不同类型的节点和边缘进行建模，下面我们介绍的大多数网络嵌入方法都通过联合最小化每种模式的损失来学习节点嵌入。这些方法要么直接学习同一潜在空间中的所有节点嵌入，要么预先构造每种模式的嵌入，然后将它们映射到同一潜在空间。

Chang等人提出了异构网络的深层嵌入框架。他们的模型首先为每个模态(如图像、文本)构造特征表示，然后将不同模态的嵌入映射到相同的嵌入空间。优化目标是最大化链接节点嵌入之间的相似性，同时最小化非链接节点嵌入的相似性。注意，两个节点之间的边可以是相同的模态，也可以是来自不同模态的节点。

Zhao等[64]是在异构网络中构造节点表示的另一种框架。具体来说，他们认为维基百科有三种类型的节点:实体、词汇和类别。构建了相同类型节点与不同类型节点之间的共现矩阵，并利用坐标矩阵分解的方法从所有矩阵中联合学习实体、词和类别的表示。

Li等人提出了一种学习异质社交网络中用户表示的神经网络模型。他们的方法联合建模用户生成的文本、用户网络和用户与用户属性之间的多层关系。

HEBE[22]是一种用于嵌入大规模异构事件网络的算法，其中事件被定义为网络中一组节点(可能是不同类型的)之间的交互。之前的工作将事件分解为事件中涉及的每对节点之间的两两交互，而HEBE则将整个事件视为超边缘，并同时保持所有参与节点之间的接近性。具体来说，对于超边缘中的每个节点，HEBE将其视为目标节点，而超边缘中的其余节点视为上下文节点。因此，潜在的优化目标是预测给定所有上下文节点的目标节点。

EOE[53]是一种用于耦合异构网络的网络嵌入方法，其中两个同构网络与网络间的边缘相连接。EOE学习了两种网络的潜在节点表示，利用一个和谐的嵌入矩阵将不同网络的表示转化为相同的空间。

除了联合建模异构节点和边缘外，另一个很有前途的工作方向是扩展随机游动并将学习方法嵌入到异构场景中。Metapath2vec[17]是DeepWalk的扩展，该扩展适用于异构网络。为构造随机游动，metapath2vec使用了基于元路径的游动，它捕获了不同类型节点之间的关系。为了从随机游走序列中学习表示，他们提出了在模型优化过程中考虑节点类型信息的异构跳格。

## 5 网络嵌入的应用

网络嵌入在实践中得到了广泛的应用，因为它易于将邻接数据转化为可操作的特性。在这里，我们回顾了几种典型的网络嵌入应用程序，以演示如何使用它们.

### 5.1 知识表示

知识表示问题涉及到使用由主语、谓词和对象组成的短句子(或元组)来编码关于世界的事实。虽然它可以被严格地看作是一个异构网络，它足够是一个重要的应用领域在这里提到的:

- GenVector[58]研究的是学习社交知识图的问题，目标是将在线社交网络与知识库连接起来。他们的多模态贝叶斯嵌入模型利用DeepWalk在社交网络中生成用户表示。
- RDF2Vec[38]是一种在资源描述框架(RDF)图中学习潜在实体表示的方法。RDF2Vec首先将RDF图转换为图随机游动序列和weisfeiller - lehman图核序列，然后在序列上采用CBOW和Skip-gram模型构建实体表示。

### 5.2 推荐系统

另一个工作分支试图将网络嵌入合并到推荐系统中。用户、用户查询和项之间的交互自然形成了一个异构网络，它编码了用户对项的潜在偏好。在这种交互图上嵌入网络可以作为推荐系统的增强。

- Chen et al.[8]利用社交聆听图来增强音乐推荐模型。他们利用DeepWalk学习社会倾听图中的潜在节点表示，并将这些潜在表示合并到分解机器中。
- Chen等人[9]提出了异构偏好嵌入，将用户偏好和查询意图嵌入到低维向量空间中。在用户首选项嵌入和查询嵌入都可用的情况下，可以根据项目和查询之间的相似性提出建议。

### 5.3 自然语言处理

最先进的网络嵌入方法主要是受自然语言处理领域的进展启发，尤其是神经语言模型。同时，网络嵌入方法也能更好地模拟人类语言。

- PLE[37]研究实体类型中标签降噪的问题。它们的模型共同学习了同一特征空间中实体提及、文本特征和实体类型的表示。这些表示进一步用于估计每个训练示例的类型路径。
- CANE b0是一个上下文感知的网络嵌入框架。他们认为，一个节点在与不同的邻居交互时可能会表现出不同的特性，因此它对这些邻居的嵌入应该是不同的。甘蔗通过使用相互注意机制来实现这一目标。
- Fang等人[18]提出了一个基于社区的问答(cQA)框架，该框架利用了社区内的社会互动来进行更好的问答匹配。他们的框架将用户、问题和答案以及它们之间的交互视为一个异构网络，并在网络中的随机游走中训练一个深层神经网络。
- Zhao等[65]研究了基于社区的问答(cQA)站点的专家查找问题。他们的方法在DeepWalk中采用了随机游走的方法，将用户之间的社会关系嵌入到RNNs中，建模用户对问题的相对质量等级。

### 5.4 社交网络分析

社交网络在现实世界中普遍存在，网络嵌入方法在社交网络分析中越来越流行也就不足为奇了。对于广泛的应用程序来说，在社交网络上嵌入网络已经被证明是功能强大的特性，从而提高了许多下游任务的性能。

- Perozzi et al.[35]研究了预测社交网络用户确切年龄的问题。他们使用DeepWalk学习社交网络中的用户表示，并对这些用户表示进行线性回归来预测年龄。
- Yang等人[56]提出了一种神经网络模型，用于同时建模社交网络和移动轨迹。他们采用DeepWalk在社交网络中生成节点嵌入，使用RNN和GRU模型生成移动轨迹。
- Dallmann等人的[16]研究表明，通过学习维基百科链接网络和维基百科点击流网络中的维基页面表示，与基于计数的维基百科网络方法相比，他们可以获得更高质量的概念嵌入。
- Liu等人[28]提出了输入-输出网络嵌入(Input-output Network embed, IONE)，即利用网络嵌入在不同的社交网络中对齐用户。IONE通过在一个公共嵌入空间中保持与拥有相似追随者和追随者的用户的接近来实现这一点。
- Chen和Skiena[14]展示了网络嵌入方法在测量历史人物相似性方面的有效性。他们通过维基百科页面之间的链接构建了一个历史人物之间的网络，并使用DeepWalk获得历史人物的矢量表示。结果表明，历史人物的深度行走表现之间的相似性可以作为一种有效的象样相似性度量方法。
- DeepBrowse[10]是一种在没有预定义层次结构的情况下浏览大型列表的方法。DeepBrowse定义为两个固定的、全球定义的物体空间排列组合:一个是根据相似度排序，第二个是根据大小或重要性排序。通过使用在对象的交互图上生成的深度遍历嵌入来计算项目之间的相似性。
- TransNet[47]是一种基于翻译的网络嵌入模型，利用图边中丰富的语义信息对边进行关系预测。TransNet将节点间的交互视为转换操作，并进一步使用深层自动编码器来构造边缘表示。

### 5.5 其他应用

- Geng等人[19]和Zhang等人[62]开发了深度神经网络模型，该模型从用户图像共生网络中学习用户和图像的分布式表示。网络中的表示学习过程类似于DeepWalk[33]，只是在优化过程中加入了DCNN提取的图像特征。
- Wu等人[52]将图像搜索引擎中从用户搜索行为中收集的点击数据视为异类图。单击图中的节点是文本查询和作为搜索结果返回的图像，而边缘表示给定搜索查询的图像的单击计数。通过提出一种基于截断随机游动的神经网络模型，他们的方法学习了文本和图像的多模态表示，从而提高了对不可见查询或图像的跨模态检索性能。
- Zhang等[63]将DeepWalk应用到大规模的社交图像标签集合中，在统一的嵌入空间中学习图像特征和单词特征。

这些应用只是冰山一角。网络嵌入的未来似乎很光明，随着深度学习的普及和重要性的不断增长，新的算法方法产生更好的嵌入，以满足日益复杂的神经网络。

## 6 结论及未来方向

网络嵌入是一个令人兴奋和快速增长的研究领域，它吸引了来自不同社区的研究人员，特别是数据挖掘、机器学习和自然语言处理。虽然大多数工作关注的是网络嵌入的一般方法，但我们认为网络嵌入的应用研究更少。我们预计将有大量工作用于网络嵌入的其他应用，例如提高自然语言处理和信息检索模型的性能、挖掘生物网络和社交网络等等。

此外，对于具有不同性质和来自不同领域的图也做了大量工作。从图的性质出发，提出了有向图、有符号图、异构图和带属性图的各种方法。在应用领域方面，网络嵌入方法被广泛应用于各种图形，包括知识图形、生物图形和社交网络。然而，通过利用这些图表的独特特性，在这方面无疑可以做更多的工作。

### 6.1 寻找合适的上下文

受DeepWalk中提出的两阶段网络嵌入学习框架的启发，我们提出了各种搜索合适上下文的策略，如表1所示。然而，大多数策略都依赖于对所有网络都相同的上下文节点的严格定义，这是不可取的。

在这样的背景下，在一个通用框架下统一不同的网络嵌入有很多努力[13,36]。gemd[13]将图像嵌入算法分解为三个构件:节点邻近函数、翘曲函数和损失函数。结果表明，拉普拉斯特征向量、深度步长、直线和node2vec等算法都可以在此框架下统一。通过在现实世界的图形上测试每个构建块的不同设计选择，他们选择了经验上最有效的三种方法:有限阶跃转移矩阵、指数翘曲函数和翘曲弗鲁比尼乌斯规范损失的组合。然而，这样的设计决策纯粹是基于对有限数量的网络的经验性能模型做出的，这可能并不适用于所有的网络。

一个很有前途的方法是最近在GraphAttention[2]中提出的注意模型。通过将转移矩阵的幂级数上的注意参数化，GraphAttention自动学习不同网络的不同注意参数。

### 6.2 改进的损耗/优化模型

神经嵌入方法的另一个问题是它们依赖于一般的损失函数和优化模型，如跳格。这些优化目标和模型没有针对任何特定任务进行调优。因此，虽然已经证明了所学习的网络嵌入在各种任务(如节点分类和链接预测)上的竞争性能，但与专门为任务设计的端到端嵌入方法相比，它们并不是最优的。

因此，网络嵌入算法的另一个未来方向是设计特定任务的损失函数和优化模型。从这个角度来看，半监督网络嵌入方法可以看作是专门为节点分类任务而设计的。Abu-El-Haija et al.[1]做了另一项尝试，将图似然提出作为链接预测的新目标。给定一个训练图G = (V, Etrain)，其图似然被定义为所有节点对的边缘估计Q的乘积:

![1542862546351](F:\Machine-learning-and-data-science-notebook\images\一个关于网络嵌入的教程\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542862546351.png)

其中$Q: V\times V->[0,1]$是一个可训练的边缘估计量。

## 引用文献