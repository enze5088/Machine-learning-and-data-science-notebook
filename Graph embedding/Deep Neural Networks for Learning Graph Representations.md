# 学习图形表示的深层神经网络

摘要：本文提出了一种新的图形表示学习模型，该模型通过捕获图形结构信息，为每个顶点生成一个低维矢量表示。与以往的研究不同，我们采用随机冲浪模型直接捕捉图形结构信息，而不是使用基于采样的方法生成线性序列。（2014年）。我们的方法的优点将从理论和经验两个角度加以说明。我们还对Levy和Goldberg（2014）提出的矩阵因式分解方法提出了一个新的观点，其中，将点态互信息（PMI）矩阵作为Mikolov等人提出的带有负抽样的skipgram模型目标函数的解析解。（2013年）。然而，与使用SVD从PMI矩阵中寻找低维投影的方法不同，我们的模型中引入了堆叠去噪自动编码器，以提取复杂特征并对非线性进行建模。为了验证模型的有效性，我们利用学习的顶点表示作为特征，对聚类和可视化任务进行了实验。对不同规模数据集的实验结果表明，在这类任务中，我们的模型优于其他现有模型。

## 1.导言

在许多实际问题中，图是用于信息管理的常见表示。例如，在蛋白质网络研究中，从蛋白质-蛋白质相互作用网络中挖掘蛋白质复合物对描述同源蛋白质的功能多样性起着重要作用，并提供了有价值的进化见解，这本质上是一个聚类问题。因此，开发自动化算法从图形中提取有用的深层信息至关重要。组织与潜在的大而复杂的图相关的信息的有效方法之一是学习图表示法，该图表示法为图的每个顶点分配一个低维密集向量表示法，对图所传递的有意义的信息进行编码。

最近，人们对学习单词嵌入的工作产生了极大的兴趣（Bullinaria和Levy 2007；Mikolov等人2013年）。他们的目标是从大量的自然语言文本中，为每个自然语言单词学习基于上下文的低维向量表示。由于语料库是由自然语言的词序列组成的线性结构，因此可以将其视为线性序列的学习表征。由此产生的紧凑、低维向量表示被认为能够捕获丰富的语义信息，并被证明对各种自然语言处理任务有用。虽然已经确定了学习线性结构良好表示的有效方法（Mikolov等人2013年；Pennington、Socher和Manning，2014年），处理具有丰富拓扑结构的一般图形结构更为复杂。为此，一种自然的方法是找出有效的方法，将一般图结构的顶点表示学习任务转化为线性结构的表示学习任务。Perozzi等人提出的深水。（2014）提出了一种通过称为截断随机游走的统一采样方法将未加权图转换为线性序列集合的想法。在他们的方法中，采样的顶点序列描述了图中顶点之间的连接。这一步可以理解为将一个一般的图结构转换成一个大型线性结构集合的过程。接下来，他们利用了Mikolov等人提出的跳克模型。（2013）从这些线性结构中学习顶点的低维表示。研究表明，学习的顶点表示在一些任务中是有效的，优于以前的几种方法，如光谱聚类和模块化方法（Tang和Liu 2009a；2009b；2011；Macskassy和Provost 2003）。

虽然这种学习无权重图顶点表示的方法是有效的，但还有两个重要问题有待解决。首先，如何更准确、更直接地捕获加权图的图结构信息？其次，是否有更好的方法来表示线性结构的顶点？为了回答第一个问题，我们设计了一个适用于加权图的随机冲浪模型，它可以直接产生一个概率共现矩阵。这种矩阵类似于从图（Bullinaria和Levy 2007）中的线性序列采样得到的共现矩阵，但我们的方法不需要采样过程。为了回答第三十届AAAI人工智能会议（AAAI-16）第二次会议记录中的一个问题，我们首先回顾了一种流行的现有方法，该方法用于学习线性结构的顶点表示。Levy和Goldberg（2014）最近的一项研究表明，采用负抽样法优化与skipgram相关的目标函数（Mikolov等人2013年）与将词及其上下文的移位正点互信息（ppmi）矩阵（Bullinaria和Levy 2007）分解成因子有内在关系。具体来说，他们证明了可以使用标准奇异值分解（SVD）方法分解PPMI矩阵，从而从分解的矩阵中诱导顶点/单词表示。我们最近的方法Grarep（Cao、Lu和Xu，2015）在学习图形表示的任务上取得了良好的经验结果。然而，该方法采用支持向量机进行线性降维，而没有探索出更好的非线性降维技术。

在本文中，我们对Levy和Goldberg（2014）的工作给出了一个新的视角。我们认为原始的ppmi矩阵本身是图形的一个显式表示矩阵，而svd步骤本质上起着降维工具箱的作用。Levy等人认为，诱导最终词语表达的SVD步骤是有效的。（2015）还证明了使用ppmi矩阵本身作为单词表示的有效性。有趣的是，如作者所示，从SVD方法中获得的表示不能完全优于PPMI矩阵本身的表示（Levy、Goldberg和Dagan 2015）。由于我们的最终目标是学习好的顶点表示法，以便有效地捕获图形的信息，因此有必要研究从ppmi矩阵中恢复顶点表示法的更好方法，在ppmi矩阵中，不同垂直线之间可能存在复杂的非线性关系。S可以被捕获。

深入学习揭示了非线性复杂现象的建模路径，在语音识别等不同领域有着许多成功的应用（Dahl等人。2012）和计算机视觉（Krizevsky、Sutskever和Hinton 2012）。深层神经网络（dnn）是一种从低层次特征学习高层次抽象的有效方法，例如堆叠的自动编码器。这个过程实质上是执行尺寸缩减，将数据从高维空间映射到低维空间。与通过线性投影将原始表示空间映射到低阶新空间的（截断）支持向量降维方法不同，深度神经网络（如堆叠的自动编码器）可以学习高度非线性的投影。实际上，田等人最近的一项研究。（2014）在聚类任务中用稀疏自编码代替谱聚类的特征值分解步骤，取得了显著的改善。在他们工作的激励下，我们还研究了使用基于深度学习的替代方法从原始数据表示中学习低维表示的有效性。与他们的工作不同，我们的目标是学习一般图的顶点表示，而不是只关注集群任务。我们将深度学习方法应用于PPMI矩阵而不是拉普拉斯矩阵。在他们的模型中使用。前者有可能比后者产生更好的代表性（Perozzi、Al-Rfou和Skiena，2014年）。为了增强模型的鲁棒性，我们还使用了堆叠去噪自动编码器来学习多层表示。

我们称我们提出的模型为dngr（图表示的深层神经网络）。学习的表示可以被视为可以输入到其他任务中的输入特性，例如无监督的集群和有监督的分类任务。为了验证我们的模型的有效性，我们进行了实验，利用所学的表示方法处理一系列不同任务，其中考虑了不同类型和拓扑的现实网络。
为了证明该模型在考虑更简单、更大规模的实际图形结构时的有效性，我们将该算法应用于一个非常大的语言数据集，并对单词相似性任务进行了实验。在所有这些任务中，我们的模型优于其他学习图形表示的方法，而且我们的模型也非常可并行。我们的主要贡献是双重的：

- 从理论上讲，我们认为，深层神经网络具有能够捕获由图形传递的非线性信息的优点，而许多广泛使用的传统线性降维方法无法捕获此类信息。此外，我们认为我们的随机冲浪模型可以取代广泛使用的基于采样的图形信息采集方法。
- 根据经验，我们证明了我们的新模型能够更好地学习加权图的低维顶点表示，在这里可以捕获有意义的语义、关系和结构信息。我们表明，所得到的表示可以有效地用作不同下游任务的特征。

## 2.背景及相关工作

在这一部分中，我们首先提出了一个在深水中提出的无权重图的随机抽样，以说明将顶点表示转化为线性表示的可行性。接下来，我们考虑两个词的表示方法：带负采样的跳过图和基于ppmi矩阵的矩阵分解。这些方法可以看作是从线性结构数据中学习单词表示的线性方法。

表示法给出加权图g=v，e，其中v=v1，v2，…，vn是顶点集，e=ei，j是两个顶点之间的边集。在未加权的图中，边权重表示两个顶点之间是否存在关系，因此是二元的。相反，加权图中的边权是表示两个顶点之间关联度的实数。虽然加权图的边权可以是负的，但本文只考虑非负的边权。为了便于注释，本文还使用w和c来表示顶点。我们试图通过捕捉深层结构信息来获得代表图G顶点的矩阵R。

### 2.1 深水随机行走

深水是一种有效的方法，称为截断随机游动，将未加权的图结构信息转换为线性序列，表达图顶点之间的关系。本文提出的随机游动是一种适用于无权重图的均匀抽样方法。他们首先从图中随机选择一个顶点v1，并将其标记为当前顶点，然后从当前顶点v1的所有相邻点中随机选择下一个顶点v2。现在，他们将这个新选择的顶点v2标记为当前顶点，并重复这样一个顶点采样过程。当一个序列中的顶点数达到一个预先设定的值，称为行走长度η时，算法终止。重复上述步骤γ次（称为总行走）后，收集大量线性序列。

截断随机游动是用线性序列表达无权重图结构信息的一种有效方法，但该方法具有采样过程缓慢、超参数η和γ不易确定等特点。我们注意到，深水产生了一个基于采样线性序列的共现矩阵。在第3节中，我们描述了我们的随机冲浪模型，该模型直接从加权图构造概率共现矩阵，避免了昂贵的采样过程。

### 2.2 带负采样的跳克数

自然语言语料库由线性序列的单词流组成。近年来，神经嵌入方法和基于矩阵分解的方法在词汇表征学习中得到了广泛的应用。（Mikolov等人2013年）已被证明是学习单词表示的有效和有效的方法。改进跳格模型的两个显著方法是负抽样（SGN）和分层SoftMax。在本文中，我们选择使用前一种方法。

在SGN中，采用了噪声对比估计（NCE）方法的简化变种（Gutmann和Hyvarinen，2012年；¨mnih和teh，2012年），以增强跳跃图模型的鲁棒性。sgns随机地从一个经验的unigram词分布中创建负对（w，cn），并尝试用低维向量表示每个词w∈v和上下文词c∈v对。sgns的目标函数是最大化正对（w，c）和最小化负对（w，cn）。

$\underset{\overline{w}, \vec{c}}{\arg \max } \log \sigma(\vec{w} \cdot \vec{c})+\lambda \cdot \mathbb{E}_{c_{N} \sim P_{D}}\left[\log \sigma\left(-\vec{w} \cdot \overrightarrow{c_{N}}\right)\right]$

其中，σ（·）是定义为σ（x）=（1+e−x）−1的乙状结肠函数，λ是负样本数。ecn～pd[·]是当从负采样cn获得的实例符合分布pd=（c）d（数据集d中unigram c的分布）时的期望值。

### 2.3 PMI矩阵和SVD

另一种学习图形表示（尤其是单词表示）的方法是基于矩阵分解技术。这种方法学习了基于全局共现计数统计的表示方法，并优于基于局部分离的神经网络方法。某些预测任务中的上下文窗口（Bullinaria和Levy 2012；Levy、Goldberg和Dagan 2015）。

矩阵因式分解方法的一个例子是超空间模拟分析（Lund和Burgess，1996），它因式分解单词共现矩阵以生成单词表示。这种方法和相关方法的一个主要缺点是，频繁出现的词义值相对较小的词（如停止词）对所生成的词表示有不均衡的影响。Church和Hanks的Pointwise Mutual Information（PMI）矩阵（Church和Hanks 1990）被提议解决这个问题，并被证明能够提供更好的词汇表达（Bullinaria和Levy 2007）：

$P M I_{w, c}=\log \left(\frac{\#(w, c) \cdot|D|}{\#(w) \cdot \#(c)}\right)$

$|D|=\sum_{w} \sum_{c} \#(w, c)$

提高绩效的一个常见方法是将每一个负值赋给0，详见（Levy和Goldberg 2014），以形成PPMI矩阵。

$X_{w, c}=\max \left(P M I_{w, c}, 0\right)$

where *X* is the PPMI matrix

虽然ppmi矩阵是一个高维矩阵，但使用截断SVD方法（Eckart和Young 1936）进行降维时，会产生与l2损失相关的最优秩d因子分解（Levy和Goldberg 2014）。我们假设矩阵x可以分解为三个矩阵u∑v t，其中u和v是正交矩阵，∑是对角矩阵。换句话说，

$X \approx X_{d}=U_{d} \Sigma_{d} V_{d}^{T}$

这里，u d和vd是u和v的左d列，对应于顶部d的奇异值（in∑d）。据Levy等人（2015年），单词表示矩阵r可以是：$R=U_{d}\left(\Sigma_{d}\right)^{1 / 2} \quad$ or $\quad R=U_{d}$

ppmi矩阵x是单词表示矩阵和上下文矩阵的乘积。SVD程序为我们提供了一种从矩阵x中找到矩阵r的方法，其中r的行向量，即单词/顶点的低维表示，可以通过x给出的高维表示的线性投影得到。我们认为，这样的一种方法可以从矩阵x中找到矩阵r。截面不必是线性投影。在这项工作中，我们研究使用非线性投影方法来取代线性SVD方法，通过使用深神经网络。

### 2.4 深层神经网络

深层神经网络可用于学习多层次的特征表示，在不同领域取得了成功的结果（Dahl等人2012年；Krizhevsky、Sutskever和Hinton 2012年）。培训这样的网络很困难。一种有效的解决方案，在（Hinton和Salakhutdinov 2006；Bengio等人2007年），是利用贪心层无监督的预训。此策略旨在一次学习每个层的有用表示。然后，将学习到的低级表示作为输入输入输入到下一层以进行后续表示。神经网络通常采用非线性激活函数，如乙状结肠或TANH来捕获从输入到输出的复杂非线性投影。

为了训练包含多个功能表示层的深层架构，自动编码器已经成为常用的构建块之一（Bourlard和Kamp 1988；Hinton和Zemel 1994）。自动编码器执行两个操作——编码步骤，然后是解码步骤。在编码步骤中，将函数fθ1（·）应用于输入空间中的向量，并将其发送到新的特征空间。在这一过程中，通常涉及一个激活函数来模拟两个向量空间（输入向量空间和潜在向量表示空间）之间的非线性。在解码步骤中，使用重构函数gθ2（·）从潜在表示空间重构原始输入向量。假设fθ1（x）=σ（w1x+b1）和gθ2（y）=σ（w2y+b2），其中σ（·）是激活函数，θ1=w1，b1是编码器涉及的权重（参数），θ2=w2，b2是解码器涉及的权重（参数）。这里，w1和w2是从输入空间转换向量的线性映射（矩阵），b1和b2是偏移向量。我们的目标是通过找到θ1和θ2来最小化以下重建损失函数：

$\sum_{i} L\left(x^{(i)}, g_{\theta_{2}}\left(f_{\theta_{1}}\left(x^{(i)}\right)\right)\right)$

其中，l是样本损失，x（i）是第i个实例。

堆叠式自动编码器是由多层此类自动编码器组成的多层深度神经网络。堆叠式自动编码器采用分层训练的方法来提取基本规律，逐层从数据中捕获不同级别的抽象，较高的层从数据中传输较高级别的抽象。

### 3 dngr模型

我们现在详细说明我们的dngr模型。如图1所示，模型由三个主要步骤组成。首先，我们引入随机冲浪模型（第3.1节）来获取图形结构信息并生成概率共现矩阵。接下来，我们根据概率共现矩阵通过以下方法计算PPMI矩阵（Bullinaria和Levy 2007）（见第2.3节）。之后，使用叠加去噪自动编码器（第3.2节）学习低维顶点表示。



### 3.1随机冲浪和上下文加权

尽管有效，但将图结构转换为线性序列的抽样方法存在一些缺陷。首先，采样序列的长度是有限的。这使得捕获出现在采样序列边界的顶点的正确上下文信息变得困难。其次，对于某些超参数，如行走长度η和总行走γ，特别是对于大型图形，确定这些超参数并不容易。

为了解决这些问题，我们考虑使用一个由pagerank模型驱动的随机冲浪模型来对任务进行排名。我们首先随机排列图中的顶点。我们假设我们当前的顶点是第i个顶点，并且有一个转换矩阵A捕捉不同顶点之间的转换概率。

我们引入一个行向量pk，它的j次项表示K阶跃迁后到达j次顶点的概率，p0是初始的1-热向量，i次项的值为1，其他项的值为0。我们考虑一个重新启动的随机冲浪模型：每次都有一个概率α，随机冲浪程序将继续，概率1−α，它将返回到原始顶点并重新启动程序。这会导致以下循环关系：

$p_{k}=\alpha \cdot p_{k-1} A+(1-\alpha) p_{0}$

如果我们假设过程中没有随机重启，那么在转换的K步之后到达不同顶点的概率由以下公式指定：

$p_{k}^{*}=p_{k-1}^{*} A=p_{0} A^{k}$

直观地说，两个顶点之间的距离越近，它们之间的关系就越亲密。因此，根据上下文节点的相对d来衡量其重要性是合理的。 保持到当前节点。这种加权策略都是在Word2vec(Mikolov等人)中实施的。(2013年)和手套(彭宁顿、索契和曼宁，2014年)，并被认为对实现g很重要。 OOD实证结果(Levy、Goldberg和Dagan 2015)。基于这一事实，我们可以看到，理想情况下，I-th顶点的表示应该以下方式构造：

$r=\sum_{k=1}^{K} w(k) \cdot p_{k}^{*}$

其中w（·）是一个递减函数，$w(t+1)<w(t)$

我们认为，基于上述随机冲浪程序构造顶点表示的以下方法实际上满足上述条件：

$r=\sum_{k=1}^{K} p_{k}$

事实上，我们有以下几点：

$p_{k}=\alpha^{k} p_{k}^{*}+\sum_{t=1}^{k} \alpha^{k-t}(1-\alpha) p_{k-t}^{*}$

其中p 0=p0。$p_{t}^{*}$ in $r$系数为

$w(t)=\alpha^{t}+\alpha^{t}(1-\alpha)(K-t)$

式中w（t）为α<1后的递减函数。而跳跃图中的加权函数可以描述为

$w^{\prime}(t)=-\frac{t}{K}+\left(1+\frac{1}{K}\right)$

在GloVe中的加权函数是

$w^{\prime \prime}(t)=\frac{1}{t}$

如图2所示，所有加权函数都是单调递减函数。类似于word2vec和gLove，我们的模型还允许对上下文信息进行不同的权衡。 根据它们与目标之间的距离，这对于构建良好的单词表示是非常重要的。我们将通过实验来评估这方面的重要性。

3.2叠加去噪自动编码器