# 用逆正则自编码器学习深度网络表示

摘要：网络表示学习的问题，也称为网络嵌入，出现在许多机器学习任务中，假设在顶点表示中存在少量可变性，其可以捕获原始网络结构的“语义”。大多数现有的网络嵌入模型，具有浅或深的体系结构，从采样的顶点序列中学习顶点表示，使得低维嵌入保留了局部性属性和/或全局重建能力。然而，由于来自输入网络的采样序列的固有稀疏性，所得到的表示对于模型概括是困难的。因此，解决问题的理想方法是通过学习采样序列上的概率密度函数来生成顶点表示。然而，在许多情况下，低维歧管中的这种分布可能并不总是具有分析形式。在本研究中，我们建议学习具有对称正规化自动编码器（NetRA）的网络表示。NetRA通过共同考虑局部保持和全局重建约束来学习平滑的正则化顶点表示，从而很好地捕获网络结构。联合推理被封装在生成性对抗训练过程中，以规避明确的先验分布的要求，从而获得更好的泛化性能。我们凭经验证明了网络结构的关键特性是如何被捕获的，以及NetRA对各种任务的有效性，包括网络重建，链路预测和多标签分类。

## 1引言

网络分析以其在挖掘有用信息方面的巨大潜力吸引了许多研究兴趣，这些信息有助于下游任务，如链路预测，社区检测和社交网络上的异常检测[34]，生物网络[31]和语言网络[28]。仅举几例。为了分析网络数据，一个基本问题是学习每个顶点的低维矢量表示，以便在学习的矢量空间中保留网络结构[23]。对于这个问题，存在两个主要挑战：（1）保存复杂的结构性质。网络嵌入的目的是训练模型到“训练网络”，即保持网络的结构属性[23,26]。然而，网络的潜在结构太复杂，不能通过可以捕获本地网络邻域信息和全局网络结构的明确形式的概率密度来描绘。（2）网络抽样稀疏性。目前关于网络层叠的研究采用网络采样技术，包括随机游走采样，广度优先搜索等，将顶点序列导出为训练数据集。然而，采样数据仅代表所有顶点序列的一小部分。

另一种方法是在连续的代码空间中编码这些离散结构[37]。不幸的是，学习继续潜在的离散网络表示仍然是一个具有挑战性的问题，因为在许多情况下，先验分布可能不存在于低维流形中[26]。

最近关于网络嵌入的工作在学习复杂网络的顶点表示方面取得了丰硕的成果[23,26,37]。这些表示采用非线性变换来捕获原始网络的“语义”。大多数现有方法首先采用随机游走技术从输入网络中采样一堆顶点序列，然后用这些序列提供学习模型以推断最佳的低维顶点嵌入dings。
		然而，采样策略受到数据稀疏性问题的困扰，因为在实际网络中顶点序列的总量通常非常大，并且通常难以枚举所有数据。随后，对稀疏样本集的学习倾向于产生过于复杂的模型来解释采样数据集，最终导致过度拟合。虽然采用自动编码器将输入编码为连续的潜在表示[37]，但仍需要进行注册以迫使学习的表示保留在潜在的流形上。理想情况下，我们可以生成具有先验分布的连续顶点表示。然而，在许多情况下，如果不是不可能的话，在低维流形中预先定义先验分布的显式形式是困难的。
		例如，Dai等人。[6]提出训练一个鉴别器来区分固定的先验分布和输入编码产生的guish样本，从而推动嵌入分布与固定的先验分配相匹配。虽然这提供了更大的灵活性，但却遭遇模式崩溃问题[16]。此外，大多数具有深层体系结构的网络层面模型通常不考虑采样顶点序列中顶点的顺序[37]。因此，不能很好地考虑接近顺序的信息。

为了解决上述挑战，在本研究中，我们提出了一种新的模型，用于学习具有对称正规化自动终结器（NetRA）的网络表示。NetRA联合最小化网络局部保留损失和自动编码器的重建误差，其利用长短期存储网络（LSTM）作为编码器将输入序列映射成固定长度表示。联合嵌入推理被封装在生成性对抗训练过程中，以规避明确的先验分布的要求。如图1所示，我们的模型采用离散LSTM自动编码器来学习连续顶点表示，并将采样的顶点序列作为输入。在该模型中，除了最小化LSTM自动编码器中的重建误差之外，隐藏层处的局部保持损失也同时最小化。同时，连续空间发生器也通过约束与编码器的分配一致来训练。生成对抗训练可以被视为网络嵌入过程的补充正则化器。

NetRA展示了网络嵌入模型所需的理想属性：1）结构属性保留，NetRA杠杆使LSTM成为编码器，以捕获从网络采样的每个序列中的顶点之间的邻域信息。另外，模型也与局部保持约束同时训练。2）泛化能力，泛化能力要求网络嵌入模型能够很好地推广看不见的顶点序列，这些顶点序列遵循与总体相同的分布。
生成对抗性训练过程使得所提出的模型能够学习平滑的正则化表示，而无需预先定义明确的密度分布，从而克服了顶点输入序列的稀疏性问题。
		我们提出实验结果，以显示NetRA在各种任务上的嵌入能力，包括网络重建，链路预测和多标签分类。
总而言之，这项工作的主要贡献如下：

我们提出了一种新的深度网络嵌入模型，该模型具有广泛的正则化自动编码器NetRA，通过使用生成的广告训练过程联合最小化保持位置丢失和全局重建误差来学习顶点表示。结果表示对从网络导出的稀疏输入是鲁棒的。
		“NetRA”学习一种异常正则化的LSTM编码器，该编码器可以从离散输入产生有用的顶点表示，而不需要预定义的显式潜在空间先验。
		我们利用现实世界的信息网络对网络重建，链路预测和多标签分类等任务进行了广泛的实验。
实验结果证明了NetRA的有效性和有效性。
		本文的其余部分安排如下。在第2节中，我们回顾了自动编码器，生成广告网络和网络嵌入算法的初步知识。在第3节中，我们描述了使用生成对抗训练过程学习低维映射的NetRA框架。在第4节中，我们通过在网络重建，链路预测和多标签分类的任务上调整此联合学习框架来演示NetRA的性能。在第5节中，我们将NetRA框架与其他网络嵌入算法进行比较，并讨论了几个相关的工作。最后，在第6节中，我们总结了这项研究，并提出了未来工作的几个方向

## 2 初步

### 2.1 自动编码器神经网络

训练自动编码器神经网络以将目标值设置为等于输入。网络由两部分组成：编码器f蠒（路）将输入（x鈭鈭Rn）映射到潜在的低维表示，而解码器h蠄（路）产生输入的重建。具体来说，给定一个数据分布Pdata，从中得出x，即x鈭鈭Pdata（x），我们想要学习代表f蠒（x），使得输出假设h蠄（f蠒（x））大约等于x。学习过程简单地描述为最小化成本函数：

$\min \mathbb{E}_{\mathbf{x} \sim \mathbb{P}_{\text { data }}(\mathbf{x})}\left[\operatorname{dist}\left(\mathbf{x}, h_{\psi}\left(f_{\phi}(\mathbf{x})\right)\right)\right]$

dist（路）是数据空间中的某种相似性度量。
在实践中，距离测量有很多选择。例如，如果我们使用鈩鈩orm来测量重建误差，那么目标函数可以定义为$\mathcal{L}_{\mathrm{AE}}(\phi, \psi ; \mathbf{x})=\mathbb{E}_{\mathbf{x} \sim \mathbb{P}_{\text {data}}(\mathbf{x})}\left\|\mathbf{x}-h_{\psi}\left(f_{\phi}(\mathbf{x})\right)\right\|^{2}$。类似地，交叉熵损失的目标函数可以定义为

$-\mathbb{E}_{\mathbf{x} \sim \mathbb{P}_{\text {data}}(\mathbf{x})}\left[\mathbf{x} \log h_{\psi}\left(f_{\phi}(\mathbf{x})\right)+(1-\mathbf{x}) \log \left(1-h_{\psi}\left(f_{\phi}(\mathbf{x})\right)\right)\right]$

编码器fϕ(·)和解码器hψ(·)的选择可能因任务不同而有所不同。在本文中，我们使用LSTM自动编码器[27]，它能够处理作为输入的序列。

### 2.2 生成对抗性网络

生成对抗性网络(GANS)[11]为两个参与者(生成器дθ(·)和鉴别器dw(·)建立了一个对抗训练平台，用于玩极小极大游戏：$\min _{\theta} \max _{\boldsymbol{w}} \underset{\mathbf{x} \sim \text { data }(\mathbf{x})}{\mathbb{E}}\left[\log d_{w}(\mathbf{x})\right]+\underset{\mathbf{z} \sim \mathbb{P}_{g}(\mathbf{z})}{\mathbb{E}}\left[\log \left(1-d_{w}\left(g_{\theta}(\mathbf{z})\right)\right)\right]$

生成器дθ（·）尝试将噪声映射到输入空间，与真实数据一样，而鉴别器dw（x）表示x来自数据而不是噪声的概率。它旨在区分实际数据分布Pdata（x）和假样本分布Pg（z），例如，z~N（0，I）。Wasserstein GANs [1]通过用Earth-Mover（Wasserstein-1）距离代替Jensen-Shannon散度来克服不稳定的训练问题，该距离考虑解决问题

$\min _{\theta} \max _{\boldsymbol{w} \in \boldsymbol{W} \mathbf{x} \sim \mathbb{P}_{\text {data}}(\mathbf{x})}\left[d_{\boldsymbol{w}}(\mathbf{x})\right]-\underset{\mathbf{z} \sim \mathbb{P}_{g}(\mathbf{z})}{\mathbb{E}}\left[d_{w}\left(g_{\theta}(\mathbf{z})\right)\right]$

通过在紧凑空间[-c，c]内剪切鉴别器的权重来保持鉴别器上的Lipschitz约束W.

### 2.3 网络嵌入

网络嵌入方法试图学习编码关于网络的结构信息的表示。这些方法学习了将顶点嵌入到低维空间中的映射。给定编码顶点集{x（1），...，x（n）}，找到每个x（i）的em床上用量f（x（i））可以形式化为优化问题[39,41]]

$\min _{\phi} \sum_{1 \leq i<j \leq n} L\left(f_{\phi}\left(\mathbf{x}^{(i)}\right), f_{\phi}\left(\mathbf{x}^{(j)}\right), \varphi_{i j}\right)$

其中fφ（x）∈Rd是给定输入x的嵌入结果。L（·）是一对输入之间的损失函数。φij是x（i）和x（j）之间的权重 

我们认为Laplacian Eigenmaps（LE）非常适合框架。LE使嵌入能够保留网络结构的局部性。形式上，可以通过最小化以下目标函数来获得嵌入：

$\mathcal{L}_{L E}(\phi ; \mathbf{x})=\sum_{1 \leq i<j \leq n}\left\|f_{\phi}\left(\mathbf{x}^{(i)}\right)-f_{\phi}\left(\mathbf{x}^{(j)} \|^{2} \varphi_{i j}\right.\right.$

## 3 方法

在本节中，我们介绍NetRA，一种使用对向正则化自动编码器的深度网络嵌入模型，以顶点序列作为输入来学习平滑的正则化顶点表示。结果表示可以用在下游任务中，例如链路预测，网络重建和多类分类。

### 3.1 随机游走发生器

给定网络G（V，E），DeepWalk [23]中的随机游走生成器用于获得以G（V，E）中的每个顶点$v \in V$为根的截断随机游走（即，版本序列）。从最后访问的顶点的邻居中随机地对步行进行采样，直到达到预设的最大长度。

随机游走采样技术广泛应用于网络嵌入研究[12,23,37]。但是，它遭受网络采样中的稀疏性问题。对于给定网络中的每个顶点，如果我们假设平均节点度为炉d，步行长度为l且样本数为k，则可以通过以下方式计算步行的采样分数：

$p_{\text {frac}} \propto \frac{|V| \times k}{|V| \times \overline{{d}^{l}}}=\frac{k}{\overline{{d}^{l}}} \times 100 \%$

采样分数的影响如图2所示。在该示例中，DeepWalk用于在4.1节中描述的UCI消息网络上执行链路预测任务。图2（a）和图2（b）显示，如果步行长度或平均顶点度增加，则性能急剧下降1。根据Eq。（7），显然，当l或¯d增加时，步行的采样分数越来越小。因此，由于稀疏输入，训练的模型易于过度拟合。相反，如果样本数k增加，性能会变得更好，如图2（c）所示。然而，更多的采样走路也要求模型训练带来更多的计算负担。因此，期望在稀疏采样的网络漫游上开发具有更好的泛化能力的有效模型。

### 3.2 使用对称正规化自动编码器嵌入

在本文中，我们提出NetRA，一种具有异常正则化自动编码器的网络嵌入模型，以解决稀疏性问题。自动编码器通常用于数据嵌入，例如图像和文档。它通过将输入数据映射到潜在空间来提供输入数据的信息性低维表示。遗憾的是，如果允许编码器和解码器容量过大，则自动编码器可以学习执行复制任务，而无需提取有关数据分布的有用信息[10]。我们建议使用生成对抗训练过程作为补充正则化器。该过程有两个优点。一方面，正则化指导者可以指导有关数据的有用信息的提取[10]。另一方面，生成对抗训练提供了更强大的离散空间表示学习，可以很好地解决稀疏采样步行中的过度拟合问题[19]。具体来说，在NetRA中，鉴别器通过比较来自自动编码器潜在空间的样本与来自生成器的伪样本进行更新，如图1所示。自动编码器的潜在空间为网络中的顶点提供了最佳嵌入。编码器和鉴别器的更新。在本研究中，在本研究中，我们将LSTM用作编码器和解码器网络[27]，因为它需要考虑采样的步道的顺序信息。

这种联合架构需要针对每个部分的专门培训目标。可以通过最小化重建的负对数似然来单独训练自动编码器，这通过实施中的交叉熵损失来指示。

$\mathcal{L}_{\mathrm{AE}}(\phi, \psi ; \mathbf{x})=-\mathbb{E}_{\mathbf{x} \sim \mathbb{P}_{\text {data}}(\mathbf{x})}\left[\operatorname{dist}\left(\mathbf{x}, h_{\psi}\left(f_{\phi}(\mathbf{x})\right)\right)\right]$

其中dist（x，y）= x log y +（1∞x）log（1∞y）。这里x是来自训练数据的抽样批次。fφ（x）是x的嵌入潜在表示，它也是鉴别器的正样本，在图1中用箭头表示“+”.φ和ψ分别是编码器和解码器功能的参数。在自动编码器的训练迭代中，不仅更新编码器和解码器，还联合最小化保持位置的损失（方程（6））。

如图1所示，NetRA最小化了来自编码器函数$f_{\phi}(\mathbf{x}) \sim \mathbb{P}_{\phi}$的学习表示与来自连续发生器模型的表示之间的分布写作$g_{\theta}(\mathrm{z}) \sim \mathbb{P}_{\theta}(\mathrm{z})$之间的基准移动距离的对偶形式可以描述如下[1]

$W\left(\mathbb{P}_{\phi}(\mathbf{x}), \mathbb{P}_{\theta}(\mathbf{z})\right)=\sup _{\|d(\cdot)\|_{L \leq 1}} \mathbb{E}_{\mathbf{y} \sim \mathbb{P}_{\phi}(\mathbf{x})}[d(\mathbf{y})]-\mathbb{E}_{\mathbf{y} \sim \mathbb{P}_{\theta}(\mathbf{z})}[d(\mathbf{y})]$

其中$\|d(\cdot)\|_{L \leq 1}$是Lipschitz连续性约束（Lip schitz常数1）。如果我们有一系列函数$\left\{d_{w}(\cdot)\right\}_{w \in W}$对于某些K都是K-Lipschitz，那么我们有

$W\left(\mathbb{P}_{\phi}(\mathbf{x}), \mathbb{P}_{\theta}(\mathbf{z})\right)\propto \max _{w \in \mathcal{W}}\underset{\mathbf{x} \sim \mathbb{P}_{\text {data}}^{\mathbb{E}}(\mathbf{x})}{\mathbb{E}}\left[d_{w}\left(f_{\phi}(\mathbf{x})\right)\right]-\underset{\mathbf{Z} \sim \mathbb{P}_{g}(\mathbf{z})}{\mathbb{E}}\left[d_{w}\left(g_{\theta}(\mathbf{z})\right)\right]$

鉴别器的成本函数是，

$\mathcal{L}_{\mathrm{DIS}}(w ; \mathbf{x}, \mathbf{z})=-\mathbb{E}_{\mathbf{X} \sim \mathbb{P}_{d a t a}(\mathbf{x})}\left[d_{w}\left(f_{\phi}(\mathbf{x})\right)\right]+\mathbb{E}_{\mathbf{Z} \sim \mathbb{P}_{g}(\mathbf{z})}\left[d_{w}\left(g_{\theta}(\mathbf{z})\right)\right]$

NetRA通过联合最小化自组织编码器重建错误和对抗训练过程中的局部保留损失来学习平滑表示。具体来说，我们考虑用目标函数求解联合优化问题：

$\mathcal{L}_{\mathrm{NETRA}}(\phi, \psi, \theta, w)=\mathcal{L}_{\mathrm{AE}}(\phi, \psi ; \mathbf{x})+\lambda_{1} \mathcal{L}_{\mathrm{LE}}(\phi ; \mathbf{x})+\lambda_{2} W\left(\mathbb{P}_{\phi}(\mathbf{x}), \mathbb{P}_{\theta}(\mathbf{z})\right)$

定理3.1。设P蠒（x）为任何分布。设Pθ（z）为写θ（z）的分布，其中z是从分布Pg（z）绘制的样本，写胃（路）是满足局部Lipschitz常数的函数$\left\{d_{w}(\cdot)\right\}_{w \in \mathcal{W}}$然后我们有

$\nabla_{\theta} \mathcal{L}_{N E T R A}=-\lambda_{2} \nabla_{\theta} \mathbb{E}_{z \sim \mathbb{g}_{g}(z)}\left[d_{w}\left(g_{\theta}(z)\right)\right]$

$\nabla_{w} \mathcal{L}_{N E T R A}=-\lambda_{2} \nabla_{w} \mathbb{E}_{x-\mathbb{P}_{\text {duta}}(x)}\left[d_{w}\left(f_{\phi}(x)\right)\right]$

$+\lambda_{2} \nabla_{w} \mathbb{E}_{\boldsymbol{z} \sim \mathbb{P}_{g}(\mathbf{z})}\left[d_{w}\left(g_{\theta}(\boldsymbol{z})\right)\right]$



我们现在拥有所需的所有衍生物。为了训练模型，我们使用块坐标下降来优化模型的不同部分之间的交替：（1）局部保持损失和自动编码器重建误差（更新$\phi$和$\psi$）（2）对抗训练过程中的鉴别器（更新w），以及（3）生成器（更新$\theta$）。算法1给出了完整方法的伪代码。

NetRA的训练过程包括以下步骤：首先，给定网络G（V，E），我们运行随机游走发生器获取长度为l的随机游走。然后，将每个顶点的独热表示$\mathbf{X}^{(i)}$作为LSTM单元的输入。通过编码层传递随机游走，得到顶点的矢量表示。在解码器网络之后，顶点表示将被转换回n维。通过最小化自动编码器操作中的重建误差，在输入和输出之间计算交叉熵损失。同时，局部保持约束确保相邻顶点接近(算法1中的步骤2-7)。编码器的潜在表示法和发生器的输出将被输入鉴别器，以获得对抗损失(步骤10-17)。此外，发生器通过多层感知器将高斯噪声转换成与真实数据一样接近的潜在空间(步骤20-23)。在Netra的训练之后，通过使输入穿过编码器功能，我们获得了网络的顶点表示$f_{\phi}(\mathbf{x})$。

**最优性分析：**如图1所示，NetRA可以解释为最小化两个分布之间的差异，即$\mathbb{P}_{\phi}(\mathbf{x})$ and $\mathbb{P}_{\theta}(\mathbf{z})$。我们提供以下命题，表明在我们的参数设置下，如果Wasserstein距离收敛，编码器分布$f_{\phi}(\mathbf{x}) \sim \mathbb{P}_{\phi}(\mathbf{x})$与发生器分布相符合$g_{\theta}(\mathbf{z}) \sim \mathbb{P}_{\theta}(\mathbf{z})$。