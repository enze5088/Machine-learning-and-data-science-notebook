# **Dynamic Graph Convolutional Network**

## **1 Introduction**

许多研究工作致力于对顶点聚焦和图形聚焦应用中的结构化数据进行分类[9,19,21,23]。然而，现有的研究存在很大的局限性，这些研究大多集中在静态图上。然而，许多现实世界的结构化数据是动态的，图中的节点/边可能会随时间变化。在这种动态场景中，时间信息也可以发挥重要作用。

在过去的十年中，（深度）神经网络通过学习将世界表示为一个嵌套的概念层次，在许多不同的应用领域取得了显著的效果，从而显示出其强大的能力和灵活性。需要强调的是，只有少数研究工作致力于直接使用神经网络模型对图形结构进行编码[1,3,4,12,15,20]。其中，据我们所知，没有人能够管理动态图。

为了通过使用神经网络模型来利用结构化数据和时间信息，我们提出了两种新的方法，将长短期记忆网络（lstms，[8]）和图卷积网络（gcns，[12]）结合起来。它们都能够处理顶点聚焦的应用程序。这些技术分别能够捕获时间信息和适当地管理结构化数据。此外，我们还扩展了处理以图形为中心的应用程序的方法。

LSTMS是一种特殊的递归神经网络（RNN，[10]），它能够改善长期短期依赖关系的学习。所有RNN都具有神经网络重复模块链的形式。准确地说，RNN是人工神经网络，其中各单元之间的连接形成有向循环。这将创建网络的内部状态，使其能够显示动态的时间行为。在标准RNN中，重复模块基于简单的结构，例如单个（双曲正切）单元。LSTM通过组合四个相互作用单元来扩展重复模块。

LSTMS是一种特殊的递归神经网络（RNN，[10]），它能够改善长期短期依赖关系的学习。所有RNN都具有神经网络重复模块链的形式。准确地说，RNN是人工神经网络，其中各单元之间的连接形成有向循环。这将创建网络的内部状态，使其能够显示动态的时间行为。在标准RNN中，重复模块基于简单的结构，例如单个（双曲正切）单元。LSTM通过组合四个相互作用单元来扩展重复模块。

## 2、相关工作

为了处理这类数据，获得良好的分类结果，文献中提出的传统方法主要遵循两个不同的方向：用传统的学习方法将结构属性识别为特征来管理它们，或传播标签以获得直接分类。

朱等。[24]提出一种基于高斯随机场模型（也称为标签传播）的半监督学习算法。将学习问题表述为图上的高斯随机场，用谐波函数描述一个场，并利用矩阵方法或信念传播有效地解决了该问题。Xu等人[21]提出了一种半监督因子图模型，该模型能够利用节点之间的关系。在这种方法中，每个顶点都被建模为一个变量节点，而各种关系则被建模为因子节点。Grover和Leskovec在[6]中提出了一种高效且可扩展的网络特征学习算法，该算法利用随机梯度下降优化了一种新的网络感知、邻域保持目标函数。Perozzi等人[18]提出一种称为深水的方法。此技术使用截断随机行走来有效地学习图中顶点的表示。
这些潜在的表现，编码的图形关系在一个向量空间，可以很容易地利用统计模型，从而产生最先进的结果。

不幸的是，所描述的技术无法处理随时间动态变化的图（图中的节点/边可能会随时间变化）。在动态网络中，设计了少量的方法来对节点进行分类[14,22]。Li等人[14]提出一种能够学习潜在特征表示和捕获动态模式的方法。Yao等人[22]提出了一种基于支持向量机的方法，该方法将前一时间瞬间的支持向量与当前的训练数据相结合，以利用时间关系。Pei等人[17]定义一种称为动态因子图模型的方法，用于动态社会网络中的节点分类。更准确地说，这种方法将动态图数据组织成一系列的图。设计了节点因子、相关因子和动态因子三类因子分别捕捉节点特征、节点相关性和时间相关性。设计了节点因子和相关因子来捕捉图形结构的全局和局部特性，同时动态因子利用时间信息。

必须强调的是，很少有人关注神经网络模型对结构化数据集的泛化。在过去的几年中，许多研究工作重新审视了将神经网络归纳为任意结构图[1,3,4,12,15,20]的问题，其中一些研究工作在以前由其他技术主导的领域取得了很好的结果。斯卡塞利等人[20]形成一个新的神经网络模型，称为图神经网络（GNN）。该模型是在扩展神经网络方法的基础上，以图形结构的形式处理数据。GNNS模型可以处理不同类型的图（例如，非循环图、循环图、有向图和无向图），并将图及其节点映射到一个多维欧几里得空间中，以学习最终的分类/回归模型。Li等人[15]通过使用门控循环单元[2]放宽传播步骤的收缩性要求，并通过预测单个输入图的输出序列，扩展GNN模型。Bruna等人[1]描述卷积神经网络（CNN，[5]）的两个推广。准确地说，作者提出了两种变体：一种基于域的层次聚类，另一种基于拉普拉斯图的频谱。Duvenaud等人[4]展示了CNN的另一个变体，用于图形结构。这个模型允许对任意大小和形状的图进行端到端的学习。Deffarrard等人[3]在光谱图理论的背景下，介绍了CNN的一个公式。该模型为图形上快速局域卷积滤波器的设计提供了有效的数值方案。值得注意的是，它达到了在任何图结构上工作的经典CNN的计算复杂性。Kipf和Welling[12]提出了一种基于CNN的图形结构化数据半监督学习方法。在他们的工作中，他们利用光谱图卷积框架的局部一阶近似[7]。他们的模型在图的边缘数量上线性缩放，学习隐藏层表示编码局部和结构图特征。

注意，这些神经网络架构不能正确处理时间信息。

### 3、方法

在本节中，我们将介绍两种新的网络体系结构来处理顶点/图形聚焦应用程序。它们都依赖于以下直觉：

- GCN可以有效地处理图形结构化的信息，但是它们缺乏处理在时间上改变的数据结构的能力。这一限制至少有两个方面：(i)不能管理AG E动态顶点特征，(ii)无法管理动态边连接。
- LSTM擅长寻找长期的短期依赖关系，但它们缺乏显式利用图结构信息的能力。

由于我们感兴趣的任务具有动态特性，本文提出的新网络结构将处理图的有序序列和顶点特征的有序序列。注意，对于长度为1的序列，这减少到了第1节中描述的顶点/图形聚焦应用程序。

我们的贡献是基于将图卷积(GCNS的基本层GC)的扩展与LSTM的改进版本相结合的思想，从而学习下游递归。 NT单元，利用图结构数据和顶点特征。

我们提出了两个以图形序列为输入的GC类和相应的顶点特征序列，并输出一个新的顶点表示的有序序列。这些 层次是：

- 瀑布动态GC层，它在序列的每一步对顶点输入序列执行图卷积。该层的一个重要特点是，每个图卷积的可训练参数在序列的各个步骤之间共享；
- 连接动态GC层，它在序列的每个步骤对顶点输入特征执行一个图卷积，并将其连接到输入。同样，可训练参数在序列中的步骤之间共享。

这两层中的每一层都可以与LSTM的修改版本一起执行顶点序列的半监督分类或图序列的监督分类。这两个任务之间的区别仅仅在于我们如何执行数据的最后一次处理（有关进一步的详细信息，请参见等式（1）和等式（2））。

在下一节中，我们将提供两个修改后的GC层的数学定义，修改后的LSTM版本，以及一些其他方便的定义，这些定义在我们描述最终网络架构时很有用。

### 3.1 定义

设(GI)i∈ZT与ZT：={1，2，.....，T}是无向图GI=(Vi，EI)的有限序列，Vi=V∀i∈ZT，即序列中的所有图都有相同的顶点。考虑到 图GI，对于每个顶点v，k，∈，V，设x，k，i，∈，R，d为对应的特征向量。序列ZT中的每一步i完全可以由它的图GI(由邻接矩阵x4ai建模)a来定义。 由顶点特征矩阵xi∈R_xV_x×d(行向量为x~k_i)构成的矩阵。

我们将用[Y]i，j表示矩阵Y的第I行，j-列元素，用Y表示Y的转置。ID是R_d的恒等矩阵，Softmax和relu是软最大值，rrect是R_d的单位矩阵。 化线性单位函数[5]。

矩阵P∈R_d×d是R_d上的投影，如果它是一个对称的半正定矩阵，且P_2=P。特别是，如果它是对角线矩阵，它就是对角线投影仪(可能有一些 主对角线上的零条目）。换句话说，Rd上的对角线投影仪是对角线矩阵，在主对角线上具有约1s，当右乘以d维列矢量时。 它将与P的主对角线上的零对应的v的所有条目归零：

![1563772446880](F:\Machine-learning-and-data-science-notebook\images\DGCN\1563772446880.png)

我们在这里回顾GC层[12]和LSTM[8]的数学，因为它们是我们贡献的基本组成部分。