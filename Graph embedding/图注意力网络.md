# 图注意网络

摘要：我们提出了图形注意力网络(GATs)，一种基于图形结构数据的新型神经网络架构，利用掩蔽的自注意层来解决先前基于图形卷积或其近似的方法的缺点。通过堆叠层，节点可以参与到它们的邻域特性中，我们可以(隐式地)为一个邻域中的不同节点指定不同的权重，而不需要任何计算密集型矩阵操作(例如倒置)，也不需要预先了解图形结构。通过这种方法，我们同时解决了基于谱的图神经网络的几个关键挑战，并使我们的模型易于适用于归纳和转导问题。我们的GAT模型已经在四个已建立的转导和归纳图基准上取得或匹配了最先进的结果:Cora、Citeseer和Pubmed引文网络数据集，以及蛋白质-蛋白质相互作用数据集(其中测试图在培训过程中不可见)。

## 1. 介绍

卷积神经网络(CNNs)已成功应用于解决图像分类(He et al.， 2016)、语义分割(Jegou et al.， 2017)或机器翻译(Gehring et al.， 2016)等问题，其中底层数据表示具有网格结构。这些体系结构通过将它们应用于所有输入位置，有效地重用它们的本地过滤器，并带有可学习的参数。

然而，许多有趣的任务所涉及的数据不能用网格结构表示，而是位于不规则的域中。这是三维网格、社交网络、电信网络、生物网络或脑连接的例子。这些数据通常可以用图形表示。

文献中已经有几次尝试将神经网络扩展到处理任意结构的图。早期的研究使用递归神经网络处理以有向无环图表示的图域中数据(Frasconi et al.， 1998;Sperduti,Starita, 1997)。图神经网络(GNNs)是Gori等人(2005)和Scarselli等人(2009)提出的一种递归神经网络的推广，它可以直接处理更一般的一类图，如循环图、有向图和无向图。gnn由一个迭代过程组成，该迭代过程将节点状态传播到均衡状态;然后是一个神经网络，它根据每个节点的状态产生一个输出。Li et al.(2016)采用并改进了这一思想，提出在传播步骤中使用门控回归单元(Cho et al.， 2014)。

然而，将卷积推广到图域的兴趣越来越大。这方面的进展通常分为光谱方法和非光谱方法。

一方面，谱方法是对图进行谱表示，并已成功地应用于节点分类。Bruna et al.(2014)通过计算图Laplacian的特征分解，在傅立叶域中定义卷积运算，从而产生了潜在的密集计算和非空间局域滤波器。这些问题在后续的工作中得到了解决。Henaff等人(2015)引入了平滑系数的光谱滤波器参数化，使其具有空间局域性。随后，Defferrard等人(2016)提出通过图Laplacian的Chebyshev展开近似滤波器，消除了计算Laplacian特征向量的需要，产生空间局域滤波器。最后,Kipf,Welling(2017)通过限制过滤器在每个节点周围的1步邻域内运行，简化了之前的方法。然而，在所有上述光谱方法中，学习滤波器依赖于拉普拉斯特征基，而拉普拉斯特征基依赖于图结构。因此，针对特定结构的模型不能直接应用于具有不同结构的图。

另一方面，我们有非光谱方法(Duvenaud et al.， 2015;阿特伍德,Towsley,2016;(Hamilton et al.， 2017)，直接在图上定义卷积，对空间上相邻的群进行操作。这些方法的挑战之一是定义一个操作符，它可以处理不同大小的邻域，并维护CNNs的权值共享属性。在某些情况下，这需要学习每个节点度的具体权重矩阵(Duvenaud et al.， 2015)，使用转换矩阵的幂定义邻域，同时学习每个输入通道和邻域度的权重(Atwood &amp;或者提取包含固定数量节点的邻域并对其进行规范化(Niepert et al.， 2016)。Monti等人(2016)提出了混合模型CNNs (MoNet)，这是一种将CNN架构统一概括为图形的空间方法。最近，Hamilton等人(2017)引入了GraphSAGE，一种以归纳方式计算节点表示的方法。该技术通过对每个节点的固定大小的邻域进行采样，然后在其上执行特定的聚合器(例如对所有采样的邻域特征向量的均值，或通过递归神经网络对它们进行馈送的结果)。这种方法在几个大型归纳基准测试中取得了令人印象深刻的性能。

注意机制已经成为许多基于顺序的任务事实上的标准(Bahdanau et al.， 2015;Gehring等，2016)。注意力机制的好处之一是，它允许处理不同大小的输入，将注意力集中在输入中最相关的部分，以便做出决策。当注意力机制被用来计算单个序列的表示时，它通常被称为自我注意或内部注意。与递归神经网络(RNNs)或卷积一起，自我注意被证明对机器阅读(Cheng et al.， 2016)和学习句子表示(Lin et al.， 2017)等任务是有用的。然而，Vaswani等人(2017)的研究表明，自我关注不仅可以改进基于RNNs或卷积的方法，而且足以构建强大的模型，在机器翻译任务中获得最先进的性能。

受最近这项工作的启发，我们引入了一个基于注意力的架构来执行图形结构化数据的节点分类。该方法的思想是，按照自我注意策略，通过关注相邻节点来计算图中每个节点的隐藏表示。注意力架构有几个有趣的特性:(1)操作是有效的，因为它可以跨节点对并行化;(2)通过对相邻节点指定任意权重，可以应用于不同程度的图节点;(3)该模型直接适用于归纳学习问题，包括模型必须推广到完全看不见的图形的任务。我们在四个具有挑战性的基准上验证了所提出的方法:Cora、Citeseer和Pubmed引用网络，以及一个蛋白质-蛋白质相互作用的归纳数据集，实现或匹配最先进的结果，这些结果突出了在处理任意结构图时基于注意力的模型的潜力。

值得注意的是，作为Kipf &;韦林(2017)和阿特伍德;托斯利(2016)，我们的工作也可以被重新表述为莫奈的一个特例(Monti et al.， 2016)。此外，我们跨边缘共享神经网络计算的方法让人想起关系网络(Santoro et al.， 2017)和VAIN (Hoshen, 2017)的公式，其中对象或代理之间的关系是通过使用共享机制成对聚合的。同样，我们提出的注意模型可以与Duan et al.(2017)和Denil et al.(2017)的作品联系起来，他们利用邻域注意运算来计算环境中不同物体之间的注意系数。其他相关方法包括局部线性嵌入(LLE) (Roweis &amp;Saul, 2000)和memory networks (Weston et al.， 2014)。LLE在每个数据点周围选取一定数量的邻域，并学习每个邻域的权系数，将每个邻域重构为其加权和。第二优化步骤提取点s特征嵌入。内存网络还与我们的工作共享一些连接，特别是当我们将节点的邻域解释为内存时，内存用于通过关注节点的值来计算节点的特征，然后通过将新特征存储在相同的位置来进行更新。

## 2. GAT 结构

在本节中，我们将介绍用于构建任意图形注意网络的构建块层(通过叠加这一层)，并直接概述其相对于之前在神经图形处理领域的工作在理论和实践上的好处和局限性。

### 2.1 图形注意层

我们将从描述单个图形注意层开始，作为在我们的实验中使用的所有GAT架构中使用的唯一层。我们使用的特殊注意设置与Bahdanau等人(2015)的工作密切相关，但该框架对特定的注意机制选择是不可知的。

我们层的输入是一组节点特征，$h= { \overrightarrow{h1}，\overrightarrow{h2}，。。。，\overrightarrow{h_n} }，\overrightarrow{h_i}∈R F，$其中N是节点数，F是每个节点的特征数。这一层产生了一套新的集合。 节点特征(具有潜在不同基数的F0)，h0={h0 1，h0 2，。。。，h0N}，h0 i∈R F0，作为其输出。

为了获得足够的表达能力将输入特征转化为更高层次的特征，至少需要一个可学习的线性变换。为此目的，作为第一步，一个共享的线性变换，由权矩阵wrf0f参数化，应用于每个节点。然后我们在节点上执行自我注意，一个共享的注意机制a: rf0 rf0 R计算注意系数

![1544519057439](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544519057439.png)

这表明节点j的特性对节点i的重要性。在其最一般的公式中，该模型允许每个节点在每个其他节点上参与，删除所有的结构信息。我们通过执行掩蔽注意将图结构注入到机制中，我们只计算节点jni的eij，其中Ni是图中节点i的某个邻域。在我们所有的实验中，这些就是i(包括i)的一阶邻域，为了使系数在不同节点间易于比较，我们使用softmax函数对j的所有选项进行规范化:

![1544519213777](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544519213777.png)

在我们的实验中,注意力机制是一个单层前馈神经网络,由权向量参数化~ 0 R 2 f,并应用LeakyReLU非线性负输入坡(α= 0.2)。完全展开后，注意机制计算出的系数(如图1(左)所示)可以表示为：

![1544533085424](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544533085424.png)

其中·T表示转位，k为连接操作。

![1544533175921](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544533175921.png)

图1:左:模型采用的注意机制$a(W\overrightarrow{h_i},W\overrightarrow{h_j})$，由权重向量$\overrightarrow{a} \in R^{2F'}$ 参数化，应用LeakyReLU激活。右图:由节点1在其邻域上绘制的多头注意力(K = 3 head)图。不同的箭头样式和颜色表示独立的注意力计算。不同的箭头样式和颜色表示独立的注意力计算。将每个头部的聚合特征串联或平均，得到$\overrightarrow{h_1'}$。

一旦获得,规范化注意系数是用于计算的线性组合相对应的功能,为每个节点作为最终的输出特性(潜在应用非线性后,σ)

![1544533931843](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544533931843.png)

为了稳定自我注意的学习过程，我们发现扩展我们的机制，采用多头注意是有益的，类似于Vaswani等人(2017)。具体来说，K个独立的注意机制执行方程4的变换，然后将它们的特征串联起来，得到如下输出特征表示

![1544533961281](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544533961281.png)

k代表连接,αk ij是规范化注意k系数计算的注意机制(k),和工作是相应的输入线性变换年代权重矩阵。注意，在此设置中，最终返回的输出h0将包含每个节点的KF0特性(而不是f0)。

特别地，如果我们对网络的最终(预测)层进行多头注意，则连接不再是明智的，我们采用平均法，并在此之前延迟应用最终的非线性(通常是用于分类问题的softmax或logistic sigmoid)

![1544534013135](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544534013135.png)

图1(右)显示了一个多头部图注意力层的聚合过程。

### 2.2 与相关工作的比较

2.1小节中描述的图形注意层直接解决了先前使用神经网络建模图形结构数据的方法中存在的几个问题.

- 计算效率高:自注意层的操作可以在所有边缘上并行化，输出特征的计算可以在所有节点上并行化。不需要特征分解或类似的计算密集型矩阵运算。计算F0特征的单个GAT注意头的时间复杂度可以表示为O(|V |F F0 + |E|F 0)，其中F为输入特征个数，|V |和|E|分别为图中节点数和边数。这种复杂性可以与基础方法相媲美，如图卷积网络(GCNs) (Kipf &amp;威林,2017)。应用多磁头注意力将存储和参数要求乘以K，而单个磁头计算是完全独立的，可以并行化。
- 与GCNs相反，我们的模型允许(隐式地)将不同的重要性分配给相同邻域的节点，从而实现模型容量的飞跃。此外，分析学习注意力权重可能会带来可解释性方面的好处，就像机器翻译领域的情况一样(例如Bahdanau et al.(2015)的定性分析)。

- 注意机制以一种共享的方式应用于图中的所有边，因此它不依赖于对全局图结构的预先访问，也不依赖于对其所有节点的(特性)的预先访问(许多先前技术的限制)。这有几个可取的含义.

  > -- 图不需要无向(我们只会造成计算αij如果边缘j我不存在)。
  >
  > -- 它使我们的技术直接适用于归纳学习-包括在训练过程中完全看不见的图表对模型进行评估的任务。

- Hamilton等(2017)最近发表的归纳方法对每个节点的邻域进行了固定大小的采样，以保持其计算足迹的一致性;这不允许它在执行推理时访问整个邻域。此外，这种技术在LSTM (Hochreiter &amp;使用基于社区聚合器。这假设存在一个跨邻域的连续节点排序，作者通过不断地向LSTM提供随机顺序的序列来纠正它。我们的技术不受这两种问题的影响，它与整个邻域一起工作(以变量计算占用空间为代价，它仍然与GCN之类的方法相同)，并且不假定其中有任何顺序。

- 如第1节所述，GAT可以被重新表述为MoNet的一个特例(Monti et al.， 2016)。更具体地说，伪坐标函数设为u(x, y) = f(x)kf(y)，其中f(x)表示节点x和k的特征(可能为mlp变换)为级联;权重函数wj (u) = softmax(MLP(u)) (softmax在节点的整个邻域内执行)将使MoNet s patch算子类似于我们的算子。然而，需要注意的是，与之前考虑的MoNet实例相比，我们的模型使用节点特性进行相似性计算，而不是节点的结构属性(假设预先了解了图结构)

我们能够生成一个利用稀疏矩阵操作的GAT层版本，将存储复杂性降低到节点和边缘数量的线性，并支持在更大的图形数据集上执行GAT模型。然而，我们使用的张量操作框架只支持稀疏矩阵乘法的秩-2张量，这限制了层的批处理能力，因为它是目前实现的(特别是对于具有多个图的数据集)。适当地解决这一限制是今后工作的一个重要方向。根据图结构的规律性，在这些稀疏的场景中，gpu可能无法提供与cpu相比的主要性能优势。还应该注意的是，我们的模型的接受域的大小受网络深度的限制(类似于GCN和类似模型)。然而，跳跃连接(He et al.， 2016)等技术可以很容易地用于适当地扩展深度。最后，所有图形边缘的并行化，特别是分布式并行化，可能会涉及大量的冗余计算，因为在感兴趣的图形中，邻域往往高度重叠。

> ![1544534490528](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544534490528.png)
>
> 表1:实验数据集汇总。

## 3 评价

我们已经对GAT模型与各种强大基线和以前的方法进行了比较评估，在四个已建立的基于图形的基准任务(转导的和归纳的)上，在所有这些基准任务中实现或匹配最先进的性能。本节总结了我们的实验设置、结果，并对GAT模型的提取的特征表示进行了简要的定性分析。

### 3.1 数据集

转换式学习：我们利用Cora、Citeseer和Pubmed三个标准引文网络基准数据集(Sen et al.， 2008)，密切关注Yang et al.(2016)的转导实验设置。在所有这些数据集中，节点对应于文档，而边对应于(无向)引用。节点特性对应于文档的字袋表示的元素。每个节点都有一个类标签。我们只允许每个类使用20个节点来进行训练，但是，按照转换设置，训练算法可以访问所有节点的特征向量。在1000个测试节点上评估了所训练模型的预测能力，我们使用了500个额外节点进行验证(与Kipf和amp使用的节点相同)。威林(2017))。Cora数据集每个节点包含2708个节点、5429条边、7个类和1433个特性。Citeseer数据集每个节点包含3327个节点、4732条边、6个类和3703个特征。Pubmed数据集包含19717个节点、44338条边、3个类和每个节点500个特征。

归纳学习：我们利用蛋白质-蛋白质相互作用(PPI)数据集，它由与不同人体组织(Zitnik &amp;Leskovec,2017)。数据集包含20个用于培训的图形，2个用于验证，2个用于测试。关键的是，测试图在训练过程中完全没有被观察到。为了构建这些图，我们使用Hamilton et al.(2017)提供的预处理数据。每个图的平均节点数是2372。每个节点具有50个特征，这些特征由定位基因集、基序基因集和免疫签名组成。从分子签名数据库(Subramanian et al.， 2005)收集的基因本体中，每个节点集有121个标签，一个节点可以同时拥有多个标签。

表1概述了这些数据集的有趣特性。

### 3.2 先进的方法

转换学习：对于转导性学习任务，我们将与Kipf中指定的相同的强基线和最先进的方法进行比较。威林(2017)。这包括标签传播(LP) (Zhu et al.， 2003)、半监督嵌入(sememb) (Weston et al.， 2012)、流形正则化(ManiReg) (Belkin et al.， 2006)、基于skip-gram的图形嵌入(DeepWalk) (Perozzi et al.， 2014)、迭代分类算法(ICA) (Lu &amp;Getoor, 2003)和Planetoid (Yang et al.， 2016)。我们还直接将我们的模型与GCNs (Kipf &amp;以及使用高阶Chebyshev滤波器的图卷积模型(Defferrard et al.， 2016)，以及Monti et al.(2016)提出的MoNet模型。

归纳学习：对于归纳学习任务，我们比较了Hamilton等人(2017)提出的四种不同的监督图归纳方法。这些提供各种聚合方法的功能在一个采样社区:GraphSAGE-GCN(图convolution-style操作扩展到感应设置),GraphSAGE-mean(以elementwise特征向量的平均值),GraphSAGE-LSTM(聚合喂社区特性成为LSTM)和GraphSAGE-pool(elementwise最大化的操作特征向量的改变了一个共享的非线性多层感知器)。其他的转换方法要么在归纳设置中完全不合适，要么假设节点被增量地添加到单个图中，这使得它们不能用于在培训期间完全看不到测试图的设置(例如PPI数据集)。

此外，对于这两个任务，我们提供了每个节点共享多层感知器(MLP)分类器的性能(完全不包含图结构)。

### 3.3 实验装置

转换式学习：对于转导性学习任务，我们采用了双层GAT模型。它的架构超参数已经在Cora数据集上进行了优化，然后被Citeseer重用。第一层由K = 8个注意头组成，每个注意头计算F 0 = 8个特征(总共64个特征)，然后是一个指数线性单元(ELU) (Clevert et al.， 2016)非线性。第二层用于分类:单个注意力头计算C特性(其中C是类的数量)，然后是softmax激活。针对训练集规模小的问题，模型中大量采用正则化方法。培训期间,我们应用L2正规化λ= 0.0005。此外，drop (Srivastava et al.， 2014)和p = 0.6分别应用于两层输入和归一化注意系数(关键是，这意味着在每次训练迭代中，每个节点都暴露于随机采样的邻域)。同样观察到蒙蒂et al。(2016),我们发现Pubmed年代训练集规模(60例子)需要轻微的变化得到架构:我们应用K = 8输出注意正面(而不是一个),并加强了L2正规化λ= 0.001。否则，该体系结构将与Cora和Citeseer使用的体系结构相匹配。

归纳学习：对于归纳学习任务，我们应用了一个三层的GAT模型。前两层都由K = 4的注意头计算F 0 = 256个特征(总共1024个特征)，然后是ELU非线性。最后一层用于(多标签)分类:K = 6个注意头，每个注意头计算121个特征，取平均值，然后进行逻辑乙状结肠激活。该任务的训练集足够大，我们发现不需要应用L2正规化或辍学，然而，我们成功地使用了跨越中间注意力层的跳过连接(He et al.， 2016)。我们在训练中使用了2个图的批处理大小。为了严格评估在这种环境下应用注意机制的好处(即与近似gcn等效模型相比)，我们还提供了使用恒定注意机制(a(x, y) = 1时的结果，使用相同的体系结构，这将为每个邻居分配相同的权重。

这两个模型都是使用Glorot初始化(Glorot &amp;使用Adam SGD优化器(Kingma &amp;(Ba, 2014)初始学习率Pubmed为0.01，其他所有数据集为0.005。在这两种情况下，我们都对交叉熵损失和验证节点上的精度(转导)或微f1(归纳)评分采用早期停止策略，耐心等待100个epochs1。

### 3.4 结果

我们的比较评价实验结果总结在表2和表3中。对于转导任务，我们报告了100次运行后我们方法的测试节点上的平均分类精度(带有标准偏差)，并重用Kipf & Welling(2017)和Monti等(2016)中已经报告的用于最先进技术的指标。具体来说，对于基于Chebyshev过滤器的方法(Defferrard et al.， 2016)，我们为订单K = 2和订单K = 3的过滤器提供了最大报告性能。为了公平评估注意力机制所带来的好处,我们进一步评估政府通讯模型,计算64年隐藏功能,尝试ReLU和ELU激活和报告(GCN - 64∗)更好的结果在100年之后运行(ReLU在所有三个案例)。

对于归纳任务，我们报告两个未见测试图节点上的微平均F1分数，在10次运行后平均，并将Hamilton et al.(2017)中已经报告的指标用于其他技术。具体来说，由于我们的设置是受监督的，我们将与受监督的GraphSAGE方法进行比较。评价聚合在整个社区的好处,我们进一步提供(如GraphSAGE)最好的结果我们可以实现GraphSAGE通过修改其架构(这是三层GraphSAGE-LSTM(512、512、726)特性计算每一层和128年特性用于聚合社区)。最后，我们报告了我们的持续注意GAT模型(as st-GAT)的10次运行结果，以公平地评估针对类似gcn的聚合方案(具有相同的体系结构)的注意机制的好处。

根据2.2节的讨论，我们的结果成功地展示了在所有四个数据集中实现或匹配的最新性能，这些性能与我们的预期一致。更具体地说，我们可以在Cora和Citeseer上分别对GCNs进行1.5%和1.6%的改进，这表明为相同邻域的节点分配不同的权重可能是有益的。值得注意的改进实现PPI数据集:我们的手枪模型提高了20.5% w.r.t. GraphSAGE最好我们能够获得结果,证明我们的模型有可能应用于感应设置,并且可以利用更大的预测能力通过观察整个社区。此外，它还提高3.9% w.r.t. Const-GAT(相同的架构，持续的关注机制)，再次直接体现了能够给不同的邻居分配不同权重的重要性。

我们还可以定性地研究学习到的特征表示的有效性，为此我们提供了一个t-SNE (Maaten &amp;Hinton, 2008)-在Cora数据集上预先训练的GAT模型的第一层提取的变换后的特征表示(图2)。注意，这些集群对应于数据集的七个标签，验证了在Cora的七个主题类中model s的区分能力。此外，我们将标准化注意系数的相对强度(8个注意头的平均值)形象化。正确地解释这些系数(如Bahdanau et al.(2015)所做的)将需要进一步了解正在研究的数据集的领域知识，并留给未来的工作。

## 4. 结论

我们已经介绍了图形注意力网络(GATs)，一种新型的卷积式神经网络，它基于图形结构数据，利用掩蔽的自我注意层。在这些网络中使用的图形注意层计算效率高(不需要计算密集的矩阵运算，并且可以在图形中的所有节点之间并行)，允许(隐式地)在处理不同大小的邻域时，将不同的重要性分配给邻域内的不同节点，它不依赖于预先了解整个图形结构，因此可以用以前的基于光谱的方法解决许多理论问题。我们的模型利用注意力成功地实现或匹配了四个成熟的节点分类基准的最先进的性能，包括转换和归纳(特别是使用完全不可见的图进行测试)。

图形注意力网络有几个潜在的改进和扩展，可以作为未来的工作加以解决，例如克服2.2小节中描述的实际问题，以便能够处理更大的批处理大小。一个特别有趣的研究方向是利用注意机制对模型的可解释性进行深入分析。

> ![1544535616035](F:\Machine-learning-and-data-science-notebook\images\图注意力网络\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1544535616035.png)
>
> 图2:经过预处理的GAT model s第一个隐层在Cora数据集上的计算特征表示的t-SNE图。节点颜色表示类。边缘厚度表明聚合的归一化系数节点之间i和j注意,在所有八个注意头(PK k = 1αk ij +αk)。

此外，从应用的角度来看，扩展执行图形分类而不是节点分类的方法也是有意义的。最后，扩展模型以合并边缘特征(可能表示节点之间的关系)将允许我们处理更多种类的问题。

### 致谢

作者要感谢TensorFlow的开发者(Abadi et al.， 2015)。根据第634821号赠款协议，PV和PL已经从欧盟的Horizon 2020研究和创新计划传播老龄化项目中获得了资助。我们进一步感谢以下机构对研究资金和计算支持的支持:CIFAR、加拿大研究主席、Compute Canada和Calcul Quebec，以及NVIDIA对GPU的慷慨支持。特别感谢:Benjamin Day和Fabian Jansen在前一期的论文中指出的问题;MichałDrozd zal有用的讨论,反馈和支持;和Ga etan Marceau在提交前对论文进行审阅。