# 对于邻域形成的网络嵌入

摘要：鉴于网络挖掘的丰富实际应用以及近年来代表性学习的激增，网络嵌入已成为学术界和工业界日益增长的研究热点的焦点。然而，在现有研究中很少建模以节点之间的顺序交互事件为特征的网络的完整时间形成过程，这需要进一步研究所谓的时间网络嵌入问题。鉴于此，在本文中，我们引入邻域形成序列的概念来描述节点的演化，其中序列中的邻居之间存在时间激励效应，因此我们提出基于霍克斯过程的时间网络嵌入（HTNE）） 方法。HTNE将Hawkes流程整合到网络嵌入中，以捕捉历史邻居对当前邻居的影响。特别地，低维向量的相互作用分别作为基本速率和时间影响被馈入霍克斯过程。此外，HTNE还集成了注意机制，以更好地确定历史邻居对节点当前邻居的影响。三个大规模现实生活网络上的实验表明，从提出的HTNE模型中学到的嵌入在各种任务（包括节点分类，链接预测和嵌入可视化）方面比最先进的方法获得了更好的性能。特别是，基于到达的时间推荐

## 1 导言

网络嵌入近年来已成为研究的焦点，旨在通过将节点映射到低维空间来表示大规模网络[6,9,10,20]。它提供了一种有效的方法来揭示网络结构并执行各种网络挖掘任务，如节点分类[20]，链路预测[9]，社区检测[5]等。最近的网络嵌入方法工作[9,20,22]通常通过考虑各种上下文信息（例如，节点的邻居）来关注静态网络结构。这些方法背后的一个非平凡但经常被忽视的假设是节点的邻居是无序的;换句话说，省略了链接形成历史。

然而，实际上，通过顺序地添加节点和边缘来形成网络，这实际上应该被视为由节点及其邻居之间的交互事件驱动的动态过程。结果，节点的邻域不是同时形成的，并且观察到的快照网络结构是在某些时间段内邻域的累积。例如，图1a显示了一个作者的自我网络：节点1和他/她的邻居，即节点2到6.从网络结构的快照角度来看，我们只观察最新的共同作者，而节点连接的方式和时间仍然未知。事实上，在大多数真实网络中，节点之间的边缘通常由顺序事件建立，这构成了所谓的时间网络[12]。例如，共同作者网络由具有明确时间戳的共同撰写的论文驱动。如图1a所示，我们看到每个边缘都注释了节点1与其邻居之间按时间顺序共同创作的几篇论文。因此，自我时间网络可以根据事件的定时展开成节点特定的相邻序列，其被定义为邻域形成序列并且如图1b所示。

然而，实际上，通过顺序地添加节点和边缘来形成网络，这实际上应该被视为由节点及其邻居之间的交互事件驱动的动态过程。
结果，节点的邻域不是同时形成的，并且观察到的快照网络结构是在某些时间段内邻域的累积。
例如，图1a显示了一个作者的自我网络：节点1和他/她的邻居，即节点2到6.从网络结构的快照角度来看，我们只观察最新的共同作者， 而节点连接的方式和时间仍然未知。
	事实上，在大多数真实网络中，节点之间的边缘通常由顺序事件建立，这构成了所谓的时间网络[12]。例如，共同作者网络由具有明确时间戳的共同撰写的论文驱动。如图1a所示，我们看到每个边缘都注释了节点1与其邻居之间按时间顺序共同创作的几篇论文。因此，自我时间网络可以根据事件的定时展开成节点特定的相邻序列，其被定义为邻域形成序列并且如图1b所示。邻域形成序列确实包含比表示节点中的静态网络快照更丰富的信息。我们可以从图1b中看到，由于作者之间的重复共同作者，邻居可能在序列中重复出现，这可能提供比单个边缘更多的语义含义。我们还可以更明确地观察邻居的动态变化，节点1更有可能在早些年与节点2和3共同作者，而最近转向共同作者5,6和7。此外，序列中的目标邻居的事件彼此相关，或者换句话说，历史事件可以影响当前的邻域形成。例如，我们假设节点1是博士。早年的学生，因此他/她的大部分论文都是与他/她的顾问共同撰写的，例如节点2.节点3可能是节点2的学术朋友，因此与节点2的共同作者可以 用节点3激发一些其他邻居到达事件。假设节点1在毕业后成为教授，然后他/她可以开发一些新的共同作者，并且顾问的影响可能会消失并且新连接的共同作者（例如，节点） 5）可能进一步激发其他共同作者（例如，节点6），如图1c所示。

因此，节点如何顺序地连接到它们的邻居能否揭示动态变化，并加以更好的开发利用代表了网络。虽然最近几个动态网络嵌入方法[29,30]尝试对动力学进行建模通过将时间线分割到固定的时间窗口中，学习到的嵌入仍然是特定时间段的表示考虑到动态过程。因此，直接对邻域形成序列进行建模仍然是一个巨大的挑战。

为了解决上述挑战，在本文中，我们提出了基于Hawkes过程的时间网络嵌入（HTNE）方法。具体而言，我们首先从连续事件驱动的网络结构中诱导邻域形成序列。由于霍克斯过程[11]很好地捕捉了连续事件之间的激动效应，特别是历史对当前事件的影响，我们将其调整为邻域形成过程的建模。然后，为了从霍克斯过程导出节点嵌入，通过将成对向量分别映射到基本速率和来自历史的影响，将低维向量馈送到霍克斯过程。此外，历史邻居对当前邻居形成的影响可能随着不同的节点而变化，因此我们进一步采用关注机制来增强邻域形成历史对当前邻域形成事件的影响的表现力。

为了处理大规模网络，我们的HTNE模型通过优化邻域形成序列的可能性而不是条件强度函数来解决。我们在三个大规模实际时间网络上进行了广泛的实验，以训练节点嵌入并将它们应用于几个有趣的任务，包括节点分类，链接预测和可视化。特别地，我们通过利用从邻域形成序列推断的条件强度函数来设计时间推荐实验。实验结果均显示出对一些最先进的基线方法的显着改进。

## 2 预演

### 2.1 邻域形成序列

网络形成可以被视为添加节点和边缘的动态过程，其编码节点如何彼此连接并在网络中进化的基础机制。静态网络快照确实是历史形成过程的积累，仅代表一个特定时间段的结构。因此，更期望考虑详细历史网络形成过程以恢复网络结构并代表网络。然而，网络嵌入中的大多数先前研究通常关注网络快照而不依赖于网络的形成方式，并且大多数网络嵌入方法仅基于表示节点的静态邻域[9,20,22]。

传统上，动态网络试图基于预定义的时间窗来捕获不断发展的网络结构[29,30]，然而，这只能代表不同时间段的快照，并且不能揭示网络形成的完整时间过程。因此，在本文中，我们通过跟踪每个节点的邻域形成来追溯网络形成过程。事实上，边缘通常由成对节点之间的顺序交互事件形成。例如，在共同作者网络中，作者之间的关系是由于他们在特定时间在纸上共同作者而形成的。在一个网络工作快照中，任何两位作者之间的关系只能由一个单一的边缘来表示，而论文的时间是共同作为权重。在每个边缘的顺序交互事件的驱动下，我们可以正式定义时间网络。

传统上，动态网络试图基于预定义的时间窗来捕获不断发展的网络结构[29,30]，然而，这只能代表不同时间段的快照，并且不能揭示网络形成的完整时间过程。因此，在本文中，我们通过跟踪每个节点的邻域形成来追溯网络形成过程。事实上，边缘通常由成对节点之间的顺序交互事件形成。例如，在共同作者网络中，作者之间的关系是由于他们在特定时间在纸上共同作者而形成的。在一个网络工作快照中，任何两位作者之间的关系只能由一个单一的边缘来表示，而论文的时间是共同作为权重。在每个边缘的顺序交互事件的驱动下，我们可以正式定义时间网络。定义2.1。（时间网络。）时间网络是一个网络，边缘由节点之间的时间交互事件注释，可以表示为G = <V，E;A>，其中V表示节点集，E表示边集，A表示事件集。节点x和y之间的每个边（x，y）鈭鈭E由时间顺序事件注释，即ax，y = {a1鈫鈫a2鈫鈫路·路}鈯鈯A，其中ai表示具有时间戳ti的事件。



定义2.2。（邻域形成序列。）给定时间网络中的源节点x鈭鈭V，节点的邻域是N（x）= {yi | i = 1,2，路路径}，节点之间的边缘和每个邻居都用按时间顺序的交互事件ax，yi进行注释。在数学上，邻域形成序列可以表示为一系列目标邻居到达事件，即{x：（y1，t1）鈫鈫（y2，t2）鈫鈫路路 （yn，tn）}，每个元组表示节点yi在时间ti形成为节点x的邻居的事件。

值得一提的是，节点的邻域通常表示非重复节点集。根据邻域形成序列的定义，每个邻居可以在序列中重复出现以表示与源节点的多个交互。利用定义的序列，可以明确地表明节点连接随时间的变化，从而可以从序列中推断出网络中节点的隐藏结构。再以共同作者网络为例，节点的邻域形成序列显示其共同作者的变化，我们可以帮助推断作者的研究兴趣。

此外，邻域形成序列中的事件不是独立的，因为历史邻居形成事件可以影响当前邻居形成。例如，重新搜索者可能专注于一个特定领域，例如数据挖掘，因此共同作者主要是数据挖掘研究人员。但是，当深度学习近年来成为研究的焦点时，我们可能会在他/她的共同作者序列中逐渐观察到一些AI研究人员，并且可以从最近的邻域序列中预测作者可能将研究兴趣转移到AI和未来的共同作者可能会有更多的AI人。因此，在本文中，我们的目标是考虑动态邻域形成序列，以便学习节点的表示。

### 2.2 问题定义

给定大规模时间网络G = <V，E;A>，通过跟踪x与其邻居交互的所有时间戳事件，可以将每个节点x鈭鈭V的邻居和相应的时间顺序事件引入邻域形成序列Hx。然后，时间网络嵌入旨在学习D维向量来表示每个节点，这确实学习了映射函数蠒：V鈫鈫RD，其中D≤ | V |。

与最近仅考虑静态邻居集的网络嵌入方法不同，我们通过首先建模邻域形成序列和邻居之间的激励效应来解决嵌入问题。

### 3方法学

### 3.1霍克斯过程

点过程通过假设时间t之前的历史事件可以影响当前事件的发生来对离散顺序事件建模。条件强度函数表征顺序事件的到达率，其可以被定义为在给定所有历史事件H（t）的情况下在小时间窗口[t，t +Δt]中发生的事件的数量。$\lambda(t | \mathcal{H}(t))=\lim _{\Delta t \rightarrow 0} \frac{\mathbb{E}\left[N(t+\Delta t) | \mathcal{H}_{t}\right]}{\Delta t}$

霍克斯过程是一个典型的时间点过程，条件强度函数定义如下，

$\lambda(t)=\mu(t)+\int_{-\infty}^{t} \kappa(t-s) \mathrm{d} N(s)$

其中μ（t）是特定事件的基本强度，表示在时间t的自发事件到达率;κ（·）是一个核函数，用于模拟过去历史对当前事件的时间衰减效应，该事件通常采用指数函数的形式。

霍克斯过程的条件强度函数表明，当前事件的发生不仅取决于最后一步的事件，而且还受具有时间衰减效应的历史事件的影响。这种性质对于邻域形成序列的建模是合乎需要的，因为当前的邻居形成可以通过更近的事件以更高的强度来影响，而在更长的历史中发生的事件将对目标邻居的当前发生贡献更少。

为了处理不同类型的到达事件，Hawkes过程可以扩展到多变量情况，其中条件强度函数被设计为每个事件类型为一个维度[14]。激励效应确实是所有具有不同类型的历史事件的总和，由维度d和d'之间的激发率a d，d'捕获。接下来，我们将介绍如何使用多变量Hawkes过程对邻域形成进行建模。

### 3.2 通过多变量Hawkes过程建模邻域形成序列

如前所述，当我们将每个节点视为源时，可能需要由交互事件驱动的一系列目标邻居。节点的邻域形成序列确实是计数过程，当前目标节点受历史事件的影响。因此，应用Hawkes过程来模拟源节点x的邻域形成序列自然是有吸引力的，并且x的序列中目标y的到达事件的条件强度函数可以表示为，

$\tilde{\lambda}_{y | x}(t)=\mu_{x, y}+\sum_{t_{h}<t} \alpha_{h, y} \kappa\left(t-t_{h}\right)$

其中，x，y表示事件在x和y之间形成边缘的基本速率，而h是节点x在时间t之前的邻域形成序列中的历史目标节点。αh，y表示。 历史邻居h激发当前邻居y的程度，核函数κ(·)表示可以用指数函数形式表示的时间衰减效应，

$\kappa\left(t-t_{h}\right)=\exp \left(-\delta_{s}\left(t-t_{h}\right)\right)$

要注意的是，贴现率未是源依赖参数，其说明了对于每个源节点，历史邻居可以以不同的强度影响当前邻居形成的事实。

通过遵循等式（3）中的强度函数，可以对每个源节点的邻近形成序列进行建模。接下来，为了学习网络中节点的D维表示，假设每个节点由D维向量表示并馈入强度函数。具体地，假设节点i的节点嵌入是ei，则可以从函数f（路）映射用于连接源x到目标y的基本速率：RD脳RD鈫鈫R。

从直觉上看，基率揭示了源节点x与目标节点y的自然亲和力，因此，我们使用负平方欧氏距离作为相似性度量来捕捉t之间的亲和力。 关于简洁性的节点x和y的嵌入，即x，y=f(ex，ey)=−_x_x§ey_x_2。类似地，在计算历史对当前节点αh，y的影响时，我们使用相同的相似性度量α。 h，y=f(eh，ey)=-||eh ey||2.

当我们引入的相似测度取负值时，我们应用指数函数将条件强度率传递给一个正实数，即д：r→R，因为λy_x(T) 当被视为单位时间的比率时，应取正值。然后，我们可以将邻居的条件强度函数定义为：

$\lambda_{y | x}(t)=\exp \left(\tilde{\lambda}_{y | x}(t)\right)$

### 3.3 层序形成注意事项。

考虑条件强度函数，将历史事件的影响分解为历史节点与当前目标节点之间的亲缘关系。直觉上，亲和力 在历史和目标节点之间，应该依赖于源节点。例如，一些研究人员可能通过时间有相对固定的合著者，例如邻域形成序列。 e保持稳定且更可预测，历史事件对此场景中的当前目标节点有更大的影响。而其他一些研究人员可能会从时间上改变他们的合著者。 因此，他们与不同历史合作作者具有不同的亲和力强度。因此，在对不同节点进行建模时，必须结合源节点的这些特征。 激励效应的α，这不是以前提出的条件强度函数。

根据最近关注的神经机器翻译模型[2]，我们使用Softmax单元定义了源节点与其历史节点之间的权重，如下所示：$w_{h, x}=\frac{\exp \left(-\left\|\mathbf{e}_{x}-\mathbf{e}_{h}\right\|^{2}\right)}{\sum_{h^{\prime}} \exp \left(-\left\|\mathbf{e}_{x}-\mathbf{e}_{h^{\prime}}\right\|^{2}\right)}$

为了一致性，我们选择负的欧氏距离函数来表示源节点和历史节点之间的亲和力。因此，历史邻居对当前目标的影响 可重新制定为：

$\alpha_{h, y}=w_{h, x} f\left(\mathbf{e}_{h}, \mathbf{e}_{y}\right)$

### 3.4 模型优化

通过用多变量Hawkes过程对邻域形成序列进行建模，可以从条件强度来推断近邻形成事件。然后，给出邻域形式 节点x在时间t之前的序列，用HS(T)表示，可以通过条件强度来推断x与目标近邻y在t之间形成联系的概率，



$p\left(y | x, \mathcal{H}_{x}(t)\right)=\frac{\lambda_{y | x}(t)}{\sum_{y^{\prime}} \lambda_{y^{\prime} | x}(t)}$

由于方程(5)中引入了exp(·)传递函数，p(y\x，hx(T)实际上是应用于˜λy_xx(T)的一个软件最大单位，它可以通过负采样近似优化[18]。内加特 在计算方程(8)时，IVE抽样帮助我们避免了对整个节点集的求和，这需要大量的计算。根据PN(V)∝dV 3/4的度分布，其中dV为 节点v的程度，我们对邻域形成序列中没有发生的负节点进行采样。然后源x和历史目标节点之间的边的目标函数。 时间t处的y可以按以下方式计算，$\log \sigma\left(\tilde{\lambda}_{y | x}(t)\right)+\sum_{k=1}^{K} \mathbb{E}_{v^{k} \sim P_{n}(v)}\left[-\log \sigma\left(\tilde{\lambda}_{v^{k} | x}(t)\right)\right]$

其中K是根据PN(V)采样的负节点数，σ(X)=1/(1 exp(⇐x)是Sigmoid函数。

此外，邻域列的长度影响λy_x(T)的计算复杂度，其中节点具有较长的历史长度。因此，在模型优化中，我们修复了 历史h的最大长度，并且仅保留最近序列中的目标节点。

采用随机梯度下降法(SGD)对方程(10)中的目标函数进行优化。在每次迭代中，我们用时间戳和最近形成的固定长度的小批边进行采样。 源节点的邻居来更新参数。