Streaming Graph Neural Networks / Dynamic graph neural networks

## 1.概括

目前的大多数嵌入方式都是针对静态图，实际中很多图是动态的，并且随着时间开始的动态交互信息有时候也很重要。演化结构也很重要。

在本文中，我们接受了研究动态图的图神经网络的机遇和挑战。本质上，我们的目标是回答以下问题：1）如何在新的交互发生时不断更新节点信息；2）如何将交互信息传播到受影响的节点；3）如何在更新和Propa期间合并交互之间的时间间隔国家统计局。我们提出一个动态图神经网络（dgnn）来同时回答上述三个问题。我们的贡献可以概括为：•我们为引入新的边缘时的节点信息更新和传播提供了一种原则性的方法；•我们提出了一种新的动态图的图神经网络（dgnn），它建立了边缘的阶次和时间间隔模型。o一个连贯的框架；并且•我们证明了在各种真实动态图上具有几个与图相关的任务的建议模型的有效性。

## 2.相关工作与定义

节点集

$\mathcal{V}=\left\{v_{1}, v_{2}, \dots, v_{N}\right\}$

在某时刻发生更新，有向边更新，$\left(v_{s}, v_{g}, t\right)$

这边文章只考虑增加边和节点，暂不考虑删除

为实现这一目标，引入了动态图神经网络(DGNN)，并在图1中展示了DGNN框架的概述，该框架由两个主要组件组成：1）更新组件和2）传播组件

我们简要描述了引入新交互时两个组件的操作。$\left\{v_{s}, v_{g}, t\right\}$

更新组件涉及节点$v_{s}$，$v_{g}$，并更新交互信息。例如，在图1中，一个新的交互$\left\{v_{2}, v_{5}, t_{7}\right\}$发生在t7上，更新组件中涉及的两个交互节点是v2和v5。传播分量涉及两个相互作用的节点VS，vд和“受影响节点”，因为它将交互信息{VS，vд，t}传播到“受影响节点”。“受影响的节点”可以用不同的方式定义，我们将在后面的小节中讨论。在图1中，我们将“受影响的节点”定义为与这两个节点交互的所有节点。 交互节点“，它包括{v1，v7}-v2的1跳”近邻“，和{v3，v6}-V6的1跳”邻居“。接下来，我们详细介绍每个组件。

### 2.1 更新组件

在本小节中，我们将讨论交互节点的更新组件。我们首先概述了更新组件的操作，重点是图1左侧所示的动态图的单个nodev2）。有三种交互涉及节点v2、v2、v1、t0、v7、v2、t3和v2、v5、t7。节点之间的交互自然会影响节点的属性。例如，如以上所建议的,，具有类似兴趣的用户很可能在社交网络中建立联系[26]。因此，更新组件应该将交互信息更新到两个交互节点。如图2所示，有三个更新组件，分别处理涉及节点v2和节点v2的三个交互。每个更新组件都将交互作为输入，并将交互信息更新到节点v2。注意，我们只在图2中显示节点v2的更新组件，同时还有另一个更新组件，用于为每个交互将交互信息更新到另一个交互节点。此外，交互的顺序对于理解节点的属性也很重要。例如，在电子商务用户项目图中，最近的交互可以更好地捕获用户的最新偏好。因此，捕获订单信息非常重要。将交互（涉及同一个节点）视为一个“序列”，并将更新组件循环应用于交互是很自然的。注意，虽然交互可以被视为一个“序列”，但是我们不需要存储这个“序列”的所有信息。我们只存储节点的最新信息。如图2所示，在下一个组件将前一个组件的输出作为输入的意义上，连接了三个更新组件。因此，我们基于长短期内存（LSTM）单元[17]对更新组件进行建模。如前所述，时间间隔信息也很重要，因此，我们也将其合并到更新组件中。如图2所示，单个更新组件由三个单元组成——交互单元、更新单元和合并单元。接下来我们将详细描述这三个单元。

每个节点的两个角色的隐藏状态。时间t前节点v源角色的单元记忆和隐藏状态分别表示为c s c（t-）和h s v（t-），时间t前节点v目标角色的单元记忆和隐藏状态分别表示为c ol c（t-）和h ol v（t-）。这里，符号t表示无限接近t，但在t之前的时间，这样在t之前的所有交互都被处理了。例如，在图2中，在时间t7时，对于节点v2，c s c（t7−）实际上等于c s c（t3）。注意，为了说明更新组件，我们现在不考虑传播组件。源和目标隐藏状态与合并单元合并，生成节点V的一般特征UV（T-），描述节点V的一般属性。这些细胞存储器S C（T-）、C？、C（T-）、隐藏状态H S V（T-）、H？、V（T-）和一般特征UV（T-）是为每个节点V存储的信息，在发生新的交互时需要更新。例如，图3显示了两个更新组件在发生交互v2、v5、t7时对节点v2和v5执行更新的操作。图2（a）显示了T7之前为两个节点存储的信息。

#### 2.1.1交互单元。

交互单元设计为从节点信息生成$\left\{v_{s}, v_{g}, t\right\}$的交互信息。生成的交互信息后来用作更新单元的输入。我们的模式 使用深度前馈神经网络的EL交互单元，该公式如下：$e(t)=\operatorname{act}\left(W_{1} \cdot u_{v_{s}}(t-)+W_{2} \cdot u_{v_{g}}(t-)+b_{e}\right)$

其中$u_{v_{s}}(t-)$和$u_{v_{g}}(t-)$是时间t之前节点vs和$v_{g}$的一般特征，W1，W2和$b_{e}$是神经网络的参数，$\operatorname{act}(\cdot)$是一个激活函数，如sigmoid或tanh。输出E(t)包含交互$\left\{v_{s}, v_{g}, t\right\}$的信息。作为一个示例，图3(b)显示了交互单元如何为交互$\left\{v_{2}, v_{7}, t_{7}\right\}$工作。

#### 2.1.2更新单元。

如前所述，交互（涉及同一节点）可以看作是一个“序列”。随着这些交互的顺序发生，这个节点的信息逐渐演化。因此，为了捕获该节点的这些交互信息，我们反复应用更新组件来处理交互信息。更新单元是执行将交互单元生成的交互信息更新到交互节点的操作的部分。回想一下，相互作用并不是在时间上均匀地出现。涉及同一节点的交互之间的时间间隔可能会有很大的变化。时间间隔会影响应如何忘记旧信息。直观地说，过去发生的交互应该对节点的当前信息影响较小，因此应该“严重”地忘记它们。另一方面，最近的交互应该对节点的当前信息更加重要。因此，需要将时间间隔合并到更新组件中。因此，为了构建更新单元，我们对lstm单元进行了类似于[3]的修改，以合并时间间隔信息来控制遗忘的“magtitude”。

更新单元如图4所示，该单元的输入包括最近的单元存储器cv（t−）、隐藏状态hv（t−）、时间间隔∆t和交互单元计算的交互信息e（t）。更新单元的输出是更新后的单元存储器V（T）和隐藏状态HV（T）。注意，为了便于说明，我们不区分图4中的源和目标单元内存以及隐藏状态。实际上，我们有两种类型的更新单元，即S-update单元和G-update单元，它们具有相同的结构，但参数不同。对于交互v s，v_，t，我们使用s-update单元更新源节点v的信息，并使用g-update单元更新目标节点v_的信息。更新单元基于LSTM单元，更新单元和标准LSTM单元之间的唯一区别在于图4的蓝色虚线框部分。本部分对应的公式如下：

$\begin{aligned} C_{v}^{I}(t-1) &=\tanh \left(W_{d} \cdot C_{v}(t-1)+b_{d}\right) \\ \hat{C}_{v}^{I}(t-1) &=C_{v}^{I}(t-1) * g\left(\Delta_{t}\right) \\ C_{v}^{T}(t-1) &=C_{v}(t-1)-C_{v}^{I}(t-1) \\ C_{v}^{*}(t-1) &=C_{v}^{T}(t-1)+\hat{C}_{v}^{I}(t-1) \end{aligned}$

在本部分中，根据时间间隔调整旧单元存储器cv（t_1）以生成调整后的旧单元存储器c_v（t_1）。首先将其分解为两个部分：短期记忆C I V（t_1）和长期记忆C T V（t_1），其中C I V（t_1）由神经网络生成，长期记忆C T V（t_1）=c v（t_1）−c I V（t_1）。长期记忆保持不变，而短期记忆根据具有贴现函数oj的事件之间的时间间隔∆t进行贴现（遗忘）。贴现函数是一个递减函数，这意味着时间间隔越大，保存的短期记忆就越少。因此，我们使用这个模型来模拟我们应该如何忘记模型中的旧信息。然后将贴现的短期存储器c_i v（t_1）和长期存储器组合起来，生成调整后的旧单元存储器c_v（t_1）=c_i v（t_1）+c t v（t_1），这可以被视为输入到标准LSTM单元（剩余部分更新单元）。分解和重组可以确保在这个过程中不会丢失旧单元存储器的全部信息。