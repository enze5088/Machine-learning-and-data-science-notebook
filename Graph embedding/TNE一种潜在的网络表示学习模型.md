# TNE：一种潜在的网络表示学习模型

Abdulkadir C¸ elikkanat∗ Fragkiskos D. Malliaros∗

#### 摘要：

网络表示学习(NRL)方法的目标是通过保持给定网络的局部和全局结构，将每个顶点映射到一个低维空间，近年来由于在几个具有挑战性的问题上的成功，它们受到了极大的关注。虽然提出了各种方法来计算节点嵌入,许多成功的方法受益于随机漫步为了给定网络转换成一个序列集合的节点,然后学习目标的表示节点通过预测上下文中的每个顶点序列。在本文中，我们引入了一个通用的框架来增强基于随机步行方法的节点嵌入。与NLP中的话题词嵌入概念类似，该方法利用各种统计模型和社区检测方法将每个顶点分配到一个主题，然后生成增强的社区表示。我们评估了我们的方法的两个下游任务:节点分类和链接预测。实验结果表明，顶点嵌入和主题嵌入的合并优于众所周知的基础NRL方法。

## 1. 简介

图形是重要的数学结构，通常用于表示对象及其在现实系统中的关系，如万维网、社交网络和蛋白质-蛋白质相互作用。由于网络出现的广泛应用，网络分析方法引起了研究人员的极大兴趣，人们提出了许多技术来更好地理解和揭示网络的潜在特性。近年来，在网络表示学习(NRL)领域出现了许多突出而有力的方法。NRL技术的主要目标是通过保留网络的重要结构特性，学习图中节点对应的特征向量(也称为节点嵌入);这些向量以后可以用来执行各种分析和挖掘任务，包括可视化、节点分类和链接预测，这些都是机器学习算法的优势。

在节点表示学习领域的最初研究大多依赖于矩阵分解技术，因为节点间的各种性质和相互作用可以表示为矩阵运算。然而，这些方法主要适用于小规模网络，因为计算成本高，特别是对于由数百万个节点和边组成的[11]图。最近的研究集中于开发适用于相对大规模网络的方法，以便能够有效地逼近潜在的目标函数，从而捕获关于图中节点及其属性的有意义的信息。

大量的节点表示学习方法受到自然语言处理(NLP)领域的进步的启发，借鉴了最初为计算词嵌入而开发的各种思想。其中一个成功的技术是Skip-Gram architecture[17]，它的目标是通过在文本语料库的句子中估计单词的上下文来发现单词的潜在表示。这样，NRL的许多先锋研究利用随机游动的思想将图形转换成句子的集合，以此类推到自然语言领域，这些句子或散步后来被用来学习节点嵌入。

尽管基于随机步行的方法足够强大，可以捕获本地连接模式，但它们主要是为了充分传递关于网络的全局结构属性的信息。更准确地说，真实世界网络具有固有的集群(或社区)结构，可用于进一步提高节点嵌入的预测能力。可以根据与文档集合中的主题概念类似的概念来解释这种结构信息。就像word嵌入可以通过基于主题的信息[16]来增强一样，这里我们的目标是通过使用关于图的社区结构的信息来增强节点嵌入，这种信息可以通过类似于主题建模的过程来实现。

在本文中，我们提出了拓扑节点嵌入(topical node embed, TNE)，这是一个框架，在这个框架中，节点和主题嵌入是分开从网络中学习的，然后它们合并成一个单一的向量，从而进一步提高下游任务的性能。本文的主要贡献可以总结如下:

- **一个新的节点表示学习框架。**我们提出了一种名为TNE的新策略，该策略从图中学习社区嵌入，并使用它们改进基于随机遍历方法提取的节点表示
- **富集特征向量。**我们对TNE学习到的嵌入在节点分类和链路预测任务上进行了详细的实证评估。实验结果表明，该模型提供了能够提高下游任务性能的特征向量。

本文的其余部分组织如下。在第二部分中，我们描述了相关的工作。在第3节中，我们提出了这个问题，在第4节中，我们提出了这个方法。第5节给出了实验结果，最后在第6节我们总结了我们的工作，并提供了未来的研究方向。

## 2. 相关工作

近年来，人们提出了许多方法来学习无监督方式下节点的潜在表示。开发学习网络表示的技术固有地包含了大量的挑战，因为良好的表示应该捕获网络的各种潜在属性。例如，许多现实世界的网络都由紧密连接的社区组成，并且在程度分布上服从无尺度的特性;换句话说，少量节点(称为集线器)连接到大多数节点。因此，保留结构的方法应该能够产生潜在的表示，在这种表示中，连接到中心的节点应该在嵌入空间中足够靠近它，而如果它们属于完全不同的社区，它们也应该彼此远离[11]。

传统的无监督特征学习方法是考虑给定网络的性质和连接，对矩阵表示进行因式分解。MDS[12]、Laplacian特征映射[1]、局部线性嵌入(LLE)[24]和异构映射[26]只是其中一些旨在保持节点一阶邻近性的方法。最近，提出的算法包括GraRep[4]和HOPE[18]，旨在保持节点的高阶近似性。然而，尽管矩阵分解方法提供了一种优雅的方法来捕获所需的属性，但是它们主要受到时间复杂度的影响。

近年来，基于随机步行的[11]方法受到了广泛的关注，主要原因是其有效性。事实上，[21]最近的一项研究表明，DeepWalk和node2vec[10,20]隐式地执行矩阵分解。按照这一思路，提出了不同的随机抽样策略，出现了多种方法[15,22]。

就我们所知，很少有研究能从真正的网络工作的社区结构属性中获益来学习节点嵌入。[27]的作者提出了一种基于矩阵分解的算法，将社区结构融入到嵌入过程中，隐含地关注模块化的数量。在模型[5]中，提出了一种社区编码、学习节点嵌入和网络社区检测之间的闭环过程。正如我们即将介绍的，我们的工作旨在独立学习节点和社区(主题)嵌入，然后将它们组合成表达的主题特征向量

> ![1542885104762](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542885104762.png)
>
> 图1：Zachary的空手道俱乐部网络中的主题分配。每个节点v通过tne-LDA模型被分配给社区k最大化P(v=k)。

## 3. 图的问题的形式和隐模型

设$G = (V, E)$为图，其中V为节点集，E为边集。我们的目标是找到一个映射函数Φ:V R d,在Φ(V)的表示表示顶点V在低维空间R d(这是我们想要学习的，以供下游学习任务之用。)和d通常被称为嵌入或维数，它比顶点集的基数| V|要小得多。

节点嵌入方法基于流行的SkipGram架构[17]主要目标最大化对数概率maxΦ,Φe P v P u Nγ(v)日志P(Φ(u)|Φ(e v)),在Nγ(v)代表可及节点的数量最多的从顶点v vγ步骤。然而,我们必须处理计算问题当我们的目标是找到Φ(u)和Φ(e v)为每个u,v v,主要是因为计算成本会显著增大随着γ长度增加由于Nγ求和(v)。因此,很多方法倾向于近似使用随机漫步上述目标函数如下:

![1542885730522](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542885730522.png)

其中w = (v1，…)， vi，……Vl)是长度的散步,γ是指窗口大小,W是散步的集合。注意,我们得到两个不同的嵌入向量Φ(v)和Φ为每个节点(e v)v v,但我们只会考虑向量Φ(v)的节点嵌入v v。

复杂的网络，如社会或生物网络，由不同大小的潜在集群组成，其中的节点更有可能彼此连接到[8]。虽然一些基于随机游动的方法隐含地受益于网络的这种结构特性，但我们在这里的主要目标是使用给定网络的集群增强节点嵌入向量。我们主要依靠两种不同的方法来提取潜在群落:随机游动和网络结构本身。对于给定的图G，我们将使用符号K表示G的集合。

### 3.1 基于随机游动的图形主题模型

大多数现实世界网络都可以表示为嵌套或重叠社区[19]的组合。因此,当初始化随机漫步,它不仅访问邻近的节点,而且还遍历社区网络中(见图2)。在这方面,我们假设每个随机漫步可以表示为随机混合物在潜在的社区,每个社区可以在节点分布的特征。换句话说,我们可以编写以下为每个走在网络上生成模型:

![1542885882188](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542885882188.png)

这里，N是走过的次数，L是走过的长度，K是聚类的个数。

如果我们把每个随机游动看作一个文档，把随机游动的集合看作一个语料库，那么我们可以看到，上面定义的统计过程对应于众所周知的潜狄利克雷分配(potential Dirichlet Allocation, LDA)模型[7]。因此，在NLP术语中，每个社区对应一个不同的主题(在本文的其余部分中，主题和社区这两个术语可以互换使用)。

现在我们可以使用社区或主题中游走的$w\in \mathcal W$中的节点们其中的z获得更好的向量表示。通过将节点替换为其主题标签，我们的目标是预测主题上下文中的节点。更正式,我们声明我们的目标函数可以找到社区或主题表示如下:![1542889128144](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542889128144.png)

通过上面的对数概率最大化,我们获得嵌入向量Ψ(e k)R d为每个主题标签k k,称为主题嵌入或陈述。在本文中，我们将这个模型称为Lda。

在以前的LDA模型中，每个节点的潜在社区分配是独立地从行走中的前一个节点的主题标签中选择的。然而，当前节点的隐藏状态对于确定要访问的下一个节点起着重要作用，因为随机游走也会遍历各个社区。因此，我们可以修改Lda模型，并定义以下生成过程：![1542889237500](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542889237500.png)

上述模型实际上是著名的隐马尔可夫模型，它具有对称的关于跃迁和发射分布的狄利克雷先验(我们将该模型称为Hmm)。注意，在每个节点序列的生成中，使用相同的转移概率，不同于Lda模型的主题分布，向量ak和bk分别包含K和|V|分量。此外，引理3.1还表明，Lda模型在选择合适的分布后，也可以看作是Hmm生成特定节点序列的特殊情况。

引理3.1。生成主题和节点序列的概率z = (z1，…， zL)， w = (v1，…,六世)Ldaθwφk和主题分布,对于一个给定节点的概率等于生产序列嗯如果初始、过渡和发射概率是选为π:=θw (·k): =θw k和bk =φk。证据。请参阅附录。

### 3.2 基于网络结构的建模

在以前的模型中，生成的随机游走用于检测给定节点序列中每个节点的community(或topic)赋值。在这里，我们提出了两个额外的模型，即BigC和Louvain，它们的直接目标是确定给定网络中的节点群落。Louvain模型使用Louvain方法[2]提取群落，BigC模型基于重叠的群落检测方法BigClam[29]。

## 4. 局部节点嵌入

在本节中，我们将详细描述所提议的局部节点嵌入(TNE)模型。模型概述如图2所示。我们的总体目标是使用关于图的底层主题的信息增强节点嵌入。这可以通过学习节点和主题嵌入向量相互独立，共同最大化式(3.1)和式(3.2)中定义的目标来实现。通过结合这些目标函数,我们得到以下方程:![1542889535084](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542889535084.png)

在Skip-Gram模型[17]中，将上式中概率测度P(·|·)视为softmax函数![1542889587900](C:\Users\DELL\AppData\Roaming\Typora\typora-user-images\1542889587900.png)

为了提高计算效率，我们采用了负抽样技术[17]。

在获得节点和主题表示之后，我们的最后一步是有效地合并这些特征向量。为此，我们引入三个简单的策略，即M ax(·)、WMean(·)和M in(·)

M ax (v)。它生成最终表示为节点v结合节点和社区嵌入:M ax(v):=Φ(v)Ψ(e k)。这里，主题标签k等于使表达式P(v|ek)最大化的参数k，符号表示连接操作。例如，如果我们在图1中为Zachary s空手道俱乐部选择主题数量为2，那么每个节点v都被分配给主题k，这个主题k的概率最大。

m in(V)。第二种策略可以定义为M in(V)：=Φ(V)⊕Ψ(e k∗)，其中k∗=arg mink P(v\k_k)。

WMean(V)。最后的策略是：WMean(V)：=Φ(V)⊕P kΨ(E K)·P(v\k)。

将节点与主题特征向量串联后得到的最终向量称为局部节点嵌入。算法1提供了TNE模型的伪代码。

框架的总体结构如下。首先，我们需要一组遍历网络的遍历来学习节点和主题嵌入，因此，任何方法，如Deepwalk和Node2vec，都可以用来执行随机遍历。然后，我们根据第3节中定义的图形的潜在模型，为该集合选择一种策略来获得walk w w中每个节点v v的主题赋值tw(v)。在第一种情况下，我们使用第3节中描述的随机过程Lda和Hmm，分别得到tne-lda和tne-hmm的局部节点嵌入模型。在第二种情况下，根据基于BigC模型和Louvain模型的网络结构，分别根据BigClam方法和Louvain方法推导出主题赋值，对应的局部节点嵌入模型称为tne-BigC模型和tne-Louvain模型。

![1542889722090](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542889722090.png)

然后，我们生成节点-上下文对，为Skip-Gram算法提供输入，并学习潜在节点表示。通过将每一个节点v替换为其在walk w w中的主题赋值tw(v)，我们得到了一组新的学习主题嵌入的对。最后，我们根据我们的方法组合特征向量。![1542889775412](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542889775412.png)

![1542891648044](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542891648044.png)

## 5. 实验

在本节中，我们将介绍我们在实验中使用的数据集，并进一步讨论所提出的tne模型在节点cl任务中的性能和有效性。 辅助和链接预测。我们的模型是用Python实现的，源代码可以在：[https://abdcelikkanat.github.
io/projects/TNE/](https://abdcelikkanat.github.
io/projects/TNE/). 中找到。

### 5.1 基础方法

我们将考虑两种著名的基于随机游动的方法，并将我们的框架应用于这些算法生成的游动集合。

- **Deepwalk[20]**在生成walk时使用了非常自然的采样策略。在每个步骤中，它一致地选择一个节点，该节点与当前驻留的节点具有连接，并重复相同的过程，直到获得所需长度的遍历。我们将这种方法称为deepwalk-emb。
- **Node2vec[10]**是深度步行的延伸，其步行行为由两个参数p和q控制，这两个参数提供了发现网络中较远区域的能力;它还捕获节点之间的结构相似性。我们将此方法称为node2vecemb。

### 5.2 参数设置

在本节中，我们将描述用于实验的参数设置，并阐明我们所遵循的策略。由于我们在这里研究的两种随机游动抽样策略(Deepwalk和Node2vec)共享许多公共参数，所以我们将它们分配到相同的典型值。

更具体地说,我们认为走n = 80的数量,长度= 10,窗口大小γ= 10,嵌入维数d = 128。对于所有的实验，Node2vec的return和in-out超参数p, q都被简单的设置为4.0和1.0，所以我们鼓励walk去探索网络中之前没有访问过的区域。为了加快训练过程，我们对所有模型使用负采样[17]。我们还使用随机梯度下降(SGD)[3]进行优化，将初始学习率设置为0.025。

为了学习节点序列中每个节点的主题分配，我们对tneLda模型执行折叠Gibbs抽样[9]，对tne-Hmm执行变分消息传递[28]。对于TNE框架的所有变体，实验中选取的主题个数为K = 80，采用Max串联的方法得到最终的嵌入向量。

### 5.3 多标签节点分类

在多标签节点分类实验中，将网络的每个节点至少分配到一个标签;目标是通过只观察网络的某些部分来预测正确的节点标签。为了完成节点分类任务，我们使用了我们所学的嵌入向量。将特征向量集合随机分割为训练集和测试集，采用L2正则化的one-vs-rest logistic回归分类器进行优化。为了提供更可靠的实验结果，我们重复同样的步骤50次。我们在实验中使用了以下三个数据集。

> ![1542892191340](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542892191340.png)图3：针对Deepway和Node2vec提出的TNE框架的性能评估，涉及培训数据的不同部分。x-轴表示训练数据集的比率，而 y轴显示三个不同网络上不同随机游走策略的Micro-F1评分。

- CiteSeer[6]是从CiteSeer图书馆中提取的一个引文网络，其中节点代表研究论文，边缘表示出版物之间的引用。
- 蛋白质-蛋白质相互作用(PPI)是Saphiens人PPI网络的子图，每个标记对应于一个生物状态[10]。
- Cora[25]是一个由机器学习出版物组成的引文网络，分为七类。语料库中的每一篇论文都被引用或引用至少一篇其他论文。

#### 实验量测结果

图3描述了TNE框架的变体以及基线方法的Micro-F1分数，与培训集中节点的数量有关。在表2所示的Macro-F1分数的情况下训练集和测试集的大小是相等的。可以看出，tne-BigC与原始的Deepwalk模型(deep walk-emb)相比，提供了高达6.69%的增益，与Citeseer数据集上的Node2vec (node2vecc -emb)相比，提供了高达6.31%的增益。

> ![1542892605786](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542892605786.png)
>
> 表1：用于多标签节点分类实验的网络统计.

虽然在ppi网络中，node2vec和deepway两种特征学习方法的总体性能是相同的，但tne-lda模型的得分提高了2.83%，tne-Loufan则不能实现。 现在的表演和它一样精彩。

### 5.4 专题数量的影响

> ![1542892763547](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542892763547.png)
>
> 表2:多标签节点分类的宏f1得分，其中50%的节点用于训练。上表显示了应用于Deepwalk提取的游动的各种TNE模型的性能，以及Deepwalk算法的性能。同样，下表给出了TNE相对于Node2vec的性能。

在这一段中，我们将分析主题(或集群)数量对框架性能的影响。我们在CiteSeer网络上进行实验，研究了由Deepwalk和Node2vec生成的随机游动集合上的tne-Lda和tne-Hmm模型。除主题数量外，所有参数设置与5.2小节中描述的参数设置相同。从图4可以看出，主题数量的增加对tne-Lda模型有一定的正向贡献，且贡献达到一定的值。另一方面，这对tne-嗯;当K = 120时，它在两种随机游走策略中表现更好。选择的题目个数表明，在训练数据量较大的情况下，选择的题目个数越少，得分越接近。

### 5.5 连接策略的效果

在第4节中，我们描述了如何组合节点和主题特征向量，以构建主题节点嵌入。在这里，我们做了几个实验来观察这些策略在不同的训练数据大小下的行为。图5描述了CiteSeer网络上的Micro-F1分数。从图中可以看出，MAX和WMean策略在所有情况下都要比第三个策略表现的更好，并且它们的得分非常接近。

> ![1542892896639](F:\Machine-learning-and-data-science-notebook\images\THE：一种潜在的网络表示学习模型\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1542892896639.png)
>
> 图4：对于CiteSeer网络的主题数量的不同值，micro-F1评分。

### 5.6 链接预测

在链路预测任务中，我们对网络边缘的访问是有限的，我们的目标是预测节点之间缺失的(看不见的)边缘。我们将给定网络的边缘集分为两部分，随机去掉50%的边缘(在此过程中网络保持连接)，形成训练集和测试集。去除的边作为测试集的阳性样本，选取初始网络中不存在的相同数量的节点对，得到每个训练集和测试集的阴性样本。根据表4所列的二元算子，将节点嵌入向量转换为边缘特征。

我们执行所有实验使用逻辑回归分类器和L2正规化第二网络:

- Gnutella[23]是2012年8月9日收集的p2p文件共享网络。它由8,114个节点和26,013条边组成。
- Facebook[14]是一个包含4039个节点和88234条边的社交网络。
- arXiv GR-QC[13]是一个由5,242个节点和14,496条边组成的协作网络。

#### 实验结果 

表3为链路预测任务曲线下面积(AUC)得分情况。由此可见，所提出的TNE框架在所有情况下都优于基线方法。对于Facebook网络，tne-BigC为所有运营商(除了一般运营商)提供了最好的结果，这也符合所有不同设置下的最佳表现模式。

## 6 结论和今后的工作

本文提出了一种潜在的网络表示学习模型TNE。TNE利用节点所属的主题(或集群)来引入主题节点嵌入的概念。这样，与传统的基于随机游动的方法相比，TNE能够产生丰富的潜在节点表示，从而提高节点分类和链接预测任务的性能。

目前，TNE可以与基于随机游动的方法一起应用。一个有趣的未来方向是如何扩展框架，包括其他NRL算法。此外，受许多实际网络遵循的分层社区结构的驱动，一个有趣的未来方向是将该框架扩展到学习分层节点嵌入。最后，我们计划在社区检测任务中评估TNE。