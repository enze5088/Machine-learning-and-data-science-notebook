

# 作为矩阵分解的网络嵌入：使 DeepWalk, LINE, PTE, 和 node2vec 统一

$Jiezhong Qiu^†, Yuxiao Dong^‡, Hao Ma^‡, Jian Li, Kuansan Wang^‡, and Jie Tang^†$

## 摘要

自从word2vec[25,26]发明以来，skip-gram模型显著地推动了网络嵌入的研究，例如最近出现的DeepWalk、LINE、PTE和node2vec方法。在这项工作中，我们证明了上述所有具有负采样的模型都可以被统一到具有封闭形式的矩阵分解框架中。我们的分析和证明表明:(1)根据经验，DeepWalk[28]产生了一个网络归一化拉普拉斯矩阵的低秩变换(2)理论上，LINE[35]是顶点上下文大小设置为1时深度行走的特殊情况;(3) PTE[34]作为LINE的延伸，可以看作是多个网络拉普拉斯算子的联合因数分解;(4) node2vec[15]是分解一个与二阶随机游动的平稳分布和转移概率张量有关的矩阵。进一步给出了基于跳变图的网络嵌入算法与图拉普拉斯理论之间的理论联系。最后给出了计算网络嵌入的NetMF方法及其近似算法。对于传统的网络挖掘任务，我们的方法明显优于DeepWalk和LINE(相对高达38%)。该工作为基于skip-gram的网络嵌入方法奠定了理论基础，有助于更好地理解潜在的网络表示形式。

关键词：网络嵌入;矩阵分解;图谱

## 一、简介

传统的利用网络进行挖掘和学习的范式通常是从对其结构特性的显式探索开始的[12,30]。但是，许多这样的属性，如中间中心性、三角计数和模块化，都需要大量的领域知识和昂贵的计算量。针对这些问题，以及最近出现的表征学习[1]所带来的机遇，学习网络的潜在表征，也就是。为了将网络的结构特性自动发现并映射到潜在空间中，近年来网络嵌入技术得到了广泛的研究。

形式上，网络嵌入问题被形式化为：给定一个无向加权图G=(V，E)，以V为节点集，E为边集，目标是学习一个函数。 $V→R^{|V|\times d}$，它将每个顶点映射到捕获G的结构属性的d维$(d<<|V|)$潜在表示。

输出表示可以作为挖掘和学习算法的输入，用于各种网络科学任务，如标签、分类、阳离子和社区检测等。

> ![1543300569285](F:\Machine-learning-and-data-science-notebook\images\作为矩阵分解的网络嵌入\%5CUsers%5CDELL%5CAppData%5CRoaming%5CTypora%5Ctypora-user-images%5C1543300569285.png)
>
> 表1:DeepWalk、LINE、PTE和node2vec隐式逼近和分解的矩阵。

解决这一问题的尝试可以追溯到谱图理论[10]和社会维度学习[36]。其最近的进展主要是由最初提出的词嵌入的skip-gram模型所推动的[25,26]，该模型的输入是由自然语言中的句子组成的文本语料库，输出是语料库中每个词的潜在向量表示。值得注意的是，受此启发，DeepWalk[28]将随机遍历网络的顶点路径视为句子，并利用跳格来学习潜在节点表示，从而开创了网络嵌入的先行者。随着DeepWalk的出现，许多网络嵌入模型被开发出来，例如LINE [35]， PTE [34]， node2vec[15]。

到目前为止，上述模型已经得到了相当程度的实证验证。然而，它们背后的理论机制却鲜为人知。我们注意到带负采样的词嵌入skip-gram被证明是对特定词上下文矩阵[21]的隐式分解。但是，还不清楚单词上下文矩阵与网络之间的关系是什么.此外，在理论上分析了DeepWalk的行为[47]。然而，它们的主要理论结果并不完全符合arxiv：1710.02971v1[cs.si]2017年10月9日原始“深度行走”论文(见第2.2节的详细讨论)。此外，尽管DeepWalk、Line、PTE和node2vec之间存在着超时空相似性，但对它们之间的潜在联系缺乏更深入的理解。

**主要贡献：**在这项工作中，我们提供了几个理论结果，涉及几个现有的skip-gram启发的网络嵌入方法。更具体地说，我们首先证明我们提到的四个模型DeepWalk、LINE、PTE和node2vec在理论上都是执行隐式矩阵分解的。我们推导出每个模型的矩阵的封闭形式(总结见表1)。例如，深度遍历(在图上的随机遍历+skip-gram)本质上是隐式地首先逼近封闭形式的矩阵x1，然后分解逼近的矩阵。

其次，从矩阵的封闭形式观察到，有趣的是，LINE可以看作是DeepWalk的特例，当周围上下文的窗口大小T设置为1时。 进一步证明了PTE作为线的延伸，实际上是多个网络的联合矩阵的隐因式分解。

在此基础上，我们发现了Deep-Walk隐式矩阵与图拉普拉斯算子之间新的理论联系。在此基础上，提出了一种新的算法NetMF来近似深度步隐式矩阵的封闭形式。通过对这个基于SVD的稀疏矩阵进行显式分解，我们在四个网络(用于深度遍历和node2vec方法)中的大量实验证明，NetMF在深度遍历和线性遍历上具有出色的性能(相对提高了38%)。

## 二、理论分析和证明

在这一部分中，我们对四种流行的网络嵌入方法：line、PTE、DeepWalk和node2vec进行了详细的理论分析和证明。

### 2.1 LINE 和 PTE

#### LINE [35]

我们从二阶邻近(又名LINE(2)开始我们对LINE的分析。LINE(2)的目的是学习两个表示矩阵$X，Y∈R^{|V|\times d}$，其中它们的行由$x_i和y_i表示，i=1，···，n$。LINE(2)的目的是最大化:

$$ \Large \mathcal {L}=\sum\limits_{i=1}^{|V|} \sum\limits_{j=1}^{|V|}A_{i,j}(\log g(x_i^Ty_j)+bE_{j'~P_N}[\log g(-x_i^Ty_{j'})]) $$

其中$g(Z)=1/(1+exp(−z))​$是Sigmoid函数,B是负采样的参数；$P_N​$被称为产生负样本的噪声分布。对$P_N(J)∝d j^{3/4}​$进行了经验设定。在我们的分析中，我们假设$P_N(J)∝d_j​$，因为归一化因子在图论中有一个封闭形式的解，即$P_N(J)=d_j/vol(G)^2​$。重写目标函数

$$ \Large \mathcal {L}=\sum\limits_{i=1}^{|V|} \sum\limits_{j=1}^{|V|}A_{i,j}\log g(x_i^Ty_j)+b\sum\limits_{i=1}^{|V|}d_iE_{j'~P_N}[\log g(-x_i^Ty_{j'})]$$ （1）

并将期望项显式表示为

$$E_{j'~P_N}[\log g(-x_i^Ty_{j'})]=\sum\limits _{j'} \frac{d_{j'}}{vol(G)}  \log g(-x_i^Ty_{j'})$$

$$=  \sum\limits _{j} \frac{d_{j}}{vol(G)}  \log g(-x_i^Ty_{j})+\sum\limits_{j' \neq j }\frac{d_{j'}}{vol(G)}  \log g(-x_i^Ty_{j'})$$

